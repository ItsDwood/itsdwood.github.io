[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Donovan’s Blog Post",
    "section": "",
    "text": "About this blog\nTesting this"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Whose Costs?",
    "section": "",
    "text": "The dataset below will be the one I use for this blog post\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf = pd.read_csv(url)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\n\nFor the dataset, I created two plots and one summary table in order to contextualize the data.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\nplt.figure(figsize=(12, 8))\nsns.scatterplot(x='person_age', y='person_emp_length', hue='loan_intent', data=df, palette='viridis')\nplt.title('Loan Intent vs. Age and Employment Length')\nplt.xlabel('Age')\nplt.ylabel('Employment Length (Years)')\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Loan Intent\n\n\n\n\n\n\nplt.figure(figsize=(10, 6))\nsns.countplot(x='person_home_ownership', hue='loan_grade', data=df, palette='Set2')\nplt.title('Loan Grade Distribution by Home Ownership')\nplt.xlabel('Home Ownership')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\n\nFigure 2: Loan Grade Distribution by Home Ownership\n\n\n\n\n\n\nsummary_table = df.groupby('loan_intent').agg({\n    'loan_int_rate': 'mean',\n    'loan_amnt': 'mean'\n}).reset_index()\n\nsummary_table.rename(columns={\n    'loan_int_rate': 'Average Interest Rate',\n    'loan_amnt': 'Average Loan Amount'\n}, inplace=True)\n\nprint(summary_table)\n\n         loan_intent  Average Interest Rate  Average Loan Amount\n0  DEBTCONSOLIDATION              10.983305          9620.901149\n1          EDUCATION              10.965465          9460.015604\n2    HOMEIMPROVEMENT              11.160075         10348.725017\n3            MEDICAL              11.051946          9242.269907\n4           PERSONAL              11.009814          9549.427178\n5            VENTURE              10.940866          9516.417425\n\n\n\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import Pipeline\n\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_all = pd.read_csv(url)\n\ndf = df_all.drop(['loan_grade', 'loan_status'], axis=1)  \ndf = pd.get_dummies(df, columns=['person_home_ownership', 'loan_intent', 'cb_person_default_on_file'], drop_first=True)\ndf.dropna(inplace=True)\n\ny = df_all.loc[df.index, 'loan_status']  \n\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=123)\n\n# Scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlogreg = LogisticRegression(max_iter=1000)\n\nselector = RFECV(estimator=logreg, step=1, cv=5)\nselector = selector.fit(X_train_scaled, y_train)  # Fitting RFECV to the scaled training data\n\n\nX_train_selected = selector.transform(X_train_scaled)\nX_test_selected = selector.transform(X_test_scaled)\n\nmodel_selected = LogisticRegression(max_iter=1000)\nmodel_selected.fit(X_train_selected, y_train)\n\nselected_features = X_train.columns[selector.support_]\n\n# Prediction\ny_pred_selected = model_selected.predict(X_test_selected)\nprint(f\"Optimal number of features: {selector.n_features_}\")\nprint(classification_report(y_test, y_pred_selected))\n\nOptimal number of features: 8\n              precision    recall  f1-score   support\n\n           0       0.87      0.94      0.90      3614\n           1       0.68      0.48      0.56       968\n\n    accuracy                           0.84      4582\n   macro avg       0.77      0.71      0.73      4582\nweighted avg       0.83      0.84      0.83      4582\n\n\n\n\nfeature_names = df.columns\n\nselected_features = feature_names[selector.support_]\n\nprint(\"Selected features:\", selected_features)\n\nSelected features: Index(['loan_amnt', 'loan_int_rate', 'loan_percent_income',\n       'person_home_ownership_OWN', 'person_home_ownership_RENT',\n       'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE'],\n      dtype='object')\n\n\n\n\nX = df[selected_features]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n## MODEL HERE\nmodel.fit(X_train_scaled, y_train)\n\nNameError: name 'model' is not defined\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef linear_score(X, w):\n    return X @ w\n\n\n# Extracting weights \nw = model.coef_.flatten()  \n\n\n\nscores = linear_score(X_train_selected, w) \n\n\n\n\nplt.hist(scores, bins=30, edgecolor='k', alpha=0.7)\nplt.xlabel(r\"Score $s$\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of Computed Scores\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnum_thresholds = 101\nFPR = np.zeros(num_thresholds)\nTPR = np.zeros(num_thresholds)\n\nT = np.linspace(scores.min() - 0.1, scores.max() + 0.1, num_thresholds)\n\nfor i in range(num_thresholds):\n    t = T[i]\n    preds = scores &gt;= t\n    FPR[i] = ((preds == 1) & (y_train == 0)).sum() / (y_train == 0).sum()\n    TPR[i] = ((preds == 1) & (y_train == 1)).sum() / (y_train == 1).sum()\n\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 4))\n\nax.plot(FPR, TPR, color=\"black\")\nax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"grey\")  \nax.set_aspect('equal')\n\nax.set_xlabel(\"False Positive Rate\")\nax.set_ylabel(\"True Positive Rate\")\nax.set_title(\"ROC Curve\")\n\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\ndef profit_repaid(loan_amnt, loan_int_rate):\n    return loan_amnt * (1 + 0.25 * loan_int_rate)**10 - loan_amnt\n\ndef profit_default(loan_amnt, loan_int_rate):\n    return loan_amnt * (1 + 0.25 * loan_int_rate)**3 - 2 * loan_amnt\n\n\n\nprobabilities = model.predict_proba(X_train_scaled)[:, 1]\n\n\nexpected_gains = []\nT = np.linspace(0, 1, 101)  \n\nfor t in T:\n    preds = probabilities &gt;= t  # Prediction based on threshold: whether the loan would be issued\n\n    # Initialize gains and losses\n    total_gain = 0\n\n    # Loop over each loan\n    for amt, rate, pred, actual in zip(X_train['loan_amnt'], X_train['loan_int_rate'], preds, y_train):\n        if pred:  # Loan is predicted to be issued\n            if actual == 1:  # Loan is repaid\n                total_gain += profit_repaid(amt, rate)\n            else:  # Loan defaults\n                total_gain += loss_default(amt, rate)\n\n    # Average gain per loan\n    avg_gain = total_gain / len(X_train)\n    expected_gains.append(avg_gain)\n\n\n\n\nplt.figure(figsize=(10, 6))\nplt.plot(T, expected_gains, label='Expected Gain')\nplt.xlabel(r\"Threshold $t$\")\nplt.ylabel(\"Expected Profit per Loan\")\nplt.title(\"Expected Profit per Loan vs. Threshold\")\nplt.xlim(0, 1)\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\nselected_features = ['loan_amnt', 'loan_int_rate', 'loan_percent_income', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE']\n\n\nfeature_combinations = [\n    ['loan_amnt', 'loan_int_rate'],  \n    ['loan_percent_income', 'person_home_ownership_RENT'],  \n    selected_features  \n]\n\n\nresults = {}\n\nfor features in feature_combinations:\n    # Scale features\n    X_scaled = scaler.fit_transform(df[features])\n    \n    LR = LogisticRegression(max_iter=10000)\n    \n    cv_scores_LR = cross_val_score(LR, X_scaled, y, cv=5)\n    results[str(features)] = cv_scores_LR.mean()\n\nfor combo, score in results.items():\n    print(f\"Features: {combo}\\nCV Score (mean accuracy): {score}\\n\")\n\nFeatures: ['loan_amnt', 'loan_int_rate']\nCV Score (mean accuracy): 0.7973108709793388\n\nFeatures: ['loan_percent_income', 'person_home_ownership_RENT']\nCV Score (mean accuracy): 0.8471203482091736\n\nFeatures: ['loan_amnt', 'loan_int_rate', 'loan_percent_income', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE']\nCV Score (mean accuracy): 0.848342893535451"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\n\nImportError: cannot import name 'Perceptron' from 'source' (/Users/donovanwood/itsdwood.github.io-1/posts/example-blog-post/source.py)\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Warmup Exercise Penquin",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Warmup Exercise Penquin",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "CSCI 0451: Reflective Goal-Setting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\nThis Blog Post will cover the implementation of logistic regression in Python.\n\n\n\n\n\nMay 12, 2024\n\n\nDonovan Wood\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing the Perceptron Algorithm\n\n\n\n\n\nA simple implementation of the Perceptron Algorithm in Python.\n\n\n\n\n\nApr 2, 2024\n\n\nDonovan Wood\n\n\n\n\n\n\n\n\n\n\n\n\nMid-Course Reflection\n\n\n\n\n\nWe reflect on our learning, engagement, and achievement in the first part of the semester. \n\n\n\n\n\nApr 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLimits of the Quantitative Approach to Bias and Fairness\n\n\n\n\n\nMy Blog Post Discussing the Limits of the Quantitative Approach to Bias and Fairness in Machine Learning.\n\n\n\n\n\nMar 27, 2024\n\n\nDonovan Wood\n\n\n\n\n\n\n\n\n\n\n\n\nReplication Study, Dissecting racial bias in an algorithm used to manage the health of populations\n\n\n\n\n\nReplicating the results of the paper ‘Dissecting racial bias in an algorithm used to manage the health of populations’ by Obermeyer et al. (2019).\n\n\n\n\n\nMar 19, 2024\n\n\nDonovan Wood\n\n\n\n\n\n\n\n\n\n\n\n\nThe Women in Data Science (WiDS) Conference at Middlebury College\n\n\n\n\n\nHighlighting the Middlebury Women in Data Science Conference that took place on March 4th, 2024\n\n\n\n\n\nMar 14, 2024\n\n\nDonovan Wood\n\n\n\n\n\n\n\n\n\n\n\n\nWhose Costs?\n\n\n\n\n\nMy second blog post, creating an automated decision system for a hypothetical bank extending credit\n\n\n\n\n\nFeb 28, 2024\n\n\nDonovan Wood\n\n\n\n\n\n\n\n\n\n\n\n\nWhose Costs?\n\n\n\n\n\nMy second blog post, creating an automated decision system for a hypothetical bank extending credit\n\n\n\n\n\nFeb 28, 2024\n\n\nDonovan Wood\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nMy first blog post, detailing my model for classifying Palmer Penguins\n\n\n\n\n\nFeb 19, 2024\n\n\nDonovan Wood\n\n\n\n\n\n\n\n\n\n\n\n\nWarmup Exercise Penquin\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html",
    "href": "posts/New-test-thing-post/index.html",
    "title": "Whose Costs?",
    "section": "",
    "text": "The dataset below will be the one I use for this blog post\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf = pd.read_csv(url)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\n\nFor the dataset, I created two plots and one summary table in order to contextualize the data.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\nplt.figure(figsize=(12, 8))\nsns.scatterplot(x='person_age', y='person_emp_length', hue='loan_intent', data=df, palette='viridis')\nplt.title('Loan Intent vs. Age and Employment Length')\nplt.xlabel('Age')\nplt.ylabel('Employment Length (Years)')\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Loan Intent\n\n\n\n\n\nRegarding Figure 1, we can see some patterns within the dataset among loan intentions.\nMost notably in the bottom left corner of the plot, which indicates both a young age and limited employment history, there is a culmination of both Educational and Venture loans being used.\nThis makes sense given the current context of the world, as young individuals would most likely be in college or other forms of education at this time. On a similar note, venture capital is often used for newly formed or aspiring companies. Hence, the youthful age and lack of employment history match.\nConversely, as age increases, loans for medical, home improvement, and debt consolidation become more likely. This evidence is runs concurrent with today’s societal concerns from our more elderly population.\n\nplt.figure(figsize=(10, 6))\nsns.countplot(x='person_home_ownership', hue='loan_grade', data=df, palette='Set2')\nplt.title('Loan Grade Distribution by Home Ownership')\nplt.xlabel('Home Ownership')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\n\nFigure 2: Loan Grade Distribution by Home Ownership\n\n\n\n\n\nFigure 2 illustrates the Loan grade distribution by home ownership. Loan Grade is viewed A-G with A being the highest loan grade and G being the lowest.\nAn interesting observation to make here is that for individuals in both the ‘Mortgage’ and ‘Home’ groups, the highest frequency of loans is rated as an A whereas for individuals in the ‘Rent’ group the highest frequency of loans is rated as an B.\nThis could give context to the economic state of home ownership and bank trust. Those with a mortgage/own property could perhaps be linked to being more trustworthy due to having a longer term connection with a property. Banks may view those in the ‘Rent’ category as a bit more risky due to the short term ability of renting.\n\nsummary_table = df.groupby('loan_intent').agg({\n    'loan_int_rate': 'mean',\n    'loan_amnt': 'mean'\n}).reset_index()\n\nsummary_table.rename(columns={\n    'loan_int_rate': 'Average Interest Rate',\n    'loan_amnt': 'Average Loan Amount'\n}, inplace=True)\n\nprint(summary_table)\n\n         loan_intent  Average Interest Rate  Average Loan Amount\n0  DEBTCONSOLIDATION              10.983305          9620.901149\n1          EDUCATION              10.965465          9460.015604\n2    HOMEIMPROVEMENT              11.160075         10348.725017\n3            MEDICAL              11.051946          9242.269907\n4           PERSONAL              11.009814          9549.427178\n5            VENTURE              10.940866          9516.417425\n\n\nThis summary table above displays the relative average interest rates and average loan amounts for each category of loan intent.\nHome improvement has the greatest average loan amount while similarly having the highest interest rate. On the flip side, education has the lowest average loan amount but the second lowest interest rate.\n\n\n\nI will be utilizing a Logistic Regression Model for predicting whether a prospective borrower is likely to default on a given loan.\nI will also be utilizing the RFECV class from sklearn in order to automatically find the most useful features to use in the model. RFE is a feature selection method that fits a model and removes the weakest feature(s) until the specified number of features is reached. Using RFE, you can automate the process of finding an effective subset of features.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import Pipeline\n\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_all = pd.read_csv(url)\n\ndf = df_all.drop(['loan_grade', 'loan_status'], axis=1)  \ndf = pd.get_dummies(df, columns=['person_home_ownership', 'loan_intent', 'cb_person_default_on_file'], drop_first=True)\ndf.dropna(inplace=True)\n\ny = df_all.loc[df.index, 'loan_status']  \n\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=123)\n\n# Scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlogreg = LogisticRegression(max_iter=1000)\n\nselector = RFECV(estimator=logreg, step=1, cv=5)\nselector = selector.fit(X_train_scaled, y_train)  # Fitting RFECV to the scaled training data\n\n\nX_train_selected = selector.transform(X_train_scaled)\nX_test_selected = selector.transform(X_test_scaled)\n\nmodel_selected = LogisticRegression(max_iter=1000)\nmodel_selected.fit(X_train_selected, y_train)\n\nselected_features = X_train.columns[selector.support_]\n\n# Prediction\ny_pred_selected = model_selected.predict(X_test_selected)\nprint(f\"Optimal number of features: {selector.n_features_}\")\nprint(classification_report(y_test, y_pred_selected))\n\n\n  \nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=123)\n\n\nX_train\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_Y\n\n\n\n\n7573\n26\n38000\n1.0\n8875\n7.51\n0.23\n2\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n15865\n25\n110004\n9.0\n15000\n7.66\n0.14\n4\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n18729\n29\n60000\n3.0\n7200\n7.88\n0.12\n7\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n1288\n24\n44000\n1.0\n3200\n8.00\n0.07\n2\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n12168\n65\n46000\n5.0\n10500\n16.32\n0.23\n24\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17438\n26\n48000\n10.0\n15000\n11.12\n0.31\n3\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n24566\n45\n118000\n15.0\n25000\n12.18\n0.21\n12\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n20105\n21\n26000\n5.0\n14500\n14.61\n0.56\n2\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\n\n\n17825\n35\n78000\n9.0\n9450\n10.99\n0.12\n8\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n22678\n38\n65000\n5.0\n15000\n10.36\n0.23\n17\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n18325 rows × 16 columns\n\n\n\n\n\n# Scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\nX_train_scaled\n\narray([[-0.26848589, -0.43778172, -0.91860701, ..., -0.45114679,\n        -0.4630992 , -0.46344676],\n       [-0.4252438 ,  0.65831624,  1.01527471, ..., -0.45114679,\n        -0.4630992 , -0.46344676],\n       [ 0.20178785, -0.10288151, -0.43513658, ...,  2.21657344,\n        -0.4630992 , -0.46344676],\n       ...,\n       [-1.05227545, -0.62045457,  0.04833385, ..., -0.45114679,\n        -0.4630992 ,  2.15774517],\n       [ 1.14233533,  0.17112776,  1.01527471, ..., -0.45114679,\n        -0.4630992 , -0.46344676],\n       [ 1.61260907, -0.02676782,  0.04833385, ..., -0.45114679,\n        -0.4630992 , -0.46344676]])\n\n\n\n\nlogreg = LogisticRegression(max_iter=1000)\n\nselector = RFECV(estimator=logreg, step=1, cv=5)\nselector = selector.fit(X_train_scaled, y_train)  # Fitting RFECV to the scaled training data\n\n\nX_train_selected = selector.transform(X_train_scaled)\nX_test_selected = selector.transform(X_test_scaled)\n\n\nX_train_selected\n\narray([[-0.12277222, -1.0852037 ,  0.57659278, ..., -0.49598922,\n        -0.45114679, -0.4630992 ],\n       [ 0.85178574, -1.03888993, -0.27169328, ..., -0.49598922,\n        -0.45114679, -0.4630992 ],\n       [-0.38928398, -0.97096305, -0.4602013 , ..., -0.49598922,\n         2.21657344, -0.4630992 ],\n       ...,\n       [ 0.77222999,  1.10698175,  3.68697501, ..., -0.49598922,\n        -0.45114679, -0.4630992 ],\n       [-0.0312831 , -0.01072407, -0.4602013 , ..., -0.49598922,\n        -0.45114679, -0.4630992 ],\n       [ 0.85178574, -0.20524194,  0.57659278, ...,  2.01617286,\n        -0.45114679, -0.4630992 ]])\n\n\n\nfrom sklearn.metrics import classification_report\n\nmodel_selected = LogisticRegression(max_iter=1000)\nmodel_selected.fit(X_train_selected, y_train)\n\nselected_features = X_train.columns[selector.support_]\n\n# Prediction\ny_pred_selected = model_selected.predict(X_test_selected)\nprint(f\"Optimal number of features: {selector.n_features_}\")\nprint(classification_report(y_test, y_pred_selected))\n\nOptimal number of features: 8\n              precision    recall  f1-score   support\n\n           0       0.87      0.94      0.90      3614\n           1       0.68      0.48      0.56       968\n\n    accuracy                           0.84      4582\n   macro avg       0.77      0.71      0.73      4582\nweighted avg       0.83      0.84      0.83      4582\n\n\n\nThe code below is used to identify which features will be used for the most optimal model prediction.\n\nfeature_names = df.columns\n\nselected_features = feature_names[selector.support_]\n\nprint(\"Selected features:\", selected_features)\n\nSelected features: Index(['loan_amnt', 'loan_int_rate', 'loan_percent_income',\n       'person_home_ownership_OWN', 'person_home_ownership_RENT',\n       'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE'],\n      dtype='object')\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nthresholds = np.linspace(0, 1, 101)\nissued_counts = []\n\nfor threshold in thresholds:\n    X_train['predicted_issued'] = X_train['probabilities'] &gt;= threshold\n    issued_count = X_train['predicted_issued'].sum()  \n    issued_counts.append(issued_count)\n\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, issued_counts)\nplt.title('Number of Loans Predicted to be Issued vs. Threshold')\nplt.xlabel('Threshold')\nplt.ylabel('Number of Loans Predicted to be Issued')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nX_train\n\n\n\n\n\n\n\n\nloan_amnt\nloan_int_rate\nloan_percent_income\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_EDUCATION\nloan_intent_PERSONAL\nloan_intent_VENTURE\nprobabilities\nactual\nprediction_probability\npredicted_issued\ngain_loss\n\n\n\n\n7573\n8875\n7.51\n0.23\nFalse\nFalse\nFalse\nFalse\nFalse\n0.105087\n0\n0.105087\nFalse\n0\n\n\n15865\n15000\n7.66\n0.14\nFalse\nFalse\nFalse\nFalse\nFalse\n0.023507\n0\n0.023507\nFalse\n0\n\n\n18729\n7200\n7.88\n0.12\nFalse\nFalse\nFalse\nTrue\nFalse\n0.022012\n0\n0.022012\nFalse\n0\n\n\n1288\n3200\n8.00\n0.07\nFalse\nTrue\nFalse\nFalse\nFalse\n0.061475\n0\n0.061475\nFalse\n0\n\n\n12168\n10500\n16.32\n0.23\nFalse\nTrue\nFalse\nFalse\nTrue\n0.600637\n0\n0.600637\nFalse\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17438\n15000\n11.12\n0.31\nTrue\nFalse\nFalse\nFalse\nTrue\n0.047207\n0\n0.047207\nFalse\n0\n\n\n24566\n25000\n12.18\n0.21\nFalse\nFalse\nFalse\nFalse\nFalse\n0.097552\n0\n0.097552\nFalse\n0\n\n\n20105\n14500\n14.61\n0.56\nFalse\nFalse\nFalse\nFalse\nFalse\n0.978011\n1\n0.978011\nFalse\n0\n\n\n17825\n9450\n10.99\n0.12\nFalse\nFalse\nFalse\nFalse\nFalse\n0.083158\n0\n0.083158\nFalse\n0\n\n\n22678\n15000\n10.36\n0.23\nFalse\nFalse\nTrue\nFalse\nFalse\n0.078296\n0\n0.078296\nFalse\n0\n\n\n\n\n18325 rows × 13 columns\n\n\n\n\n# Estimating the profit here into rows \n\nthresholds = np.linspace(0, 1, 101)\naverage_gains = []\n\nfor threshold in thresholds:\n    X_train['predicted_issued'] = X_train['prediction_probability'] &gt;= threshold\n    X_train['gain_loss'] = X_train.apply(calculate_gain_loss, axis=1)\n    average_gain_loss_per_issued_loan = X_train.loc[X_train['predicted_issued'], 'gain_loss'].mean()\n    average_gains.append(average_gain_loss_per_issued_loan)\n\n# Plotting the average gain per issued loan across thresholds\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, average_gains, label='Average Gain Per Issued Loan')\nplt.xlabel('Threshold')\nplt.ylabel('Average Gain Per Issued Loan')\nplt.title('Average Gain Per Issued Loan vs. Threshold')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\nX_train\n\n\n\n\n\n\n\n\nloan_amnt\nloan_int_rate\nloan_percent_income\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_EDUCATION\nloan_intent_PERSONAL\nloan_intent_VENTURE\nprobabilities\nactual\nprediction_probability\npredicted_issued\ngain_loss\n\n\n\n\n7573\n8875\n7.51\n0.23\nFalse\nFalse\nFalse\nFalse\nFalse\n0.105087\n0\n0.105087\nFalse\n0\n\n\n15865\n15000\n7.66\n0.14\nFalse\nFalse\nFalse\nFalse\nFalse\n0.023507\n0\n0.023507\nFalse\n0\n\n\n18729\n7200\n7.88\n0.12\nFalse\nFalse\nFalse\nTrue\nFalse\n0.022012\n0\n0.022012\nFalse\n0\n\n\n1288\n3200\n8.00\n0.07\nFalse\nTrue\nFalse\nFalse\nFalse\n0.061475\n0\n0.061475\nFalse\n0\n\n\n12168\n10500\n16.32\n0.23\nFalse\nTrue\nFalse\nFalse\nTrue\n0.600637\n0\n0.600637\nFalse\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17438\n15000\n11.12\n0.31\nTrue\nFalse\nFalse\nFalse\nTrue\n0.047207\n0\n0.047207\nFalse\n0\n\n\n24566\n25000\n12.18\n0.21\nFalse\nFalse\nFalse\nFalse\nFalse\n0.097552\n0\n0.097552\nFalse\n0\n\n\n20105\n14500\n14.61\n0.56\nFalse\nFalse\nFalse\nFalse\nFalse\n0.978011\n1\n0.978011\nFalse\n0\n\n\n17825\n9450\n10.99\n0.12\nFalse\nFalse\nFalse\nFalse\nFalse\n0.083158\n0\n0.083158\nFalse\n0\n\n\n22678\n15000\n10.36\n0.23\nFalse\nFalse\nTrue\nFalse\nFalse\n0.078296\n0\n0.078296\nFalse\n0\n\n\n\n\n18325 rows × 13 columns\n\n\n\n\n\n\nI did not want to limit myself to only one model however, so I used cross_val_score from the sklearn.model_selection in order to compare how my ‘Optimal’ LR model did against other LR models with a different combination of features.\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\nselected_features = ['loan_amnt', 'loan_int_rate', 'loan_percent_income', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE']\n\n\nfeature_combinations = [\n    ['loan_amnt', 'loan_int_rate'],  \n    ['loan_percent_income', 'person_home_ownership_RENT'],  \n    selected_features  \n]\n\n\nresults = {}\n\nfor features in feature_combinations:\n    # Scale features\n    X_scaled = scaler.fit_transform(df[features])\n    \n    LR = LogisticRegression(max_iter=10000)\n    \n    cv_scores_LR = cross_val_score(LR, X_scaled, y, cv=5)\n    results[str(features)] = cv_scores_LR.mean()\n\nfor combo, score in results.items():\n    print(f\"Features: {combo}\\nCV Score (mean accuracy): {score}\\n\")\n\nFeatures: ['loan_amnt', 'loan_int_rate']\nCV Score (mean accuracy): 0.7973108709793388\n\nFeatures: ['loan_percent_income', 'person_home_ownership_RENT']\nCV Score (mean accuracy): 0.8471203482091736\n\nFeatures: ['loan_amnt', 'loan_int_rate', 'loan_percent_income', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE']\nCV Score (mean accuracy): 0.848342893535451\n\n\n\nAs seen above, the ‘Optimal’ feature LR model performed the best in predicting the likelihood of a default.\n\n\n\nIn this section, we will be exploring how I came up with the threshold t that will be used for our model.\nI used the numpy package to compute linear scores across all n of our training points.\nI plotted the scores below in order to easily visualize this process. By gathering scores, we can easily simulate decision-making with a given threshold.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef linear_score(X, w):\n    return X @ w\n\n\n# Extracting weights \nw = model.coef_.flatten()  \n\n\n\nscores = linear_score(X_train_selected, w) \n\n\nplt.hist(scores, bins=30, edgecolor='k', alpha=0.7)\nplt.xlabel(r\"Score $s$\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of Computed Scores\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure 3: Histogram of Computed Scores"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#classifying-the-palmer-penguins",
    "href": "posts/New-test-thing-post/index.html#classifying-the-palmer-penguins",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Image source: @allisonhorst\n\n\nOur data set for these notes is Palmer Penguins. This data set contains physiological measurements and species labels for several populations of Adelie, Chinstrap, and Gentoo penguins."
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#abstract",
    "href": "posts/New-test-thing-post/index.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "Abstract",
    "text": "Abstract\nIn this blog post we use a logistic regression model in order to classify Palmer penguins according to their Species. We find that using information such as the Island of the penguin as well as Culmen Depth (mm) and Culmen Length are sufficient features in order to provide an accurate classifying model. Within this blog post, we examine multiple data visualizations and tables in order to better understand the data and develop our reasoning for choosing the features of the model.\nBelow we will import the dataset we will use for this Blog Post\n\nimport pandas as pd\nimport numpy as np\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#data-preparation",
    "href": "posts/New-test-thing-post/index.html#data-preparation",
    "title": "Classifying Palmer Penguins",
    "section": "Data Preparation",
    "text": "Data Preparation\nIn order to prepare the qualitative columns in the data, we must convert categorical feature columns like Sex and Island into 0-1 columns using pd.get_dummies function.\nThe label column Species will also needed to be coded differently, using a LabelEncoder\nThe following function will take care of these processes:\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n# Replace the column with the first word in each entry \ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#visualizations-of-the-data",
    "href": "posts/New-test-thing-post/index.html#visualizations-of-the-data",
    "title": "Whose Costs?",
    "section": "",
    "text": "For the dataset, I created two plots and one summary table in order to contextualize the data.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\nplt.figure(figsize=(12, 8))\nsns.scatterplot(x='person_age', y='person_emp_length', hue='loan_intent', data=df, palette='viridis')\nplt.title('Loan Intent vs. Age and Employment Length')\nplt.xlabel('Age')\nplt.ylabel('Employment Length (Years)')\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Loan Intent\n\n\n\n\n\nRegarding Figure 1, we can see some patterns within the dataset among loan intentions.\nMost notably in the bottom left corner of the plot, which indicates both a young age and limited employment history, there is a culmination of both Educational and Venture loans being used.\nThis makes sense given the current context of the world, as young individuals would most likely be in college or other forms of education at this time. On a similar note, venture capital is often used for newly formed or aspiring companies. Hence, the youthful age and lack of employment history match.\nConversely, as age increases, loans for medical, home improvement, and debt consolidation become more likely. This evidence is runs concurrent with today’s societal concerns from our more elderly population.\n\nplt.figure(figsize=(10, 6))\nsns.countplot(x='person_home_ownership', hue='loan_grade', data=df, palette='Set2')\nplt.title('Loan Grade Distribution by Home Ownership')\nplt.xlabel('Home Ownership')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\n\nFigure 2: Loan Grade Distribution by Home Ownership\n\n\n\n\n\nFigure 2 illustrates the Loan grade distribution by home ownership. Loan Grade is viewed A-G with A being the highest loan grade and G being the lowest.\nAn interesting observation to make here is that for individuals in both the ‘Mortgage’ and ‘Home’ groups, the highest frequency of loans is rated as an A whereas for individuals in the ‘Rent’ group the highest frequency of loans is rated as an B.\nThis could give context to the economic state of home ownership and bank trust. Those with a mortgage/own property could perhaps be linked to being more trustworthy due to having a longer term connection with a property. Banks may view those in the ‘Rent’ category as a bit more risky due to the short term ability of renting.\n\nsummary_table = df.groupby('loan_intent').agg({\n    'loan_int_rate': 'mean',\n    'loan_amnt': 'mean'\n}).reset_index()\n\nsummary_table.rename(columns={\n    'loan_int_rate': 'Average Interest Rate',\n    'loan_amnt': 'Average Loan Amount'\n}, inplace=True)\n\nprint(summary_table)\n\n         loan_intent  Average Interest Rate  Average Loan Amount\n0  DEBTCONSOLIDATION              10.983305          9620.901149\n1          EDUCATION              10.965465          9460.015604\n2    HOMEIMPROVEMENT              11.160075         10348.725017\n3            MEDICAL              11.051946          9242.269907\n4           PERSONAL              11.009814          9549.427178\n5            VENTURE              10.940866          9516.417425\n\n\nThis summary table above displays the relative average interest rates and average loan amounts for each category of loan intent.\nHome improvement has the greatest average loan amount while similarly having the highest interest rate. On the flip side, education has the lowest average loan amount but the second lowest interest rate."
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#model-choices",
    "href": "posts/New-test-thing-post/index.html#model-choices",
    "title": "Classifying Palmer Penguins",
    "section": "Model Choices",
    "text": "Model Choices\nFor this blog, I used the combinations function from the itertools package in order to go through each variation of both qualitative and quantitative features.\n\nfrom itertools import combinations\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Region\", ]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n\nI then utilized the LoigisticRegression model from sklearn in order to run tests for each iteration.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\n\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\nLR = LogisticRegression(max_iter=10000)\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n0.99609375\n\n\nGoing through each iteration, I scored each of them found below:\n[‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Culmen Length (mm)’, ‘Body Mass (g)’] : .91  [‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Culmen Depth (mm)’, ‘Flipper Length (mm)’]: .91  [‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Culmen Depth (mm)’, ‘Body Mass (g)’] : .76  [‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Flipper Length (mm)’, ‘Body Mass (g)’] : .63  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, ‘Culmen Depth (mm)’] : .992  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, ‘Flipper Length (mm)’] : .98  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, ‘Body Mass (g)’] : .98  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Depth (mm)’, ‘Flipper Length (mm)’] : .82  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Depth (mm)’, ‘Body Mass (g)’] : .77  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Flipper Length (mm)’, ‘Body Mass (g)’] : .71  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Length (mm)’, ‘Culmen Depth (mm)’] : .996  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Length (mm)’, ‘Flipper Length (mm)’]: .97  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Length (mm)’, ‘Body Mass (g)’]: .97  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Depth (mm)’, ‘Flipper Length (mm)’]:.88  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Depth (mm)’, ‘Body Mass (g)’]: .83  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Flipper Length (mm)’, ‘Body Mass (g)’] : .75"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#testing",
    "href": "posts/New-test-thing-post/index.html#testing",
    "title": "Classifying Palmer Penguins",
    "section": "Testing",
    "text": "Testing\nIn order to test the model, I am downloading the dataset and preparing it via the prepare_data function\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n1.0\n\n\nAwesome! Our model is able to classify the three species of penguins with 100 percent accuracy.\n\n\n\n\n\n\nFigure 3: Image"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#plotting-decision-regions",
    "href": "posts/New-test-thing-post/index.html#plotting-decision-regions",
    "title": "Classifying Palmer Penguins",
    "section": "PLotting Decision Regions",
    "text": "PLotting Decision Regions\nI will be using the plot_regions function below in order to plot my decision regions for my model.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Gentoo\", \"Chinstrap\", \"Adelie\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nFor the purposes of this blog post, we will look at the decision regions for both our training data and our testing data\n\nplot_regions(LR, X_train[cols], y_train)\n\n\n\n\n\n\n\nFigure 4: Decision Regions for each island based on training data\n\n\n\n\n\nThe three plots above correspond to the decision regions for each island. As previously mentioned in the data visualization section, we see that for the Biscoe and Dream islands, there are only two species of penguins. For the Torgersen island, only the Adelie penguins reside there. As predicted, this results in comparing at most only two species of penguins against one another, allowing for easier classification.\nThe following section displays the decision regions our model makes for the testing data.\n\nplot_regions(LR, X_test[cols], y_test)\n\n\n\n\n\n\n\nFigure 5: Decision Regions for each island based on testing data\n\n\n\n\n\nOur testing data yielded a smaller number of penguins to be classified, but our classifier was able to accurately predict each species of penguin based on our three features of Island, Culmen Depth (mm), and Culmen Length (mm)\nWhile the penguins on the Torgersen would always be Adelie, and were classified as such. The closest decision region our model came to for classification of the test data was a penguin on the Dream Island. This was anticipated however as in Figure 2, Adelie and Chinstrap penguins share some overlapping points in regards to Culmen Depth (mm) and Culmen Length(mm)"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#confusion-matrix",
    "href": "posts/New-test-thing-post/index.html#confusion-matrix",
    "title": "Classifying Palmer Penguins",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nI will be using a confusion matrix for my model, evaluated on the test set.\nIn order to do this I will be using confusion_matrix from the sklearn package.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test[cols])\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nIn this confusion matrix, due to our 100 percent prediction accuracy, we do not see any errors within our matrix. As a reference, the confusion matrix just gives the number of data points that have the correct label.\nLogically, since our model predicts each penguin correctly, there will be disparities in the matrix.\nThat being said, as previously mentioned, the most likely error to occur within our model is classifying a penguin from the Dream island. This is due to the Adelie and Chinstrap penguins sharing the most similar distributions of Culmen Depth (mm) and Culmen Length (mm) which depending on the penguin’s dimensions, has the highest likelihood out of all the species to overlap in the incorrect species decision region."
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#discussion",
    "href": "posts/New-test-thing-post/index.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Discussion",
    "text": "Discussion\nIn this blog post, I have found that the using Island, Culmen Depth (mm), and Culmen Length (mm) features in a logistic regression model are useful for accurately classifying species of penguins pertaining to Adelie, Gentoo, and Chinstrap.\nI have found that Adelie penguins reside in all three islands (Dream, Biscoe, Torgersen), while Gentoo penguins only reside on the Biscoe island and Chinstrap penguins only inhabit the Biscoe island.\nIn terms of Culmen Length (mm) and Culmen Depth (mm) we found that:\nGentoo penguins seem to exhbit a longer culmen length but shorter culmen depth.  Adelie penguins have a shorter culmen length compared to Gentoo penguins but a higher culmen depth.  Chinstrap penguins appear to have the longest average combination of culmen depth/length out of all the penguins.\nIn terms of Body Mass (g) we found that:\nGentoo penguins, both male and female, have the highest mean body mass out of all species of penguins.\nAdelie and Chinstrap penguins exhibit similar mean body mass to one another for both male and female."
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html",
    "href": "posts/classifying-penguin-post/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Image source: @allisonhorst\n\n\nOur data set for these notes is Palmer Penguins. This data set contains physiological measurements and species labels for several populations of Adelie, Chinstrap, and Gentoo penguins."
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#classifying-the-palmer-penguins",
    "href": "posts/classifying-penguin-post/index.html#classifying-the-palmer-penguins",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Image source: @allisonhorst\n\n\nOur data set for these notes is Palmer Penguins. This data set contains physiological measurements and species labels for several populations of Adelie, Chinstrap, and Gentoo penguins."
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#abstract",
    "href": "posts/classifying-penguin-post/index.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "Abstract",
    "text": "Abstract\nIn this blog post we use a logistic regression model in order to classify Palmer penguins according to their Species. We find that using information such as the Island of the penguin as well as Culmen Depth (mm) and Culmen Length are sufficient features in order to provide an accurate classifying model. Within this blog post, we examine multiple data visualizations and tables in order to better understand the data and develop our reasoning for choosing the features of the model.\nBelow we will import the dataset we will use for this Blog Post\n\nimport pandas as pd\nimport numpy as np\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)"
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#data-preparation",
    "href": "posts/classifying-penguin-post/index.html#data-preparation",
    "title": "Classifying Palmer Penguins",
    "section": "Data Preparation",
    "text": "Data Preparation\nIn order to prepare the qualitative columns in the data, we must convert categorical feature columns like Sex and Island into 0-1 columns using pd.get_dummies function.\nThe label column Species will also needed to be coded differently, using a LabelEncoder\nThe following function will take care of these processes:\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n# Replace the column with the first word in each entry \ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)"
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#visualizations-of-the-data",
    "href": "posts/classifying-penguin-post/index.html#visualizations-of-the-data",
    "title": "Classifying Palmer Penguins",
    "section": "Visualizations of the Data",
    "text": "Visualizations of the Data\nFor the dataset, I created two plots and one summary table in order to contextualize the data.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\n# Create a bar plot\nplt.figure(figsize=(10, 6))\nsns.countplot(data=train, x='Island', hue='Species')\n\n# Customize the plot\nplt.title('Island Distribution for Each Penguin Species')\nplt.xlabel('Island')\nplt.ylabel('Count')\nplt.legend(title='Species')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Island Distribution for Each Penguin Species\n\n\n\n\n\nAfter running through a multitude of different plots with varying sets of data, one combination that particularly stood out to me is shown in Figure 1. The plot shows the distribution of each species of Penguin on the three islands within the dataset. Now while it seems rather plain and simple at first glance, take note that for one of these islands (Torgersen), only one species of penguins resides there, that being Adeilie. Upon further investigation, for the remaining two islands, at most, only two differing penguin species reside there. Adelie inhabit all three islands while Chinstrap penguins reside only on the Dream island and Gentoo Penguins are only found on the Biscoe Island.\nThe reason as to why this rather simple analysis is so interesting as it presents a way in which we no longer have to compare all three species against one another at a given point. Rather, we can now determine whether the penguin presented to us is one of two species if the island is Dream or Biscoe. In the case of Torgersen, we automatically know the penguin will be Adelie as they are the only species of Penguins that reside there!\nThis knowledge, paired with two more quantitative features, should allow the model to classify a penguin more accurately!\n\nsns.scatterplot(train, x = \"Culmen Depth (mm)\", y = \"Culmen Length (mm)\", hue = \"Species\", style = \"Species\")\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nWith that idea now set. Let’s move on to the second visualization, shown by Figure 2. This is a more traditional scatterplot showing the distributions of Culmen_Depth and Culmen_Length for all three species of penguin.\nNow recalling ?@fig-scatter-island, the main relationships we want to focus on are Chinstrap vs Adelie and Gentoo vs Adelie as these are the only relationships where penguins coexist on the same island.\nAs we can see from the Figure 2. Gentoo penguins seem to exhbit a longer culmen length but shorter culmen depth.  Adelie penguins have a shorter culmen length compared to Gentoo penguins but a higher culmen depth.  Chinstrap penguins appear to have the longest average combination of culmen depth/length out of all the penguins.\nReferring back to those relationships as mentioned previously, Gentoo and Adelie penguins seem to have a clear divide amongst one another in terms of culmen length/depth. Chinstrap and Adelie penguins seem to overlap at some points in the graph but fairly slightly.\nAll things considered, given the properties of both ?@fig-scatter-island and Figure 2, the combination of these three features will be useful as a starting reference for my model.\n\n## Summary table for the mean body mass of each species of penguin by sex\n\nsummary_table = train.groupby(['Species', 'Sex']).agg({'Body Mass (g)': 'mean'})\n\nprint(summary_table)\n\n                  Body Mass (g)\nSpecies   Sex                  \nAdelie    FEMALE    3350.471698\n          MALE      4052.868852\nChinstrap FEMALE    3523.387097\n          MALE      4005.769231\nGentoo    .         4875.000000\n          FEMALE    4684.693878\n          MALE      5476.704545\n\n\nMy last visualization was a simple summary table of the mean body mass (g) of each penguin species by sex. I wanted to ensure that I was covering a large portion of the data, so by doing so, I can cover two features of the dataset I did not cover in the two previous data plots.\nAs a quick analysis:  Gentoo Males exhibit the largest mean body mass (g) out of all the male species  Similarly Gentoo females also exhibit the largest mean body mass (g) out of all the female species  In regards to Adelie and Chinstrap penguins, both males and females exhibit a similar mean mass (g) in relation to the other species.\nWith that in mind, the combination of body mass (g) and sex do not seem like a good pairing to use for predicition due to the similar nature of both Adelie and Chinstrap penguins."
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#model-choices",
    "href": "posts/classifying-penguin-post/index.html#model-choices",
    "title": "Classifying Palmer Penguins",
    "section": "Model Choices",
    "text": "Model Choices\nFor this blog, I used the combinations function from the itertools package in order to go through each variation of both qualitative and quantitative features.\n\nfrom itertools import combinations\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Region\", ]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n\nI then utilized the LoigisticRegression model from sklearn in order to run tests for each iteration.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\n\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\nLR = LogisticRegression(max_iter=10000)\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n0.99609375\n\n\nGoing through each iteration, I scored each of them found below:\n[‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Culmen Length (mm)’, ‘Body Mass (g)’] : .91  [‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Culmen Depth (mm)’, ‘Flipper Length (mm)’]: .91  [‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Culmen Depth (mm)’, ‘Body Mass (g)’] : .76  [‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Flipper Length (mm)’, ‘Body Mass (g)’] : .63  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, ‘Culmen Depth (mm)’] : .992  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, ‘Flipper Length (mm)’] : .98  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, ‘Body Mass (g)’] : .98  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Depth (mm)’, ‘Flipper Length (mm)’] : .82  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Depth (mm)’, ‘Body Mass (g)’] : .77  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Flipper Length (mm)’, ‘Body Mass (g)’] : .71  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Length (mm)’, ‘Culmen Depth (mm)’] : .996  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Length (mm)’, ‘Flipper Length (mm)’]: .97  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Length (mm)’, ‘Body Mass (g)’]: .97  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Depth (mm)’, ‘Flipper Length (mm)’]:.88  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Depth (mm)’, ‘Body Mass (g)’]: .83  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Flipper Length (mm)’, ‘Body Mass (g)’] : .75"
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#testing",
    "href": "posts/classifying-penguin-post/index.html#testing",
    "title": "Classifying Palmer Penguins",
    "section": "Testing",
    "text": "Testing\nIn order to test the model, I am downloading the dataset and preparing it via the prepare_data function\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n1.0\n\n\nAwesome! Our model is able to classify the three species of penguins with 100 percent accuracy.\n\n\n\n\n\n\nFigure 3: Happy Penguins"
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#plotting-decision-regions",
    "href": "posts/classifying-penguin-post/index.html#plotting-decision-regions",
    "title": "Classifying Palmer Penguins",
    "section": "PLotting Decision Regions",
    "text": "PLotting Decision Regions\nI will be using the plot_regions function below in order to plot my decision regions for my model.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Gentoo\", \"Chinstrap\", \"Adelie\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nFor the purposes of this blog post, we will look at the decision regions for both our training data and our testing data\n\nplot_regions(LR, X_train[cols], y_train)\n\n\n\n\n\n\n\nFigure 4: Decision Regions for each island based on training data\n\n\n\n\n\nThe three plots above correspond to the decision regions for each island. As previously mentioned in the data visualization section, we see that for the Biscoe and Dream islands, there are only two species of penguins. For the Torgersen island, only the Adelie penguins reside there. As predicted, this results in comparing at most only two species of penguins against one another, allowing for easier classification.\nThe following section displays the decision regions our model makes for the testing data.\n\nplot_regions(LR, X_test[cols], y_test)\n\n\n\n\n\n\n\nFigure 5: Decision Regions for each island based on testing data\n\n\n\n\n\nOur testing data yielded a smaller number of penguins to be classified, but our classifier was able to accurately predict each species of penguin based on our three features of Island, Culmen Depth (mm), and Culmen Length (mm)\nWhile the penguins on the Torgersen would always be Adelie, and were classified as such. The closest decision region our model came to for classification of the test data was a penguin on the Dream Island. This was anticipated however as in Figure 2, Adelie and Chinstrap penguins share some overlapping points in regards to Culmen Depth (mm) and Culmen Length(mm)"
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#confusion-matrix",
    "href": "posts/classifying-penguin-post/index.html#confusion-matrix",
    "title": "Classifying Palmer Penguins",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nI will be using a confusion matrix for my model, evaluated on the test set.\nIn order to do this I will be using confusion_matrix from the sklearn package.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test[cols])\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nIn this confusion matrix, due to our 100 percent prediction accuracy, we do not see any errors within our matrix. As a reference, the confusion matrix just gives the number of data points that have the correct label.\nLogically, since our model predicts each penguin correctly, there will be disparities in the matrix.\nThat being said, as previously mentioned, the most likely error to occur within our model is classifying a penguin from the Dream island. This is due to the Adelie and Chinstrap penguins sharing the most similar distributions of Culmen Depth (mm) and Culmen Length (mm) which depending on the penguin’s dimensions, has the highest likelihood out of all the species to overlap in the incorrect species decision region."
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#discussion",
    "href": "posts/classifying-penguin-post/index.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Discussion",
    "text": "Discussion\nIn this blog post, I have found that the using Island, Culmen Depth (mm), and Culmen Length (mm) features in a logistic regression model are useful for accurately classifying species of penguins pertaining to Adelie, Gentoo, and Chinstrap as seen in Figure 5\nI have found that Adelie penguins reside in all three islands (Dream, Biscoe, Torgersen), while Gentoo penguins only reside on the Biscoe island and Chinstrap penguins only inhabit the Biscoe island.\nIn terms of Culmen Length (mm) and Culmen Depth (mm) we found that:\nGentoo penguins seem to exhbit a longer culmen length but shorter culmen depth.  Adelie penguins have a shorter culmen length compared to Gentoo penguins but a higher culmen depth.  Chinstrap penguins appear to have the longest average combination of culmen depth/length out of all the penguins.\nIn terms of Body Mass (g) we found that:\nGentoo penguins, both male and female, have the highest mean body mass out of all species of penguins.\nAdelie and Chinstrap penguins exhibit similar mean body mass to one another for both male and female."
  },
  {
    "objectID": "posts/warmup-exercise-penquin/Palmer-Penguins-Warmup.html",
    "href": "posts/warmup-exercise-penquin/Palmer-Penguins-Warmup.html",
    "title": "Donovan's Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/palmer-penguins.csv\"\ndf = pd.read_csv(url)\n\ndf\n\nsummary_table = df.groupby(['Species', 'Sex']).agg({'Body Mass (g)': 'mean'})\n\nprint(summary_table)\n\n# Create a scatterplot\nsns.scatterplot(data=df, x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", hue=\"Species\")\n\n                                                  Body Mass (g)\nSpecies                                   Sex                  \nAdelie Penguin (Pygoscelis adeliae)       FEMALE    3368.835616\n                                          MALE      4043.493151\nChinstrap penguin (Pygoscelis antarctica) FEMALE    3527.205882\n                                          MALE      3938.970588\nGentoo penguin (Pygoscelis papua)         .         4875.000000\n                                          FEMALE    4679.741379\n                                          MALE      5484.836066"
  },
  {
    "objectID": "posts/warmup-exercise-penquin/index.html",
    "href": "posts/warmup-exercise-penquin/index.html",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "",
    "text": "In this blog post, we explore the pivotal role of women in the fields of computing, math, and engineering, and the significant impact of conferences like WiDS (Women in Data Science) in fostering change. We delve into the historical context of women’s participation in computing, highlight the current challenges and barriers they face, and celebrate the achievements of notable women scholars who are pushing the boundaries of data science and its application in diverse fields. Through the insights from Professors Amy Yuen, Jessica L’Roe, Laura Biester, and Sarah Brown, we learn about the transformative power of data science in political science, geography, mental health, and the quest for fairness in machine learning. This post aims to illuminate the importance of diversity in STEM and inspire action towards a more inclusive future."
  },
  {
    "objectID": "posts/warmup-exercise-penquin/index.html#abstract",
    "href": "posts/warmup-exercise-penquin/index.html#abstract",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "",
    "text": "In this blog post, we explore the pivotal role of women in the fields of computing, math, and engineering, and the significant impact of conferences like WiDS (Women in Data Science) in fostering change. We delve into the historical context of women’s participation in computing, highlight the current challenges and barriers they face, and celebrate the achievements of notable women scholars who are pushing the boundaries of data science and its application in diverse fields. Through the insights from Professors Amy Yuen, Jessica L’Roe, Laura Biester, and Sarah Brown, we learn about the transformative power of data science in political science, geography, mental health, and the quest for fairness in machine learning. This post aims to illuminate the importance of diversity in STEM and inspire action towards a more inclusive future."
  },
  {
    "objectID": "posts/warmup-exercise-penquin/index.html#part-1-why-spotlight-women-in-data-science",
    "href": "posts/warmup-exercise-penquin/index.html#part-1-why-spotlight-women-in-data-science",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Part 1: Why Spotlight Women in Data Science?",
    "text": "Part 1: Why Spotlight Women in Data Science?"
  },
  {
    "objectID": "posts/warmup-exercise-penquin/index.html#professor-amy-yuen",
    "href": "posts/warmup-exercise-penquin/index.html#professor-amy-yuen",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Professor Amy Yuen",
    "text": "Professor Amy Yuen\nProfessor Amy Yuen is a Professor of Political Science at Middlebury College. Her research focused on the United Nations Security Council and aimed to answer if it is truly a democratic institution. Within the United Nations Security Council, elected members do not have veto power, Professor Yuen hoped to answer whether these members continue to run if this is the case, and if there is a lack of representation within the council. In her research, she measured the number of resolutions, formal meetings and consultations, presidential statements, resolution Co-sponsorships, and Aria-Formula Meetings. She found that sponsorships showed the biggest differences in who is doing what. Another aspect of her research was investigating the members that sit on the council, and the frequency of which. She found that the data showed that countries were geared more towards inclusion over time. She concluded by finding that while the council was not entirely representative, it could be worse. She plans to continue with this research to see how this finding plays into the output and effectiveness of the council. From her talk, I learned how data science can be applied to essentially any aspect of our lives. Before this talk, data science seemed strictly technical and I would have never thought about how it can be used in the context of politics and policy. The talk opened my eyes to the realm of possibilities that data science presents in so many varying fields."
  },
  {
    "objectID": "posts/warmup-exercise-penquin/index.html#professor-jessica-lroe",
    "href": "posts/warmup-exercise-penquin/index.html#professor-jessica-lroe",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Professor Jessica L’Roe",
    "text": "Professor Jessica L’Roe\nProfessor Jessica L’Roe is an assistant professor of Geography at Middlebury College. Professor Roe spoke about her research into monitoring change in tropical forest landscapes within East Africa. Within her research, she found that mothers were more willing to invest in education rather than land because it was a better bet for the success of their children. Although her research was an important crux of her talk, one of the main messages she wanted to convey was that all kinds of things can be “data”. Regardless of whether something is abstract or physical, it can often be used to analyze certain behaviors or outcomes. She concluded her talk by emphasizing how women are making large contributions in her field and to just “Go for it”! After listening to her talk, I learned the importance of varying types of data and utilization, but more importantly how it was collected. Her data was collected using the help of locals and traveling from settlement to settlement. It was so interesting to hear this type of data collection as before this, most of the data collection I was used to was simply clicking on a link or downloading a dataset. It showed me that data science can transcend the keyboard and has real implications for REAL people."
  },
  {
    "objectID": "posts/warmup-exercise-penquin/index.html#professor-laura-biester",
    "href": "posts/warmup-exercise-penquin/index.html#professor-laura-biester",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Professor Laura Biester",
    "text": "Professor Laura Biester\nProfessor Laura Biester is a Computer Science Professor at Middlebury College. Professor Biester spoke about her research regarding the connection between language and mental health. She used a corpus of Reddit posts and comments from 2006-2019 to predict whether or not language could be used to identify those who were diagnosed with depression before their diagnosis. Through multiple models such as logistic regression and MentalBeRT, she was able to successfully produce a model that was relatively accurate in predicting depression in users using only language. Like Professor L’Roe’s talk, although much of Professor Biester’s talk was focused on her research, a huge point she emphasized was that data collection is a huge part of the data science process - it’s not just about building models. This part struck true to me as I learned that no matter how good your model is, it is only as good as the data it is being supplied with. Data preprocessing and determining which parts of data to use or not use is crucial in the data science process."
  },
  {
    "objectID": "posts/warmup-exercise-penquin/index.html#professor-sarah-brown",
    "href": "posts/warmup-exercise-penquin/index.html#professor-sarah-brown",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Professor Sarah Brown",
    "text": "Professor Sarah Brown\nProfessor Sarah Brown is an assistant professor of Computer Science at the University of Rhode Island. In her talk, her research focused on how to make machine learning more fair. More broadly, however, she discussed the implications of data science through the use of “keys”. These keys were: Disciplines are communities, meet people where they are, and understand data’s context. She mentioned three projects and the “lock” for each, which is something she needed to figure out for it all to work and flow.\nThese keys were used to unlock these locks and culminate a successful data science process. The keys were picked up often from something that had no explicit relation to data science itself but were collected from other aspects of her life. For example, one of the skills she picked up from social studies was the ability to use context to understand primary sources. She mentions that oftentimes, data scientists fall into the trap of only thinking like a data scientist and approaching the problem from one particular angle. By using other disciplines/individuals’ perspectives, you may be able to figure out a problem in a more efficient/effective way than you would have otherwise. She concluded by stressing the importance of these three keys and emphasizing the idea of understanding “why” rather than just “doing” when it comes to data science. Through this, I learned key principles that will influence the future of my computer science journey."
  },
  {
    "objectID": "posts/warmup-exercise-penquin/index.html#conclusion",
    "href": "posts/warmup-exercise-penquin/index.html#conclusion",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Conclusion",
    "text": "Conclusion\nThis exploration into the status of women in computing and the supportive role of initiatives like WiDS has been enlightening. It has underscored the importance of diversity for innovation and the vast potential of data science to impact various aspects of society positively. From the historical context of women’s evolving role in computing to the cutting-edge research conducted by leading female data scientists, this journey has highlighted both the challenges and the incredible opportunities that lie ahead. As we look forward to a more inclusive and equitable future in STEM, the lessons learned here inspire continued advocacy and action. My next steps involve deepening my understanding of how to support diversity in tech and exploring more about how data science can be leveraged for social good."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#visualizations-of-the-data",
    "href": "posts/new-new-test-post/index.html#visualizations-of-the-data",
    "title": "Whose Costs?",
    "section": "",
    "text": "For the dataset, I created two plots and one summary table in order to contextualize the data.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\nplt.figure(figsize=(12, 8))\nsns.scatterplot(x='person_age', y='person_emp_length', hue='loan_intent', data=df, palette='viridis')\nplt.title('Loan Intent vs. Age and Employment Length')\nplt.xlabel('Age')\nplt.ylabel('Employment Length (Years)')\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Loan Intent\n\n\n\n\n\n\nplt.figure(figsize=(10, 6))\nsns.countplot(x='person_home_ownership', hue='loan_grade', data=df, palette='Set2')\nplt.title('Loan Grade Distribution by Home Ownership')\nplt.xlabel('Home Ownership')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\n\nFigure 2: Loan Grade Distribution by Home Ownership\n\n\n\n\n\n\nsummary_table = df.groupby('loan_intent').agg({\n    'loan_int_rate': 'mean',\n    'loan_amnt': 'mean'\n}).reset_index()\n\nsummary_table.rename(columns={\n    'loan_int_rate': 'Average Interest Rate',\n    'loan_amnt': 'Average Loan Amount'\n}, inplace=True)\n\nprint(summary_table)\n\n         loan_intent  Average Interest Rate  Average Loan Amount\n0  DEBTCONSOLIDATION              10.983305          9620.901149\n1          EDUCATION              10.965465          9460.015604\n2    HOMEIMPROVEMENT              11.160075         10348.725017\n3            MEDICAL              11.051946          9242.269907\n4           PERSONAL              11.009814          9549.427178\n5            VENTURE              10.940866          9516.417425"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#part-c-build-a-model",
    "href": "posts/new-new-test-post/index.html#part-c-build-a-model",
    "title": "Whose Costs?",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import Pipeline\n\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_all = pd.read_csv(url)\n\ndf = df_all.drop(['loan_grade', 'loan_status'], axis=1)  \ndf = pd.get_dummies(df, columns=['person_home_ownership', 'loan_intent', 'cb_person_default_on_file'], drop_first=True)\ndf.dropna(inplace=True)\n\ny = df_all.loc[df.index, 'loan_status']  \n\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=123)\n\n# Scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlogreg = LogisticRegression(max_iter=1000)\n\nselector = RFECV(estimator=logreg, step=1, cv=5)\nselector = selector.fit(X_train_scaled, y_train)  # Fitting RFECV to the scaled training data\n\n\nX_train_selected = selector.transform(X_train_scaled)\nX_test_selected = selector.transform(X_test_scaled)\n\nmodel_selected = LogisticRegression(max_iter=1000)\nmodel_selected.fit(X_train_selected, y_train)\n\nselected_features = X_train.columns[selector.support_]\n\n# Prediction\ny_pred_selected = model_selected.predict(X_test_selected)\nprint(f\"Optimal number of features: {selector.n_features_}\")\nprint(classification_report(y_test, y_pred_selected))\n\nOptimal number of features: 8\n              precision    recall  f1-score   support\n\n           0       0.87      0.94      0.90      3614\n           1       0.68      0.48      0.56       968\n\n    accuracy                           0.84      4582\n   macro avg       0.77      0.71      0.73      4582\nweighted avg       0.83      0.84      0.83      4582\n\n\n\n\nfeature_names = df.columns\n\nselected_features = feature_names[selector.support_]\n\nprint(\"Selected features:\", selected_features)\n\nSelected features: Index(['loan_amnt', 'loan_int_rate', 'loan_percent_income',\n       'person_home_ownership_OWN', 'person_home_ownership_RENT',\n       'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE'],\n      dtype='object')\n\n\n\n\nX = df[selected_features]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n## MODEL HERE\nmodel.fit(X_train_scaled, y_train)\n\nNameError: name 'model' is not defined"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#trying-to-do-threshold-stuff-here",
    "href": "posts/new-new-test-post/index.html#trying-to-do-threshold-stuff-here",
    "title": "Whose Costs?",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef linear_score(X, w):\n    return X @ w\n\n\n# Extracting weights \nw = model.coef_.flatten()  \n\n\n\nscores = linear_score(X_train_selected, w) \n\n\n\n\nplt.hist(scores, bins=30, edgecolor='k', alpha=0.7)\nplt.xlabel(r\"Score $s$\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of Computed Scores\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnum_thresholds = 101\nFPR = np.zeros(num_thresholds)\nTPR = np.zeros(num_thresholds)\n\nT = np.linspace(scores.min() - 0.1, scores.max() + 0.1, num_thresholds)\n\nfor i in range(num_thresholds):\n    t = T[i]\n    preds = scores &gt;= t\n    FPR[i] = ((preds == 1) & (y_train == 0)).sum() / (y_train == 0).sum()\n    TPR[i] = ((preds == 1) & (y_train == 1)).sum() / (y_train == 1).sum()\n\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 4))\n\nax.plot(FPR, TPR, color=\"black\")\nax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"grey\")  \nax.set_aspect('equal')\n\nax.set_xlabel(\"False Positive Rate\")\nax.set_ylabel(\"True Positive Rate\")\nax.set_title(\"ROC Curve\")\n\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\ndef profit_repaid(loan_amnt, loan_int_rate):\n    return loan_amnt * (1 + 0.25 * loan_int_rate)**10 - loan_amnt\n\ndef profit_default(loan_amnt, loan_int_rate):\n    return loan_amnt * (1 + 0.25 * loan_int_rate)**3 - 2 * loan_amnt\n\n\n\nprobabilities = model.predict_proba(X_train_scaled)[:, 1]\n\n\nexpected_gains = []\nT = np.linspace(0, 1, 101)  \n\nfor t in T:\n    preds = probabilities &gt;= t  # Prediction based on threshold: whether the loan would be issued\n\n    # Initialize gains and losses\n    total_gain = 0\n\n    # Loop over each loan\n    for amt, rate, pred, actual in zip(X_train['loan_amnt'], X_train['loan_int_rate'], preds, y_train):\n        if pred:  # Loan is predicted to be issued\n            if actual == 1:  # Loan is repaid\n                total_gain += profit_repaid(amt, rate)\n            else:  # Loan defaults\n                total_gain += loss_default(amt, rate)\n\n    # Average gain per loan\n    avg_gain = total_gain / len(X_train)\n    expected_gains.append(avg_gain)\n\n\n\n\nplt.figure(figsize=(10, 6))\nplt.plot(T, expected_gains, label='Expected Gain')\nplt.xlabel(r\"Threshold $t$\")\nplt.ylabel(\"Expected Profit per Loan\")\nplt.title(\"Expected Profit per Loan vs. Threshold\")\nplt.xlim(0, 1)\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\nselected_features = ['loan_amnt', 'loan_int_rate', 'loan_percent_income', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE']\n\n\nfeature_combinations = [\n    ['loan_amnt', 'loan_int_rate'],  \n    ['loan_percent_income', 'person_home_ownership_RENT'],  \n    selected_features  \n]\n\n\nresults = {}\n\nfor features in feature_combinations:\n    # Scale features\n    X_scaled = scaler.fit_transform(df[features])\n    \n    LR = LogisticRegression(max_iter=10000)\n    \n    cv_scores_LR = cross_val_score(LR, X_scaled, y, cv=5)\n    results[str(features)] = cv_scores_LR.mean()\n\nfor combo, score in results.items():\n    print(f\"Features: {combo}\\nCV Score (mean accuracy): {score}\\n\")\n\nFeatures: ['loan_amnt', 'loan_int_rate']\nCV Score (mean accuracy): 0.7973108709793388\n\nFeatures: ['loan_percent_income', 'person_home_ownership_RENT']\nCV Score (mean accuracy): 0.8471203482091736\n\nFeatures: ['loan_amnt', 'loan_int_rate', 'loan_percent_income', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE']\nCV Score (mean accuracy): 0.848342893535451"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#evaluate-different-thresholds",
    "href": "posts/new-new-test-post/index.html#evaluate-different-thresholds",
    "title": "Whose Costs?",
    "section": "Evaluate Different Thresholds",
    "text": "Evaluate Different Thresholds"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#part-c-build-a-model",
    "href": "posts/New-test-thing-post/index.html#part-c-build-a-model",
    "title": "Whose Costs?",
    "section": "",
    "text": "I will be utilizing a Logistic Regression Model for predicting whether a prospective borrower is likely to default on a given loan.\nI will also be utilizing the RFECV class from sklearn in order to automatically find the most useful features to use in the model. RFE is a feature selection method that fits a model and removes the weakest feature(s) until the specified number of features is reached. Using RFE, you can automate the process of finding an effective subset of features.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import Pipeline\n\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_all = pd.read_csv(url)\n\ndf = df_all.drop(['loan_grade', 'loan_status'], axis=1)  \ndf = pd.get_dummies(df, columns=['person_home_ownership', 'loan_intent', 'cb_person_default_on_file'], drop_first=True)\ndf.dropna(inplace=True)\n\ny = df_all.loc[df.index, 'loan_status']  \n\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=123)\n\n# Scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlogreg = LogisticRegression(max_iter=1000)\n\nselector = RFECV(estimator=logreg, step=1, cv=5)\nselector = selector.fit(X_train_scaled, y_train)  # Fitting RFECV to the scaled training data\n\n\nX_train_selected = selector.transform(X_train_scaled)\nX_test_selected = selector.transform(X_test_scaled)\n\nmodel_selected = LogisticRegression(max_iter=1000)\nmodel_selected.fit(X_train_selected, y_train)\n\nselected_features = X_train.columns[selector.support_]\n\n# Prediction\ny_pred_selected = model_selected.predict(X_test_selected)\nprint(f\"Optimal number of features: {selector.n_features_}\")\nprint(classification_report(y_test, y_pred_selected))\n\n\n  \nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=123)\n\n\nX_train\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_Y\n\n\n\n\n7573\n26\n38000\n1.0\n8875\n7.51\n0.23\n2\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n15865\n25\n110004\n9.0\n15000\n7.66\n0.14\n4\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n18729\n29\n60000\n3.0\n7200\n7.88\n0.12\n7\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n1288\n24\n44000\n1.0\n3200\n8.00\n0.07\n2\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n12168\n65\n46000\n5.0\n10500\n16.32\n0.23\n24\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17438\n26\n48000\n10.0\n15000\n11.12\n0.31\n3\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n24566\n45\n118000\n15.0\n25000\n12.18\n0.21\n12\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n20105\n21\n26000\n5.0\n14500\n14.61\n0.56\n2\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\n\n\n17825\n35\n78000\n9.0\n9450\n10.99\n0.12\n8\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n22678\n38\n65000\n5.0\n15000\n10.36\n0.23\n17\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n18325 rows × 16 columns\n\n\n\n\n\n# Scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\nX_train_scaled\n\narray([[-0.26848589, -0.43778172, -0.91860701, ..., -0.45114679,\n        -0.4630992 , -0.46344676],\n       [-0.4252438 ,  0.65831624,  1.01527471, ..., -0.45114679,\n        -0.4630992 , -0.46344676],\n       [ 0.20178785, -0.10288151, -0.43513658, ...,  2.21657344,\n        -0.4630992 , -0.46344676],\n       ...,\n       [-1.05227545, -0.62045457,  0.04833385, ..., -0.45114679,\n        -0.4630992 ,  2.15774517],\n       [ 1.14233533,  0.17112776,  1.01527471, ..., -0.45114679,\n        -0.4630992 , -0.46344676],\n       [ 1.61260907, -0.02676782,  0.04833385, ..., -0.45114679,\n        -0.4630992 , -0.46344676]])\n\n\n\n\nlogreg = LogisticRegression(max_iter=1000)\n\nselector = RFECV(estimator=logreg, step=1, cv=5)\nselector = selector.fit(X_train_scaled, y_train)  # Fitting RFECV to the scaled training data\n\n\nX_train_selected = selector.transform(X_train_scaled)\nX_test_selected = selector.transform(X_test_scaled)\n\n\nX_train_selected\n\narray([[-0.12277222, -1.0852037 ,  0.57659278, ..., -0.49598922,\n        -0.45114679, -0.4630992 ],\n       [ 0.85178574, -1.03888993, -0.27169328, ..., -0.49598922,\n        -0.45114679, -0.4630992 ],\n       [-0.38928398, -0.97096305, -0.4602013 , ..., -0.49598922,\n         2.21657344, -0.4630992 ],\n       ...,\n       [ 0.77222999,  1.10698175,  3.68697501, ..., -0.49598922,\n        -0.45114679, -0.4630992 ],\n       [-0.0312831 , -0.01072407, -0.4602013 , ..., -0.49598922,\n        -0.45114679, -0.4630992 ],\n       [ 0.85178574, -0.20524194,  0.57659278, ...,  2.01617286,\n        -0.45114679, -0.4630992 ]])\n\n\n\nfrom sklearn.metrics import classification_report\n\nmodel_selected = LogisticRegression(max_iter=1000)\nmodel_selected.fit(X_train_selected, y_train)\n\nselected_features = X_train.columns[selector.support_]\n\n# Prediction\ny_pred_selected = model_selected.predict(X_test_selected)\nprint(f\"Optimal number of features: {selector.n_features_}\")\nprint(classification_report(y_test, y_pred_selected))\n\nOptimal number of features: 8\n              precision    recall  f1-score   support\n\n           0       0.87      0.94      0.90      3614\n           1       0.68      0.48      0.56       968\n\n    accuracy                           0.84      4582\n   macro avg       0.77      0.71      0.73      4582\nweighted avg       0.83      0.84      0.83      4582\n\n\n\nThe code below is used to identify which features will be used for the most optimal model prediction.\n\nfeature_names = df.columns\n\nselected_features = feature_names[selector.support_]\n\nprint(\"Selected features:\", selected_features)\n\nSelected features: Index(['loan_amnt', 'loan_int_rate', 'loan_percent_income',\n       'person_home_ownership_OWN', 'person_home_ownership_RENT',\n       'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE'],\n      dtype='object')\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nthresholds = np.linspace(0, 1, 101)\nissued_counts = []\n\nfor threshold in thresholds:\n    X_train['predicted_issued'] = X_train['probabilities'] &gt;= threshold\n    issued_count = X_train['predicted_issued'].sum()  \n    issued_counts.append(issued_count)\n\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, issued_counts)\nplt.title('Number of Loans Predicted to be Issued vs. Threshold')\nplt.xlabel('Threshold')\nplt.ylabel('Number of Loans Predicted to be Issued')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nX_train\n\n\n\n\n\n\n\n\nloan_amnt\nloan_int_rate\nloan_percent_income\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_EDUCATION\nloan_intent_PERSONAL\nloan_intent_VENTURE\nprobabilities\nactual\nprediction_probability\npredicted_issued\ngain_loss\n\n\n\n\n7573\n8875\n7.51\n0.23\nFalse\nFalse\nFalse\nFalse\nFalse\n0.105087\n0\n0.105087\nFalse\n0\n\n\n15865\n15000\n7.66\n0.14\nFalse\nFalse\nFalse\nFalse\nFalse\n0.023507\n0\n0.023507\nFalse\n0\n\n\n18729\n7200\n7.88\n0.12\nFalse\nFalse\nFalse\nTrue\nFalse\n0.022012\n0\n0.022012\nFalse\n0\n\n\n1288\n3200\n8.00\n0.07\nFalse\nTrue\nFalse\nFalse\nFalse\n0.061475\n0\n0.061475\nFalse\n0\n\n\n12168\n10500\n16.32\n0.23\nFalse\nTrue\nFalse\nFalse\nTrue\n0.600637\n0\n0.600637\nFalse\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17438\n15000\n11.12\n0.31\nTrue\nFalse\nFalse\nFalse\nTrue\n0.047207\n0\n0.047207\nFalse\n0\n\n\n24566\n25000\n12.18\n0.21\nFalse\nFalse\nFalse\nFalse\nFalse\n0.097552\n0\n0.097552\nFalse\n0\n\n\n20105\n14500\n14.61\n0.56\nFalse\nFalse\nFalse\nFalse\nFalse\n0.978011\n1\n0.978011\nFalse\n0\n\n\n17825\n9450\n10.99\n0.12\nFalse\nFalse\nFalse\nFalse\nFalse\n0.083158\n0\n0.083158\nFalse\n0\n\n\n22678\n15000\n10.36\n0.23\nFalse\nFalse\nTrue\nFalse\nFalse\n0.078296\n0\n0.078296\nFalse\n0\n\n\n\n\n18325 rows × 13 columns\n\n\n\n\n# Estimating the profit here into rows \n\nthresholds = np.linspace(0, 1, 101)\naverage_gains = []\n\nfor threshold in thresholds:\n    X_train['predicted_issued'] = X_train['prediction_probability'] &gt;= threshold\n    X_train['gain_loss'] = X_train.apply(calculate_gain_loss, axis=1)\n    average_gain_loss_per_issued_loan = X_train.loc[X_train['predicted_issued'], 'gain_loss'].mean()\n    average_gains.append(average_gain_loss_per_issued_loan)\n\n# Plotting the average gain per issued loan across thresholds\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, average_gains, label='Average Gain Per Issued Loan')\nplt.xlabel('Threshold')\nplt.ylabel('Average Gain Per Issued Loan')\nplt.title('Average Gain Per Issued Loan vs. Threshold')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\nX_train\n\n\n\n\n\n\n\n\nloan_amnt\nloan_int_rate\nloan_percent_income\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_EDUCATION\nloan_intent_PERSONAL\nloan_intent_VENTURE\nprobabilities\nactual\nprediction_probability\npredicted_issued\ngain_loss\n\n\n\n\n7573\n8875\n7.51\n0.23\nFalse\nFalse\nFalse\nFalse\nFalse\n0.105087\n0\n0.105087\nFalse\n0\n\n\n15865\n15000\n7.66\n0.14\nFalse\nFalse\nFalse\nFalse\nFalse\n0.023507\n0\n0.023507\nFalse\n0\n\n\n18729\n7200\n7.88\n0.12\nFalse\nFalse\nFalse\nTrue\nFalse\n0.022012\n0\n0.022012\nFalse\n0\n\n\n1288\n3200\n8.00\n0.07\nFalse\nTrue\nFalse\nFalse\nFalse\n0.061475\n0\n0.061475\nFalse\n0\n\n\n12168\n10500\n16.32\n0.23\nFalse\nTrue\nFalse\nFalse\nTrue\n0.600637\n0\n0.600637\nFalse\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17438\n15000\n11.12\n0.31\nTrue\nFalse\nFalse\nFalse\nTrue\n0.047207\n0\n0.047207\nFalse\n0\n\n\n24566\n25000\n12.18\n0.21\nFalse\nFalse\nFalse\nFalse\nFalse\n0.097552\n0\n0.097552\nFalse\n0\n\n\n20105\n14500\n14.61\n0.56\nFalse\nFalse\nFalse\nFalse\nFalse\n0.978011\n1\n0.978011\nFalse\n0\n\n\n17825\n9450\n10.99\n0.12\nFalse\nFalse\nFalse\nFalse\nFalse\n0.083158\n0\n0.083158\nFalse\n0\n\n\n22678\n15000\n10.36\n0.23\nFalse\nFalse\nTrue\nFalse\nFalse\n0.078296\n0\n0.078296\nFalse\n0\n\n\n\n\n18325 rows × 13 columns"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#cross-validation",
    "href": "posts/New-test-thing-post/index.html#cross-validation",
    "title": "Whose Costs?",
    "section": "",
    "text": "I did not want to limit myself to only one model however, so I used cross_val_score from the sklearn.model_selection in order to compare how my ‘Optimal’ LR model did against other LR models with a different combination of features.\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\nselected_features = ['loan_amnt', 'loan_int_rate', 'loan_percent_income', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE']\n\n\nfeature_combinations = [\n    ['loan_amnt', 'loan_int_rate'],  \n    ['loan_percent_income', 'person_home_ownership_RENT'],  \n    selected_features  \n]\n\n\nresults = {}\n\nfor features in feature_combinations:\n    # Scale features\n    X_scaled = scaler.fit_transform(df[features])\n    \n    LR = LogisticRegression(max_iter=10000)\n    \n    cv_scores_LR = cross_val_score(LR, X_scaled, y, cv=5)\n    results[str(features)] = cv_scores_LR.mean()\n\nfor combo, score in results.items():\n    print(f\"Features: {combo}\\nCV Score (mean accuracy): {score}\\n\")\n\nFeatures: ['loan_amnt', 'loan_int_rate']\nCV Score (mean accuracy): 0.7973108709793388\n\nFeatures: ['loan_percent_income', 'person_home_ownership_RENT']\nCV Score (mean accuracy): 0.8471203482091736\n\nFeatures: ['loan_amnt', 'loan_int_rate', 'loan_percent_income', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE']\nCV Score (mean accuracy): 0.848342893535451\n\n\n\nAs seen above, the ‘Optimal’ feature LR model performed the best in predicting the likelihood of a default."
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#part-d-finding-a-threshold",
    "href": "posts/New-test-thing-post/index.html#part-d-finding-a-threshold",
    "title": "Whose Costs?",
    "section": "",
    "text": "In this section, we will be exploring how I came up with the threshold t that will be used for our model.\nI used the numpy package to compute linear scores across all n of our training points.\nI plotted the scores below in order to easily visualize this process. By gathering scores, we can easily simulate decision-making with a given threshold.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef linear_score(X, w):\n    return X @ w\n\n\n# Extracting weights \nw = model.coef_.flatten()  \n\n\n\nscores = linear_score(X_train_selected, w) \n\n\nplt.hist(scores, bins=30, edgecolor='k', alpha=0.7)\nplt.xlabel(r\"Score $s$\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of Computed Scores\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure 3: Histogram of Computed Scores"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#evaluate-different-thresholds",
    "href": "posts/New-test-thing-post/index.html#evaluate-different-thresholds",
    "title": "Whose Costs?",
    "section": "Evaluate Different Thresholds",
    "text": "Evaluate Different Thresholds\nAlthough the previous data from Figure 3 and ?@fig-ROC are useful, often times banks want to set a threshold that maximizes profit-loss.\nWe use a two assumptions below to account for the profit and loss of a bank if a loan is repaid in full and if a loan is defaulted on.\nWe then calculate the expected profit per issued loan for each threshold, and find the threshold that is the most optimal for generating overall profit. We can accomplish this by generating a gain_loss column within our X_train dataset in order to easily summarize the overall average gain/loss for each issued loan at each threshold value.\nThis relationship is illustrated in Figure 5.\n\nX = df[selected_features]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n## MODEL HERE\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train_scaled, y_train)\n\n# Predict probabilities\nprobabilities = model.predict_proba(X_train_scaled)[:, 1]\n\nX_train['actual'] = y_train\nX_train['prediction_probability'] = probabilities\n\n\n\ndef calculate_gain_loss(row):\n    if row['predicted_issued']:\n        if row['actual'] == 0:  # Loan was repaid\n            return row['loan_amnt'] * (1 + 0.25 * row['loan_int_rate'])**10 - row['loan_amnt']\n        elif row['actual'] == 1:  # Loan defaulted\n            return row['loan_amnt'] * (1 + 0.25 * row['loan_int_rate'])**3 - 1.7 * row['loan_amnt']\n    return 0\n\n\n# Estimating the profit here into rows \n\nthresholds = np.linspace(0, 1, 101)\naverage_gains = []\n\nfor threshold in thresholds:\n    X_train['predicted_issued'] = X_train['prediction_probability'] &gt;= threshold\n    X_train['gain_loss'] = X_train.apply(calculate_gain_loss, axis=1)\n    \n    # Check if any loans are predicted to be issued at this threshold\n    if X_train['predicted_issued'].sum() &gt; 0:\n        average_gain_loss_per_issued_loan = X_train.loc[X_train['predicted_issued'], 'gain_loss'].mean()\n    else:\n        # No loans are issued, so append a default value (e.g., 0) to avoid NaN\n        average_gain_loss_per_issued_loan = 0\n    \n    average_gains.append(average_gain_loss_per_issued_loan)\n\n\n# Plotting the average gain per issued loan across thresholds\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, average_gains, label='Average Gain Per Issued Loan')\nplt.xlabel('Threshold (Probability of Default)')\nplt.ylabel('Average Gain Per Issued Loan')\nplt.title('Average Gain Per Issued Loan vs. Threshold')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nFigure 5: Expected Profit per Loan vs Threshold\n\n\n\n\n\n\noptimal_index = np.argmax(average_gains)\noptimal_threshold = thresholds[optimal_index]\nprint(f\"Optimal Threshold: {optimal_threshold}\")\nprint(f\"Average Gain Per Issued Loan at Optimal Threshold: {average_gains[optimal_index]}\")\n\nOptimal Threshold: 0.25\nMaximum Average Gain Per Issued Loan at Optimal Threshold: 24992323662.99832\n\n\nWe use the above code to find the most optimal threshold which presents us with the highest level of expected profit.\nOur model finds that a threshold of 0.25 is the most optimal, with an expected average gain of $24,992,323,663 per borrower\nWhile yes, this number appears rather high, our simple assumptions assume compounded interest while real life situations may differ significantly"
  },
  {
    "objectID": "posts/new-test-post/goal-setting.html",
    "href": "posts/new-test-post/goal-setting.html",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "Donovan Wood\n\n\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nAs mentioned, I want to obviously grow in each of these areas, but in particular I hope to expand my knowledge in the Theory and Experimentation areas.\nI want to know why things work, not just “They work, and here is how to use them”. Throughout my college experience I oftentimes found myself just doing the motions and I really want to try to emphasize fully grasping the material.\nIn terms of relevance to my future, I want to join a quantitative firm in the future, in order to research further in the field and develop systems that work efficiently in the market. Being an Economics and Computer Science Major, my interest lies in both and looking forward, this path seems to be the most interesting to me. That being said, machine learning is a huge component of that job field and so I want to be adept in my learnings/experimentations in the course so that I can use some of what I learned in real-world applications.\n\n\n\n\n\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\nCurrently, I plan to complete as many blog posts as I can, not only for the obvious grade, but also in order to access my learning accordingly. By doing as many as I can, I can directly apply the most recent class teachings and hope to build on more fundamental skills.\nThat is my current goal, however I do realize I am taking five courses, three of which are computer science courses, so I acknowledge that there will be times in which I have too much on my plate in order to complete every one. Still, I hope to at least keep this mentality throughout the course of the semester and do the best job I can.\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\nPreparing for the warmups is a task that I believe I can achieve every week. Not only does it help with the understandings of the class material that day, but also reinforces the idea of completion due to societal and class pressure ha! All joking aside, I hope to continue to complete all the warmups throughout the semester and get help from my peers if I’m stuck on one.\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\nEver since I joined the class, I always wanted to develop something that directly applies to my future career aspirations.\nThis may seem a bit ambitious, but I want to develop an machine learning algorithm that can create a positive return rate trading on some good (stock, etf, futures, options) in the market consistently.\nObviously things that come to mind are data limitations (real-time data), effective implementation, and non bias results but I still want to try to achieve this goal.\nAs of now, we are only beginning week 3 of the class, so I am unsure as to if that is possible given the course teachings, but I am willing to learn additional things outside of the class in order to achieve this. Even if I am unsuccessful in developing a model that achieves all my goals, I believe that the learning experience and process will be useful for my future endeavors."
  },
  {
    "objectID": "posts/new-test-post/goal-setting.html#what-youll-learn",
    "href": "posts/new-test-post/goal-setting.html#what-youll-learn",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "The knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nAs mentioned, I want to obviously grow in each of these areas, but in particular I hope to expand my knowledge in the Theory and Experimentation areas.\nI want to know why things work, not just “They work, and here is how to use them”. Throughout my college experience I oftentimes found myself just doing the motions and I really want to try to emphasize fully grasping the material.\nIn terms of relevance to my future, I want to join a quantitative firm in the future, in order to research further in the field and develop systems that work efficiently in the market. Being an Economics and Computer Science Major, my interest lies in both and looking forward, this path seems to be the most interesting to me. That being said, machine learning is a huge component of that job field and so I want to be adept in my learnings/experimentations in the course so that I can use some of what I learned in real-world applications."
  },
  {
    "objectID": "posts/new-test-post/goal-setting.html#what-youll-achieve",
    "href": "posts/new-test-post/goal-setting.html#what-youll-achieve",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "Most blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\nCurrently, I plan to complete as many blog posts as I can, not only for the obvious grade, but also in order to access my learning accordingly. By doing as many as I can, I can directly apply the most recent class teachings and hope to build on more fundamental skills.\nThat is my current goal, however I do realize I am taking five courses, three of which are computer science courses, so I acknowledge that there will be times in which I have too much on my plate in order to complete every one. Still, I hope to at least keep this mentality throughout the course of the semester and do the best job I can.\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\nPreparing for the warmups is a task that I believe I can achieve every week. Not only does it help with the understandings of the class material that day, but also reinforces the idea of completion due to societal and class pressure ha! All joking aside, I hope to continue to complete all the warmups throughout the semester and get help from my peers if I’m stuck on one.\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\nEver since I joined the class, I always wanted to develop something that directly applies to my future career aspirations.\nThis may seem a bit ambitious, but I want to develop an machine learning algorithm that can create a positive return rate trading on some good (stock, etf, futures, options) in the market consistently.\nObviously things that come to mind are data limitations (real-time data), effective implementation, and non bias results but I still want to try to achieve this goal.\nAs of now, we are only beginning week 3 of the class, so I am unsure as to if that is possible given the course teachings, but I am willing to learn additional things outside of the class in order to achieve this. Even if I am unsuccessful in developing a model that achieves all my goals, I believe that the learning experience and process will be useful for my future endeavors."
  },
  {
    "objectID": "Lecture Notes/10-compas.html",
    "href": "Lecture Notes/10-compas.html",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Today we are going to study an extremely famous investigation into algorithmic decision-making in the sphere of criminal justice by @angwin2022machine, originally written for ProPublica in 2016. This investigation significantly accelerated the pace of research into bias and fairness in machine learning, due in combination to its simple message and publicly-available data.\nIt’s helpful to look at a sample form used for feature collection in the COMPAS risk assessment.\nYou may have already read about the COMPAS algorithm in the original article at ProPublica. Our goal today is to reproduce some of the main findings of this article and set the stage for a more systematic treatment of bias and fairness in machine learning.\nParts of these lecture notes are inspired by the original ProPublica analysis and Allen Downey’s expository case study on the same data.\n\n\n Let’s first obtain the data. I’ve hosted a copy on the course website, so we can download it using a URL.This data set was obtained by @angwin2022machine through a public records request. The data comprises two years worth of COMPAS scoring in Broward County, Florida.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nsns.set_style(\"whitegrid\")\nnp.set_printoptions(precision = 3)\npd.set_option('display.precision', 3)\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/compas/compas.csv\"\ncompas = pd.read_csv(url)\n\nFor today we are only going to consider a subset of columns.\n\ncols = [\"sex\", \"race\", \"decile_score\", \"two_year_recid\"]\ncompas = compas[cols]\n\nWe are also only going to consider white (Caucasian) and Black (African-American) defendants:\n\nis_white = compas[\"race\"] == \"Caucasian\"\nis_black = compas[\"race\"] == \"African-American\"\n\ncompas = compas[is_white | is_black]\ncompas = compas.copy()\n\nOur data now looks like this:\n\ncompas.head()\n\n\n\n\n\n\n\n\nsex\nrace\ndecile_score\ntwo_year_recid\n\n\n\n\n1\nMale\nAfrican-American\n3\n1\n\n\n2\nMale\nAfrican-American\n4\n1\n\n\n3\nMale\nAfrican-American\n8\n0\n\n\n6\nMale\nCaucasian\n6\n1\n\n\n8\nFemale\nCaucasian\n1\n0\n\n\n\n\n\n\n\n\n\n\nLet’s do some quick exploration of our data. How many defendants are present in this data of each sex?\n\ncompas.groupby(\"sex\").size()\n\nsex\nFemale    1219\nMale      4931\ndtype: int64\n\n\nWhat about race?\n\ncompas.groupby(\"race\").size()\n\nrace\nAfrican-American    3696\nCaucasian           2454\ndtype: int64\n\n\nThe decile score is the algorithm’s prediction. Higher decile scores indicate that, according to the COMPAS model, the defendant has higher likelihood to be charged with a crime within the next two years. In the framework we’ve developed in this class, you can think of the decile score as being produced by computing a score like \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\) for each defendant \\(i\\), and then dividing these into the lowest 10% (decile score 1), the next 10% (decile score 2), the next 10% (decile score 3) and so on.\nThe easiest way to see how this looks is with a bar chart, which we can make efficiently using the seaborn (sns) package.\n\ncounts = compas.groupby([\"race\", \"decile_score\"]).size().reset_index(name = \"n\")\np = sns.barplot(data = counts, \n                x = \"decile_score\", \n                y = \"n\", \n                hue = \"race\", \n                palette = \"BuPu\", \n                saturation = 0.5)\n\n\n\n\n\n\n\n\nYou may notice that the number of white defendants who receive a given decile score tends to decrease as the score increases, whereas the number of Black defendants remains relatively constant.\nLet’s also take a look at the recidivism rate in the data:\n\ncompas[\"two_year_recid\"].mean()\n\n0.4661788617886179\n\n\nSo, in these data, approximately 47% of all defendants went on to be charged of another crime within the next two years. This is sometimes called the prevalence of the outcome. Although this is not a “good” outcome, it is labeled 1 in the target data and so we refer to this as the “positive” outcome. Prevalence without further specification usually refers to prevalence of the positive outcome.\nThe base rate of prediction accuracy in this problem is 53%: if we always guessed that the defendant was not arrested within two years, we would be right 53% of the time.\nWe can also compute the prevalence broken down by race of the defendant:\n\ncompas.groupby(\"race\")[\"two_year_recid\"].mean()\n\nrace\nAfrican-American    0.514\nCaucasian           0.394\nName: two_year_recid, dtype: float64\n\n\n\n\nWhen interpreting these different prevalences, it is important to remember that\n\nRace is itself a socially-constructed system of human categorization invented by humans with political and economic motives to describe other humans as property [@bonilla-silvaRacismRacistsColorblind2018].\n\nThe relation between arrest and actual criminal offense can display racial bias, with effects varying by geography [@fogliatoValidityArrestProxy2021].\nDecisions about which behaviors are criminal are contingent political decisions which have, historically, fallen hardest on Black Americans [@yusefCriminalizingRaceRacializing2017].\n\nThe prevalences between the two groups are substantially different. This difference will have major consequences later on for the possibility of different kinds of fairness in classifiers.\nWe’re going to treat the COMPAS algorithm as a binary classifier, but you might notice a problem: the algorithm’s prediction is the decile_score column, which is not actually a 0-1 label. Following the analysis of @angwin2022machine, we are going to construct a new binary column in which we say that a defendant is predicted_high_risk if their decile_score is larger than 4.\n\ncompas[\"predicted_high_risk\"] = compas[\"decile_score\"] &gt; 4 \n\nNow that we’ve done that, we can ask: how likely are Black and white defendants to receive positive predictions in this data?\n\ncompas.groupby(\"race\")[[\"two_year_recid\", \"predicted_high_risk\"]].mean()\n\n\n\n\n\n\n\n\ntwo_year_recid\npredicted_high_risk\n\n\nrace\n\n\n\n\n\n\nAfrican-American\n0.514\n0.588\n\n\nCaucasian\n0.394\n0.348\n\n\n\n\n\n\n\nBlack defendants are substantially more likely to receive a positive prediction than white defendants, and the disparity is larger than the observed prevalence of the positive outcome.\n\n\n\n\n\n\nFairness (Part 1)\n\n\n\nIs this fair? What is your gut telling you? Yes, no, possibly? What information would you need in order to make a judgment? What is the principle on which your judgment rests?\n\n\n\n\n\nLet’s now ask a few questions about the the predictive accuracy of this algorithm. First, how accurate it is it overall?\n\ncompas[\"correct_prediction\"] = (compas[\"predicted_high_risk\"] == compas[\"two_year_recid\"])\ncompas[\"correct_prediction\"].mean()\n\n0.6508943089430894\n\n\nRecall that the base rate in this problem is 53%, so our accuracy is somewhat better than random guessing.\nWhat about the accuracy on Black and white defendants separately?\n\ncompas.groupby([\"race\"])[\"correct_prediction\"].mean()\n\nrace\nAfrican-American    0.638\nCaucasian           0.670\nName: correct_prediction, dtype: float64\n\n\nThe overall accuracies for Black and white defendants are comparable, and both are somewhat higher than the base rate of 53%.\nWhat about the error rates? Here is a simple calculation which computes the false positive rate (FPR) in the first row and the true positive rate (TPR) on the bottom row:\n\ncompas.groupby([\"two_year_recid\"])[\"predicted_high_risk\"].mean().reset_index()\n\n\n\n\n\n\n\n\ntwo_year_recid\npredicted_high_risk\n\n\n\n\n0\n0\n0.352\n\n\n1\n1\n0.654\n\n\n\n\n\n\n\nHowever, and this was the main finding of the ProPublica study, the FPR and FNR are very different when we break down the data by race:\n\ncompas.groupby([\"two_year_recid\", \"race\"])[\"predicted_high_risk\"].mean().reset_index()\n\n\n\n\n\n\n\n\ntwo_year_recid\nrace\npredicted_high_risk\n\n\n\n\n0\n0\nAfrican-American\n0.448\n\n\n1\n0\nCaucasian\n0.235\n\n\n2\n1\nAfrican-American\n0.720\n\n\n3\n1\nCaucasian\n0.523\n\n\n\n\n\n\n\nThe false positive rate for Black defendants is much higher than the false positive rate for white defendants. This was the main finding of @angwin2022machine. The FPR of 44% for Black defendants means that, out of every 100 Black defendants who in fact will not commit another crime, the algorithm nevertheless predicts that 44 of them will. In contrast, the FPR of 23% for white defendants indicates that only 23 out of 100 non-recidivating white defendants would be predicted to recidivate.\nThere are a few ways in which we can think of this result as reflecting bias:\n\nThe algorithm has learned an implicit pattern wherein Black defendants are intrinsically more “criminal” than white defendants, even among people who factually never committed another crime. This is a bias in the patterns that the algorithm has learned in order to formulate its predictions. This is related to the idea of representational bias, in which algorithms learn and reproduce toxic stereotypes about certain groups of people.\nRegardless of how the algorithm forms its predictions, the impact of the algorithm being used in the penal system is that more Black defendants will be classified as high-risk, resulting in more denials of parole, bail, early release, or other forms of freedom from the penal system. So, the algorithm has disparate impact on people. This is sometimes called allocative or distributional bias: bias in how resources or opportunities (in this case, freedom) are allocated or distributed between groups.\n\nSometimes predictive equality is also defined to require that the false negative rates (FNRs) be equal across the two groups as well.\nWe can think about the argument of @angwin2022machine as a two-step argument:\n\n\nThe COMPAS algorithm has disparate error rates by race.\nTherefore, the COMPAS algorithm is unjustly biased with respect to race.\n\n\nThis argument implicitly equates equality of error rates with lack of bias.\n\n\n\n\n\n\nFairness (Part 2)\n\n\n\n\nSuppose that we developed an alternative algorithm in which the false positive rates were equal, but there were still more positive predictions for Black defendants overall. Would that be enough to ensure fairness?\nSuppose that we developed an alternative prediction algorithm in which the rate of positive prediction was the same across racial groups, but the false positive rates were different. Would that be to ensure fairness?\n\n\n\n\n\n\n@angwin2022machine kicked off a vigorous discussion about what it means for an algorithm to fair and how to measure deviations from bias. In particular, Northpointe, the company that developed COMPAS, issued a report @flores2016false in which they argued that their algorithm was fair. Their argument is based on an idea of fairness which is sometimes called sufficiency @corbett-daviesAlgorithmicDecisionMaking2017.\nHere’s the intuition expressed by sufficiency. Imagine that you and your friend both received an A- in Data Structures. Suppose, however, that the instructor says different things to each of you:\n\nTo you, the instructor says: “You did fine in this class, but I don’t think that you are prepared to take Computer Architecture. I gave you a higher grade than I would normally because you wear cool hats in class.”\nTo your friend, the instructor says: “*You did fine in this class and I think you are prepared to take Computer Architecture. Some students got a bump in their grade because they are cool-hat-wearers, but you didn’t get that benefit.”\n\nFeels unfair, right? The instructor is saying that:\n\nWhat a grade means for you in terms of your future success depends on your identity group.\n\n\n\n\n\n\n\nNote\n\n\n\nSuppose that you heard this, but instead of cool hats it was because you are a member of an identity group that “needs some help” in order to achieve equitable representation in the CS major. How would you feel? Would that feel fair to you?\n\n\nWe’ll formally define sufficiency in a future lecture. For now, let’s use an informal definition:\n\nSufficiency means that a positive prediction means the same thing for future outcomes for each racial group.\n\nTo operationalize this idea, we are looking for the rate of re-arrest to be the same between (a) Black defendants who received a positive prediction and (b) white defendants who received a positive prediction.\nLet’s check this:\nThe rates of rearrest are relatively similar between groups when controlling for the predictions they collectively received. Formal statistical hypothesis tests are typically used to determine whether this difference is sufficiently “real” to warrant correction. In most of the published literature, scholars have considered that the two rates are sufficiently close that we should instead simply say that COMPAS appears to be relatively close to satisfying sufficiency.\nIndeed, in a rejoinder article published by affiliates of the company Northpointe which produced COMPAS, the fact that COMPAS satisfies sufficiency is one of the primary arguments [@flores2016false].\n\n\n\nIn these notes, we replicated the data analysis of @angwin2022machine, finding that the COMPAS algorithm has disparate error rates between Black and white defendants. We introduced the idea that fairness actually has several different facets in our moral intuitions, and found that the COMPAS algorithm satisfies one of them (sufficiency: equal scores mean the same thing regardless of your group membership) but not the others (equal prediction rates and equal error rates).\n\n\n\n\nCan we have it all? Could we modify the COMPAS algorithm in such a way that it satisfies all the ideas of fairness that we discussed above? Could we then call it “fair” or “unbiased?”\nAre there other ways to define fairness? Which ones are most compelling to us? Does the right idea of fairness depend on the context in which we apply it?\nHow did this happen? The COMPAS algorithm was never trained on race data about the defendant. How did it happen that this algorithm nevertheless made recommendations at different rates across groups?\nIs automated decision-making legitimate in this setting? Can it be legitimate (just, fair) to use an automated decision-system for making recommendations about parole and sentencing decisions at all? What safeguards and forms of recourse are necessary for the legitimate use of automated decision-making in criminal justice?\nWhat are the systemic impacts? Disparate sentencing decisions can have downstream impacts on communities and institutions. How could application of the COMPAS algorithm exacerbate systemic inequalities?"
  },
  {
    "objectID": "Lecture Notes/10-compas.html#data-preparation",
    "href": "Lecture Notes/10-compas.html#data-preparation",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Let’s first obtain the data. I’ve hosted a copy on the course website, so we can download it using a URL.This data set was obtained by @angwin2022machine through a public records request. The data comprises two years worth of COMPAS scoring in Broward County, Florida.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nsns.set_style(\"whitegrid\")\nnp.set_printoptions(precision = 3)\npd.set_option('display.precision', 3)\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/compas/compas.csv\"\ncompas = pd.read_csv(url)\n\nFor today we are only going to consider a subset of columns.\n\ncols = [\"sex\", \"race\", \"decile_score\", \"two_year_recid\"]\ncompas = compas[cols]\n\nWe are also only going to consider white (Caucasian) and Black (African-American) defendants:\n\nis_white = compas[\"race\"] == \"Caucasian\"\nis_black = compas[\"race\"] == \"African-American\"\n\ncompas = compas[is_white | is_black]\ncompas = compas.copy()\n\nOur data now looks like this:\n\ncompas.head()\n\n\n\n\n\n\n\n\nsex\nrace\ndecile_score\ntwo_year_recid\n\n\n\n\n1\nMale\nAfrican-American\n3\n1\n\n\n2\nMale\nAfrican-American\n4\n1\n\n\n3\nMale\nAfrican-American\n8\n0\n\n\n6\nMale\nCaucasian\n6\n1\n\n\n8\nFemale\nCaucasian\n1\n0"
  },
  {
    "objectID": "Lecture Notes/10-compas.html#preliminary-explorations",
    "href": "Lecture Notes/10-compas.html#preliminary-explorations",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Let’s do some quick exploration of our data. How many defendants are present in this data of each sex?\n\ncompas.groupby(\"sex\").size()\n\nsex\nFemale    1219\nMale      4931\ndtype: int64\n\n\nWhat about race?\n\ncompas.groupby(\"race\").size()\n\nrace\nAfrican-American    3696\nCaucasian           2454\ndtype: int64\n\n\nThe decile score is the algorithm’s prediction. Higher decile scores indicate that, according to the COMPAS model, the defendant has higher likelihood to be charged with a crime within the next two years. In the framework we’ve developed in this class, you can think of the decile score as being produced by computing a score like \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\) for each defendant \\(i\\), and then dividing these into the lowest 10% (decile score 1), the next 10% (decile score 2), the next 10% (decile score 3) and so on.\nThe easiest way to see how this looks is with a bar chart, which we can make efficiently using the seaborn (sns) package.\n\ncounts = compas.groupby([\"race\", \"decile_score\"]).size().reset_index(name = \"n\")\np = sns.barplot(data = counts, \n                x = \"decile_score\", \n                y = \"n\", \n                hue = \"race\", \n                palette = \"BuPu\", \n                saturation = 0.5)\n\n\n\n\n\n\n\n\nYou may notice that the number of white defendants who receive a given decile score tends to decrease as the score increases, whereas the number of Black defendants remains relatively constant.\nLet’s also take a look at the recidivism rate in the data:\n\ncompas[\"two_year_recid\"].mean()\n\n0.4661788617886179\n\n\nSo, in these data, approximately 47% of all defendants went on to be charged of another crime within the next two years. This is sometimes called the prevalence of the outcome. Although this is not a “good” outcome, it is labeled 1 in the target data and so we refer to this as the “positive” outcome. Prevalence without further specification usually refers to prevalence of the positive outcome.\nThe base rate of prediction accuracy in this problem is 53%: if we always guessed that the defendant was not arrested within two years, we would be right 53% of the time.\nWe can also compute the prevalence broken down by race of the defendant:\n\ncompas.groupby(\"race\")[\"two_year_recid\"].mean()\n\nrace\nAfrican-American    0.514\nCaucasian           0.394\nName: two_year_recid, dtype: float64\n\n\n\n\nWhen interpreting these different prevalences, it is important to remember that\n\nRace is itself a socially-constructed system of human categorization invented by humans with political and economic motives to describe other humans as property [@bonilla-silvaRacismRacistsColorblind2018].\n\nThe relation between arrest and actual criminal offense can display racial bias, with effects varying by geography [@fogliatoValidityArrestProxy2021].\nDecisions about which behaviors are criminal are contingent political decisions which have, historically, fallen hardest on Black Americans [@yusefCriminalizingRaceRacializing2017].\n\nThe prevalences between the two groups are substantially different. This difference will have major consequences later on for the possibility of different kinds of fairness in classifiers.\nWe’re going to treat the COMPAS algorithm as a binary classifier, but you might notice a problem: the algorithm’s prediction is the decile_score column, which is not actually a 0-1 label. Following the analysis of @angwin2022machine, we are going to construct a new binary column in which we say that a defendant is predicted_high_risk if their decile_score is larger than 4.\n\ncompas[\"predicted_high_risk\"] = compas[\"decile_score\"] &gt; 4 \n\nNow that we’ve done that, we can ask: how likely are Black and white defendants to receive positive predictions in this data?\n\ncompas.groupby(\"race\")[[\"two_year_recid\", \"predicted_high_risk\"]].mean()\n\n\n\n\n\n\n\n\ntwo_year_recid\npredicted_high_risk\n\n\nrace\n\n\n\n\n\n\nAfrican-American\n0.514\n0.588\n\n\nCaucasian\n0.394\n0.348\n\n\n\n\n\n\n\nBlack defendants are substantially more likely to receive a positive prediction than white defendants, and the disparity is larger than the observed prevalence of the positive outcome.\n\n\n\n\n\n\nFairness (Part 1)\n\n\n\nIs this fair? What is your gut telling you? Yes, no, possibly? What information would you need in order to make a judgment? What is the principle on which your judgment rests?"
  },
  {
    "objectID": "Lecture Notes/10-compas.html#the-propublica-findings",
    "href": "Lecture Notes/10-compas.html#the-propublica-findings",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Let’s now ask a few questions about the the predictive accuracy of this algorithm. First, how accurate it is it overall?\n\ncompas[\"correct_prediction\"] = (compas[\"predicted_high_risk\"] == compas[\"two_year_recid\"])\ncompas[\"correct_prediction\"].mean()\n\n0.6508943089430894\n\n\nRecall that the base rate in this problem is 53%, so our accuracy is somewhat better than random guessing.\nWhat about the accuracy on Black and white defendants separately?\n\ncompas.groupby([\"race\"])[\"correct_prediction\"].mean()\n\nrace\nAfrican-American    0.638\nCaucasian           0.670\nName: correct_prediction, dtype: float64\n\n\nThe overall accuracies for Black and white defendants are comparable, and both are somewhat higher than the base rate of 53%.\nWhat about the error rates? Here is a simple calculation which computes the false positive rate (FPR) in the first row and the true positive rate (TPR) on the bottom row:\n\ncompas.groupby([\"two_year_recid\"])[\"predicted_high_risk\"].mean().reset_index()\n\n\n\n\n\n\n\n\ntwo_year_recid\npredicted_high_risk\n\n\n\n\n0\n0\n0.352\n\n\n1\n1\n0.654\n\n\n\n\n\n\n\nHowever, and this was the main finding of the ProPublica study, the FPR and FNR are very different when we break down the data by race:\n\ncompas.groupby([\"two_year_recid\", \"race\"])[\"predicted_high_risk\"].mean().reset_index()\n\n\n\n\n\n\n\n\ntwo_year_recid\nrace\npredicted_high_risk\n\n\n\n\n0\n0\nAfrican-American\n0.448\n\n\n1\n0\nCaucasian\n0.235\n\n\n2\n1\nAfrican-American\n0.720\n\n\n3\n1\nCaucasian\n0.523\n\n\n\n\n\n\n\nThe false positive rate for Black defendants is much higher than the false positive rate for white defendants. This was the main finding of @angwin2022machine. The FPR of 44% for Black defendants means that, out of every 100 Black defendants who in fact will not commit another crime, the algorithm nevertheless predicts that 44 of them will. In contrast, the FPR of 23% for white defendants indicates that only 23 out of 100 non-recidivating white defendants would be predicted to recidivate.\nThere are a few ways in which we can think of this result as reflecting bias:\n\nThe algorithm has learned an implicit pattern wherein Black defendants are intrinsically more “criminal” than white defendants, even among people who factually never committed another crime. This is a bias in the patterns that the algorithm has learned in order to formulate its predictions. This is related to the idea of representational bias, in which algorithms learn and reproduce toxic stereotypes about certain groups of people.\nRegardless of how the algorithm forms its predictions, the impact of the algorithm being used in the penal system is that more Black defendants will be classified as high-risk, resulting in more denials of parole, bail, early release, or other forms of freedom from the penal system. So, the algorithm has disparate impact on people. This is sometimes called allocative or distributional bias: bias in how resources or opportunities (in this case, freedom) are allocated or distributed between groups.\n\nSometimes predictive equality is also defined to require that the false negative rates (FNRs) be equal across the two groups as well.\nWe can think about the argument of @angwin2022machine as a two-step argument:\n\n\nThe COMPAS algorithm has disparate error rates by race.\nTherefore, the COMPAS algorithm is unjustly biased with respect to race.\n\n\nThis argument implicitly equates equality of error rates with lack of bias.\n\n\n\n\n\n\nFairness (Part 2)\n\n\n\n\nSuppose that we developed an alternative algorithm in which the false positive rates were equal, but there were still more positive predictions for Black defendants overall. Would that be enough to ensure fairness?\nSuppose that we developed an alternative prediction algorithm in which the rate of positive prediction was the same across racial groups, but the false positive rates were different. Would that be to ensure fairness?"
  },
  {
    "objectID": "Lecture Notes/10-compas.html#the-rebuttal",
    "href": "Lecture Notes/10-compas.html#the-rebuttal",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "@angwin2022machine kicked off a vigorous discussion about what it means for an algorithm to fair and how to measure deviations from bias. In particular, Northpointe, the company that developed COMPAS, issued a report @flores2016false in which they argued that their algorithm was fair. Their argument is based on an idea of fairness which is sometimes called sufficiency @corbett-daviesAlgorithmicDecisionMaking2017.\nHere’s the intuition expressed by sufficiency. Imagine that you and your friend both received an A- in Data Structures. Suppose, however, that the instructor says different things to each of you:\n\nTo you, the instructor says: “You did fine in this class, but I don’t think that you are prepared to take Computer Architecture. I gave you a higher grade than I would normally because you wear cool hats in class.”\nTo your friend, the instructor says: “*You did fine in this class and I think you are prepared to take Computer Architecture. Some students got a bump in their grade because they are cool-hat-wearers, but you didn’t get that benefit.”\n\nFeels unfair, right? The instructor is saying that:\n\nWhat a grade means for you in terms of your future success depends on your identity group.\n\n\n\n\n\n\n\nNote\n\n\n\nSuppose that you heard this, but instead of cool hats it was because you are a member of an identity group that “needs some help” in order to achieve equitable representation in the CS major. How would you feel? Would that feel fair to you?\n\n\nWe’ll formally define sufficiency in a future lecture. For now, let’s use an informal definition:\n\nSufficiency means that a positive prediction means the same thing for future outcomes for each racial group.\n\nTo operationalize this idea, we are looking for the rate of re-arrest to be the same between (a) Black defendants who received a positive prediction and (b) white defendants who received a positive prediction.\nLet’s check this:\nThe rates of rearrest are relatively similar between groups when controlling for the predictions they collectively received. Formal statistical hypothesis tests are typically used to determine whether this difference is sufficiently “real” to warrant correction. In most of the published literature, scholars have considered that the two rates are sufficiently close that we should instead simply say that COMPAS appears to be relatively close to satisfying sufficiency.\nIndeed, in a rejoinder article published by affiliates of the company Northpointe which produced COMPAS, the fact that COMPAS satisfies sufficiency is one of the primary arguments [@flores2016false]."
  },
  {
    "objectID": "Lecture Notes/10-compas.html#recap",
    "href": "Lecture Notes/10-compas.html#recap",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "In these notes, we replicated the data analysis of @angwin2022machine, finding that the COMPAS algorithm has disparate error rates between Black and white defendants. We introduced the idea that fairness actually has several different facets in our moral intuitions, and found that the COMPAS algorithm satisfies one of them (sufficiency: equal scores mean the same thing regardless of your group membership) but not the others (equal prediction rates and equal error rates)."
  },
  {
    "objectID": "Lecture Notes/10-compas.html#some-questions-moving-forward",
    "href": "Lecture Notes/10-compas.html#some-questions-moving-forward",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Can we have it all? Could we modify the COMPAS algorithm in such a way that it satisfies all the ideas of fairness that we discussed above? Could we then call it “fair” or “unbiased?”\nAre there other ways to define fairness? Which ones are most compelling to us? Does the right idea of fairness depend on the context in which we apply it?\nHow did this happen? The COMPAS algorithm was never trained on race data about the defendant. How did it happen that this algorithm nevertheless made recommendations at different rates across groups?\nIs automated decision-making legitimate in this setting? Can it be legitimate (just, fair) to use an automated decision-system for making recommendations about parole and sentencing decisions at all? What safeguards and forms of recourse are necessary for the legitimate use of automated decision-making in criminal justice?\nWhat are the systemic impacts? Disparate sentencing decisions can have downstream impacts on communities and institutions. How could application of the COMPAS algorithm exacerbate systemic inequalities?"
  },
  {
    "objectID": "posts/WiDS-post/Palmer-Penguins-Warmup.html",
    "href": "posts/WiDS-post/Palmer-Penguins-Warmup.html",
    "title": "Donovan's Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/palmer-penguins.csv\"\ndf = pd.read_csv(url)\n\ndf\n\nsummary_table = df.groupby(['Species', 'Sex']).agg({'Body Mass (g)': 'mean'})\n\nprint(summary_table)\n\n# Create a scatterplot\nsns.scatterplot(data=df, x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", hue=\"Species\")\n\n                                                  Body Mass (g)\nSpecies                                   Sex                  \nAdelie Penguin (Pygoscelis adeliae)       FEMALE    3368.835616\n                                          MALE      4043.493151\nChinstrap penguin (Pygoscelis antarctica) FEMALE    3527.205882\n                                          MALE      3938.970588\nGentoo penguin (Pygoscelis papua)         .         4875.000000\n                                          FEMALE    4679.741379\n                                          MALE      5484.836066"
  },
  {
    "objectID": "posts/WiDS-post/index.html",
    "href": "posts/WiDS-post/index.html",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "",
    "text": "In this blog post, we explore the pivotal role of women in the fields of computing, math, and engineering, and the significant impact of conferences like WiDS (Women in Data Science) in fostering change. We delve into the historical context of women’s participation in computing, highlight the current challenges and barriers they face, and celebrate the achievements of notable women scholars who are pushing the boundaries of data science and its application in diverse fields. Through the insights from Professors Amy Yuen, Jessica L’Roe, Laura Biester, and Sarah Brown, we learn about the transformative power of data science in political science, geography, mental health, and the quest for fairness in machine learning. This post aims to illuminate the importance of diversity in STEM and inspire action towards a more inclusive future."
  },
  {
    "objectID": "posts/WiDS-post/index.html#abstract",
    "href": "posts/WiDS-post/index.html#abstract",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "",
    "text": "In this blog post, we explore the pivotal role of women in the fields of computing, math, and engineering, and the significant impact of conferences like WiDS (Women in Data Science) in fostering change. We delve into the historical context of women’s participation in computing, highlight the current challenges and barriers they face, and celebrate the achievements of notable women scholars who are pushing the boundaries of data science and its application in diverse fields. Through the insights from Professors Amy Yuen, Jessica L’Roe, Laura Biester, and Sarah Brown, we learn about the transformative power of data science in political science, geography, mental health, and the quest for fairness in machine learning. This post aims to illuminate the importance of diversity in STEM and inspire action towards a more inclusive future."
  },
  {
    "objectID": "posts/WiDS-post/index.html#part-1-why-spotlight-women-in-data-science",
    "href": "posts/WiDS-post/index.html#part-1-why-spotlight-women-in-data-science",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Part 1: Why Spotlight Women in Data Science?",
    "text": "Part 1: Why Spotlight Women in Data Science?"
  },
  {
    "objectID": "posts/WiDS-post/index.html#professor-amy-yuen",
    "href": "posts/WiDS-post/index.html#professor-amy-yuen",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Professor Amy Yuen",
    "text": "Professor Amy Yuen\nProfessor Amy Yuen is a Professor of Political Science at Middlebury College. Her research focused on the United Nations Security Council and aimed to answer if it is truly a democratic institution. Within the United Nations Security Council, elected members do not have veto power, Professor Yuen hoped to answer whether these members continue to run if this is the case, and if there is a lack of representation within the council. In her research, she measured the number of resolutions, formal meetings and consultations, presidential statements, resolution Co-sponsorships, and Aria-Formula Meetings. She found that sponsorships showed the biggest differences in who is doing what. Another aspect of her research was investigating the members that sit on the council, and the frequency of which. She found that the data showed that countries were geared more towards inclusion over time. She concluded by finding that while the council was not entirely representative, it could be worse. She plans to continue with this research to see how this finding plays into the output and effectiveness of the council. From her talk, I learned how data science can be applied to essentially any aspect of our lives. Before this talk, data science seemed strictly technical and I would have never thought about how it can be used in the context of politics and policy. The talk opened my eyes to the realm of possibilities that data science presents in so many varying fields."
  },
  {
    "objectID": "posts/WiDS-post/index.html#professor-jessica-lroe",
    "href": "posts/WiDS-post/index.html#professor-jessica-lroe",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Professor Jessica L’Roe",
    "text": "Professor Jessica L’Roe\nProfessor Jessica L’Roe is an assistant professor of Geography at Middlebury College. Professor Roe spoke about her research into monitoring change in tropical forest landscapes within East Africa. Within her research, she found that mothers were more willing to invest in education rather than land because it was a better bet for the success of their children. Although her research was an important crux of her talk, one of the main messages she wanted to convey was that all kinds of things can be “data”. Regardless of whether something is abstract or physical, it can often be used to analyze certain behaviors or outcomes. She concluded her talk by emphasizing how women are making large contributions in her field and to just “Go for it”! After listening to her talk, I learned the importance of varying types of data and utilization, but more importantly how it was collected. Her data was collected using the help of locals and traveling from settlement to settlement. It was so interesting to hear this type of data collection as before this, most of the data collection I was used to was simply clicking on a link or downloading a dataset. It showed me that data science can transcend the keyboard and has real implications for REAL people."
  },
  {
    "objectID": "posts/WiDS-post/index.html#professor-laura-biester",
    "href": "posts/WiDS-post/index.html#professor-laura-biester",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Professor Laura Biester",
    "text": "Professor Laura Biester\nProfessor Laura Biester is a Computer Science Professor at Middlebury College. Professor Biester spoke about her research regarding the connection between language and mental health. She used a corpus of Reddit posts and comments from 2006-2019 to predict whether or not language could be used to identify those who were diagnosed with depression before their diagnosis. Through multiple models such as logistic regression and MentalBeRT, she was able to successfully produce a model that was relatively accurate in predicting depression in users using only language. Like Professor L’Roe’s talk, although much of Professor Biester’s talk was focused on her research, a huge point she emphasized was that data collection is a huge part of the data science process - it’s not just about building models. This part struck true to me as I learned that no matter how good your model is, it is only as good as the data it is being supplied with. Data preprocessing and determining which parts of data to use or not use is crucial in the data science process."
  },
  {
    "objectID": "posts/WiDS-post/index.html#professor-sarah-brown",
    "href": "posts/WiDS-post/index.html#professor-sarah-brown",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Professor Sarah Brown",
    "text": "Professor Sarah Brown\nProfessor Sarah Brown is an assistant professor of Computer Science at the University of Rhode Island. In her talk, her research focused on how to make machine learning more fair. More broadly, however, she discussed the implications of data science through the use of “keys”. These keys were: Disciplines are communities, meet people where they are, and understand data’s context. She mentioned three projects and the “lock” for each, which is something she needed to figure out for it all to work and flow.\nThese keys were used to unlock these locks and culminate a successful data science process. The keys were picked up often from something that had no explicit relation to data science itself but were collected from other aspects of her life. For example, one of the skills she picked up from social studies was the ability to use context to understand primary sources. She mentions that oftentimes, data scientists fall into the trap of only thinking like a data scientist and approaching the problem from one particular angle. By using other disciplines/individuals’ perspectives, you may be able to figure out a problem in a more efficient/effective way than you would have otherwise. She concluded by stressing the importance of these three keys and emphasizing the idea of understanding “why” rather than just “doing” when it comes to data science. Through this, I learned key principles that will influence the future of my computer science journey."
  },
  {
    "objectID": "posts/WiDS-post/index.html#conclusion",
    "href": "posts/WiDS-post/index.html#conclusion",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Conclusion",
    "text": "Conclusion\nThis exploration into the status of women in computing and the supportive role of initiatives like WiDS has been enlightening. It has underscored the importance of diversity for innovation and the vast potential of data science to impact various aspects of society positively. From the historical context of women’s evolving role in computing to the cutting-edge research conducted by leading female data scientists, this journey has highlighted both the challenges and the incredible opportunities that lie ahead. As we look forward to a more inclusive and equitable future in STEM, the lessons learned here inspire continued advocacy and action. My next steps involve deepening my understanding of how to support diversity in tech and exploring more about how data science can be leveraged for social good."
  },
  {
    "objectID": "posts/health-blog-post/index.html",
    "href": "posts/health-blog-post/index.html",
    "title": "Replication Study, Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "",
    "text": "This blog post delves into the intricacies of algorithmic bias within healthcare management systems, inspired by Obermeyer et al. (2019), which highlighted racial biases in an algorithm used to manage health populations. By reconstructing key figures from the study and applying statistical models to assess cost disparities between Black and white patients, our analysis aims to quantify the disparity in healthcare costs and explore the nonlinear relationships between chronic conditions and healthcare expenditures. Our findings suggest that Black patients incur approximately 96.92% of the healthcare costs of white patients when adjusted for the number of chronic conditions, supporting the notion of underlying biases in healthcare algorithms that can perpetuate disparities in treatment and resource allocation."
  },
  {
    "objectID": "posts/health-blog-post/index.html#abstract",
    "href": "posts/health-blog-post/index.html#abstract",
    "title": "Replication Study, Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "",
    "text": "This blog post delves into the intricacies of algorithmic bias within healthcare management systems, inspired by Obermeyer et al. (2019), which highlighted racial biases in an algorithm used to manage health populations. By reconstructing key figures from the study and applying statistical models to assess cost disparities between Black and white patients, our analysis aims to quantify the disparity in healthcare costs and explore the nonlinear relationships between chronic conditions and healthcare expenditures. Our findings suggest that Black patients incur approximately 96.92% of the healthcare costs of white patients when adjusted for the number of chronic conditions, supporting the notion of underlying biases in healthcare algorithms that can perpetuate disparities in treatment and resource allocation."
  },
  {
    "objectID": "posts/health-blog-post/index.html#part-a-data-access",
    "href": "posts/health-blog-post/index.html#part-a-data-access",
    "title": "Replication Study, Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Part A: Data Access",
    "text": "Part A: Data Access\nFor this blog post, we will be using the file from labsysmed below in order to access the data.\n\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)"
  },
  {
    "objectID": "posts/health-blog-post/index.html#part-b-reproducing-fig.-1",
    "href": "posts/health-blog-post/index.html#part-b-reproducing-fig.-1",
    "title": "Replication Study, Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Part B: Reproducing Fig. 1",
    "text": "Part B: Reproducing Fig. 1\nIn this section, we will reproduce Fig. 1 from the paper, Obermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Science 366 (6464): 447–53. https://doi.org/10.1126/science.aax2342.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf['race'] = df['race'].str.lower()\n\nprint(\"Race counts:\", df['race'].value_counts())\n\ndf['gender'] = df['dem_female'].map({1: 'Female', 0: 'Male'})\n\nprint(\"Gender counts:\", df['gender'].value_counts())\n\ngenders = ['Male', 'Female']\nraces = ['black', 'white']\n\nmarkers = {'black': 'o', 'white': 'x'}\n\nRace counts: race\nwhite    43202\nblack     5582\nName: count, dtype: int64\nGender counts: gender\nFemale    30763\nMale      18021\nName: count, dtype: int64\n\n\nTo begin I provided a breakdown of the dataset by race and gender in order to give some context to the data.\n\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(12, 10), sharey=True)\n\nfor i, gender in enumerate(genders):\n    for race in races:\n        subset = df[(df['gender'] == gender) & (df['race'] == race)]\n        if subset.empty:\n            print(f\"No data for {gender} and {race}\")\n            continue\n\n        # Calculate percentiles of risk score\n        percentiles = np.linspace(0, 100, 101)  \n        percentile_scores = np.percentile(subset['risk_score_t'], percentiles)\n\n        # Calculate mean number of chronic conditions for each percentile\n        mean_conditions = [subset[subset['risk_score_t'] &lt;= score]['gagne_sum_t'].mean() for score in percentile_scores]\n        \n        # Plot\n        ax = axes[i]\n        ax.scatter(mean_conditions, percentiles, label=f'{race.capitalize()}')\n        \n        ax.set_title(f'{gender} Patients')\n        ax.set_xlabel('Mean Number of Active Chronic Conditions')\n        ax.set_ylabel('Percentile of Risk Score')\n        ax.legend(title='Race')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Figures of the mean number of active chronic conditions for each percentile of risk score, by gender and race\n\n\n\n\n\nAlgorithm scores are a key input to decisions about future enrollment in a care coordination program. So as we might expect, with less-healthy Blacks scored at similar risk scores to more-healthy Whites, we find evidence of substantial disparities in program screening.\nNote that for Fig 1 in the paper, the number of chronic conditions, ranged from 0 to 5, our plot here ranged from 0 to 2. While not exact, it still provides a similar representation as seen in the paper.\nSay for example that Patient A is Black, Patient B is White, and that both Patient A and Patient B have exactly the same chronic illnesses. Due to the disparity seen above, Patient A is less likely to be referred to the high risk care management program due to the current risk score screening practices."
  },
  {
    "objectID": "posts/health-blog-post/index.html#part-c-reproducing-fig-3.",
    "href": "posts/health-blog-post/index.html#part-c-reproducing-fig-3.",
    "title": "Replication Study, Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Part C: Reproducing Fig 3.",
    "text": "Part C: Reproducing Fig 3.\nIn this section, we will be focusing on reproducing Figure 3 from Obermeyer et al. (2019). Which focused on the relationship between medical expenditures and percentile of risk score and the relationship between medical expenditures and number of chronic illnesses.\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n\n# Plot 1: Mean Medical Expenditure by Percentile of Risk Score\nfor race in ['black', 'white']:\n    subset = df[df['race'] == race]\n    percentiles = np.linspace(0, 100, 101)\n    percentile_scores = np.percentile(subset['risk_score_t'], percentiles)\n    mean_costs = [subset[subset['risk_score_t'] &lt;= score]['cost_t'].mean() for score in percentile_scores]\n    axes[0].scatter(percentiles, mean_costs, marker=markers[race], label=f'{race.capitalize()}')\naxes[0].set_title('Mean Medical Expenditure by Percentile of Risk Score')\naxes[0].set_xlabel('Percentile of Risk Score')\naxes[0].set_ylabel('Mean Medical Expenditure')\naxes[0].legend()\n\n# Plot 2: Mean Medical Expenditure by Number of Chronic Illnesses\nfor race in ['black', 'white']:\n    subset = df[df['race'] == race]\n    unique_illnesses = sorted(subset['gagne_sum_t'].unique())\n    mean_costs = [subset[subset['gagne_sum_t'] == illness]['cost_t'].mean() for illness in unique_illnesses]\n    axes[1].scatter(unique_illnesses, mean_costs, marker=markers[race], label=f'{race.capitalize()}')\naxes[1].set_title('Mean Medical Expenditure by Number of Chronic Illnesses')\naxes[1].set_xlabel('Number of Chronic Illnesses')\naxes[1].set_ylabel('Mean Medical Expenditure')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 2: Scatterplots of mean medical expenditure by percentile of risk score and number of chronic illnesses\n\n\n\n\n\nFrom reproducing Figure 3, we find that there are small disparities in cost between races, but rather large disparities in health conditional on risk as seen in Figure 1\n\nPart D: Modeling Cost Disparity"
  },
  {
    "objectID": "posts/health-blog-post/index.html#data-prep",
    "href": "posts/health-blog-post/index.html#data-prep",
    "title": "Replication Study, Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Data Prep",
    "text": "Data Prep\nAs seen in Figure 2, there is a relatively stable pattern of disparity in the cost incurred by black and white patients with 5 or fewer chronic conditions, but this pattern begins to swing wildly in one direction or another as the number of active chronic conditions increases.\nDue to this, we will focus on patients with 5 or fewer active chronic conditions.\nWe will explore the percentage of patients in the dataset with 5 or fewer active chronic conditions, create a new column in the data set called log-transform which will act as our target variable for our model, create a dummy column for the qualitative race variable, and separate the data into predictor variables X and target variable y.\n\ndf['five_or_less'] = df['gagne_sum_t'] &lt;= 5\npercentage_five_or_less = df['five_or_less'].mean() * 100\nprint(f\"Percentage of patients with 5 or fewer chronic conditions: {percentage_five_or_less:.2f}%\")\n\nPercentage of patients with 5 or fewer chronic conditions: 95.54%\n\n\nAs seen with the percentage of patients with 5 or fewer active chronic conditions, the justification for focusing on this subset of the data is that the disparities in cost between black and white patients are relatively stable for this subset of the data and it accounts for a large portion of the data.\n\n# Log transform of the cost; first, remove patients with $0 medical costs\ndf = df[df['cost_t'] &gt; 0]\ndf['log_cost'] = np.log(df['cost_t'])\n\n# Create a dummy column for race where 0 = white and 1 = black\ndf['race_dummy'] = (df['race'] == 'black').astype(int)\n\n# Separate the data into predictor variables X and target variable y\nX = df[['race_dummy', 'gagne_sum_t']]\ny = df['log_cost']"
  },
  {
    "objectID": "posts/health-blog-post/index.html#modeling",
    "href": "posts/health-blog-post/index.html#modeling",
    "title": "Replication Study, Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Modeling",
    "text": "Modeling\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\n# Define a function to add polynomial features to the dataset\ndef add_polynomial_features(X, degree):\n    poly = PolynomialFeatures(degree)\n    X_poly = poly.fit_transform(X)\n    return X_poly, poly.get_feature_names(X.columns)\n\n# Evaluate models with different degrees of polynomial features using cross-validation\nscores = []\nmax_degree = 10\nfor degree in range(1, max_degree + 1):\n\n    pipeline = Pipeline([\n        ('poly', PolynomialFeatures(degree=degree)),\n        ('linear', LinearRegression())\n    ])\n    X_features = X.drop(columns='race_dummy')\n    X_poly = np.hstack([X[['race_dummy']].values, X_features])  # Adding the race dummy variable\n    score = cross_val_score(pipeline, X_poly, y, cv=5, scoring='neg_mean_squared_error')\n    scores.append(np.mean(score))\n\nIn the modeling part of the analysis, we used polynomial regression to explore and quantify disparities in healthcare costs between Black and white patients"
  },
  {
    "objectID": "posts/health-blog-post/index.html#discussion",
    "href": "posts/health-blog-post/index.html#discussion",
    "title": "Replication Study, Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Discussion",
    "text": "Discussion\nThe analysis conducted in this blog post reiterates the critical insights from Obermeyer et al. (2019), revealing subtle yet significant disparities in healthcare costs that suggest potential biases in the algorithms used for managing health populations. By employing polynomial regression models, we confirmed that the relationship between the number of chronic conditions and the associated healthcare costs is nonlinear and varies by race, reinforcing the need for models that adequately reflect the complexity of real-world data.\nThe crucial flaw was that the algorithm equated higher past costs with higher future needs without accounting for the fact that systemic inequalities often lead to underutilization of healthcare services by certain racial groups, particularly Black patients. As a result, despite having similar or even more severe chronic conditions, Black patients had been systematically assigned lower risk scores than white patients with comparable health conditions. This led to fewer healthcare resources being directed towards them, exacerbating existing health disparities.\nThe study by Obermeyer et al. is best described using the “conditional statistical parity” criterion discussed in Chapter 3 of Barocas, Hardt, and Narayanan (2023). This criterion pertains to ensuring that a decision-making process (like an algorithm) meets a fairness condition, specifically, that it is statistically independent of sensitive attributes (like race) conditioned on a legitimate set of variables (like health needs). Obermeyer et al. (2019) revealed that the algorithm’s reliance on healthcare costs as a proxy for health needs failed to account for the broader social and economic factors that influence these costs, leading to an underestimation of health needs for Black patients relative to white patients who incur similar healthcare costs."
  },
  {
    "objectID": "posts/mid-course-reflection/mid-course.html",
    "href": "posts/mid-course-reflection/mid-course.html",
    "title": "Mid-Course Reflection",
    "section": "",
    "text": "Donovan Wood\n\n\n\n\n\nHow often have you attended class? (e.g. “almost always,” “I missed three times,” etc.) Almost always, have only missed due to sickness.\nHow often have you taken notes on the core readings ahead of the class period? Again, almost always, except when I am sick\nHow often have you been prepared to present the daily warm-up exercise to your team, even if you weren’t actually called? 100% of the time so far.\nHow many times have you actually presented the daily warm-up to your team? Twice I believe.\nHow many times have you asked your team for help while presenting the daily warm-up? None, they weren’t ones I had questions on, I do, however, ask for my team’s input if anything is particularly interesting.\nHow often have you learned something new from a teammate’s presentation of the daily warm-up? Once or twice, our team usually does a good job in discussing all of the warm up when we are in our own groups.\nHow often have you helped a teammate during the daily warm-up presentation? Never actually, every time our teammates have had no trouble with the warmup.\n\n\n\n\n\nHow often have you attended Student Hours or Peer Help? Occasionally, they often conflict with my other classes or work.\nHow often have you asked for or received help from your fellow students? Hmm, not as often as I would like, but a couple times before class. Just to see if other folks had the same questions I had.\nHave you been regularly participating in a study group outside class? No, unfortunately. I wish I had a study group though!\nHow often have you posted questions or answers in Slack? Well not in the main questions tab, but I have been asking Professor Chondrow questions quite often recently via slack\n\n\n\n\n\nHow many blog posts have you submitted? 2\nHow many of your submitted blog posts are at each of the following feedback stages?\n\nE: No revisions suggested:\nM: Revisions useful: 1\nR: Revisions encouraged:\nN: Incomplete:\n\nRoughly how many hours per week have you spent on this course outside of class? Per week probably 15 lately, I’ve been working on a bunch of Blog Posts concurrently\n\n\n\n\n\nAt the beginning of the course, you may have expressed an interest in focusing a little extra on one or two of the following four categories:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nDid you choose to focus on any of these categories? If so, what have you done in order to pursue your interest?\nI would say mainly Theory and Implementation are the ones I have been focusing on lately. One of the main things I wanted to learn with this class was the implementation of ML in regards to the financial market. Through that interest, I have found a vast amount of information online with so many different communities and tips.\nI hope to use these in order to complete my final project and learn a lot through the process!\n\n\n\nFor each of the categories below, replace the “[your response here]” cell with 1-2 paragraphs in which you reflect on the following questions:\n\nIn what ways are you on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what you are doing in order to meet it.\nIn what ways are you not on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what gap you see between where you are and your goal.\nIf there’s any context you want to share about how you are faring relative to your goals, please do!\n\n\n\nAs of now, I’m currently working on three blog posts concurrently with one almost done and two nearing write up stage. On top of that I have two blog posts already submitted. All things considered, my goal of trying to complete every blog post was rather ambitious but I still want to keep it as a goal even if I don’t reach it in order to keep me motivated. One of the things I wish I did was work on my blog posts a bit over Spring Break but alas, the beach got to me unfortunately.\n\n\n\nBesides being sick a few days, I have went to class and completed the warm ups. While yes, theoretically I could have went back and did the warm ups for the classes I was sick for, I think this is a good benchmark of where I set out to be at the beginning of the semester.\nLooking forward, as the semester comes to a close, inevitably there will be deadlines and other projects that may take away from this, but I hope to not let that be the case and keep continuing onward.\n\n\n\nWell, since our project proposals were only due today, the stage of the project is still very early. That being said, I met with four individuals that seem to have a similar interest/idea to mine and I hope to work with them all in order to create something truly awesome!\nFrom talking with them, we all seem to recognize that the task will be hard and more unlike the work we have done currently in class but I believe that is part of the allure of the project. Hopefully by the end of the semester I can look back at our project and be proud of the model we created!\n\n\n\nIs there anything else that you want to share with me about what you have learned, how you have participated, or what you have achieved in CSCI 0451?\nI haven’t learned/had this much fun/struggled at times with a class in a pretty long time. I’m excited I was able to take it and look forward to developing my skills further!\n\n\n\nFrom your experience in CSCI 0451 and your other classes this semester, you may feel moved to make modifications to your goals. Are they still feasible? Too ambitious? Not ambitious enough? If you would like to revise any of your goals from your reflective goal-setting, you can do so below. For each goal you want to modify:\n\nClearly state what the goal was.\nClearly state how you’ve done on that goal so far.\nClearly state your proposed revised goal for the remainder of the course.\n\nI want to maintain goals that I had at the beginning of the semester. I feel as though it would be a disservice to my past self if I stopped pressing the go button. I want to learn as much as possible! I’ll keep trying to do so unless something drastic comes up.\n\n\n\n\nTake 15 minutes to look back on your responses in each of the sections above. Then, state the letter grade that you feel reflects your learning, participation, and achievement in CSCI 0451 so far, and contextualize it against some of the soundbytes below.\n\n\nAn A sounds like:\n\n“I am very proud of my time in this course.”\n“I have grown significantly in multiple ways that matter to me.”\n“I am ready to take the theory, techniques, and ideas of this course into my future classes, projects, hobbies, or career.”\n\nA B sounds like:\n\n“I had some opportunities to learn more, overall I feel good about my time in this course.”\n“I am able to explain some new things or achieve new tasks.”\n“I can see a few ideas from this course that will be relevant for my future classes, projects, hobbies, or career.”\n\nA C sounds like:\n\n“I often made a good effort, but I missed many opportunities to get more out of my time in this course.”\n“I might be able to complete some new tasks related to the course content, but only with significant further guidance.”\n“I don’t see any ways to take the contents of this course into my future classes, projects, hobbies, or career.”\n\nYou might find that some of these soundbytes resonate and other’s don’t! Take some time, see what feels right, and don’t be afraid to celebrate your achievements.\n\nUpon reflection, I feel that my learning, participation, and achievement in CSCI 0451 (so far) are best reflected by a grade of A-\n\n\nA way in which I resonate with the soundbytes for that grade above is…\n\nI truly am proud of my time spent so far in the course. It’s been a lot of time and effort but I’m proud of myself for not burning out and staying excited. I have learned a lot so far, but more importantly was actively seeking additional information that was interesting to me. I often find myself just ‘doing’ class so I’m excited that I actually WANT to do extra and learn. Lastly, I’m hoping this project will teach me a lot about this area of computing and be something I can look back on proudly!"
  },
  {
    "objectID": "posts/mid-course-reflection/mid-course.html#the-data",
    "href": "posts/mid-course-reflection/mid-course.html#the-data",
    "title": "Mid-Course Reflection",
    "section": "",
    "text": "How often have you attended class? (e.g. “almost always,” “I missed three times,” etc.) Almost always, have only missed due to sickness.\nHow often have you taken notes on the core readings ahead of the class period? Again, almost always, except when I am sick\nHow often have you been prepared to present the daily warm-up exercise to your team, even if you weren’t actually called? 100% of the time so far.\nHow many times have you actually presented the daily warm-up to your team? Twice I believe.\nHow many times have you asked your team for help while presenting the daily warm-up? None, they weren’t ones I had questions on, I do, however, ask for my team’s input if anything is particularly interesting.\nHow often have you learned something new from a teammate’s presentation of the daily warm-up? Once or twice, our team usually does a good job in discussing all of the warm up when we are in our own groups.\nHow often have you helped a teammate during the daily warm-up presentation? Never actually, every time our teammates have had no trouble with the warmup.\n\n\n\n\n\nHow often have you attended Student Hours or Peer Help? Occasionally, they often conflict with my other classes or work.\nHow often have you asked for or received help from your fellow students? Hmm, not as often as I would like, but a couple times before class. Just to see if other folks had the same questions I had.\nHave you been regularly participating in a study group outside class? No, unfortunately. I wish I had a study group though!\nHow often have you posted questions or answers in Slack? Well not in the main questions tab, but I have been asking Professor Chondrow questions quite often recently via slack\n\n\n\n\n\nHow many blog posts have you submitted? 2\nHow many of your submitted blog posts are at each of the following feedback stages?\n\nE: No revisions suggested:\nM: Revisions useful: 1\nR: Revisions encouraged:\nN: Incomplete:\n\nRoughly how many hours per week have you spent on this course outside of class? Per week probably 15 lately, I’ve been working on a bunch of Blog Posts concurrently"
  },
  {
    "objectID": "posts/mid-course-reflection/mid-course.html#what-youve-learned",
    "href": "posts/mid-course-reflection/mid-course.html#what-youve-learned",
    "title": "Mid-Course Reflection",
    "section": "",
    "text": "At the beginning of the course, you may have expressed an interest in focusing a little extra on one or two of the following four categories:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nDid you choose to focus on any of these categories? If so, what have you done in order to pursue your interest?\nI would say mainly Theory and Implementation are the ones I have been focusing on lately. One of the main things I wanted to learn with this class was the implementation of ML in regards to the financial market. Through that interest, I have found a vast amount of information online with so many different communities and tips.\nI hope to use these in order to complete my final project and learn a lot through the process!"
  },
  {
    "objectID": "posts/mid-course-reflection/mid-course.html#reflecting-on-goals",
    "href": "posts/mid-course-reflection/mid-course.html#reflecting-on-goals",
    "title": "Mid-Course Reflection",
    "section": "",
    "text": "For each of the categories below, replace the “[your response here]” cell with 1-2 paragraphs in which you reflect on the following questions:\n\nIn what ways are you on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what you are doing in order to meet it.\nIn what ways are you not on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what gap you see between where you are and your goal.\nIf there’s any context you want to share about how you are faring relative to your goals, please do!\n\n\n\nAs of now, I’m currently working on three blog posts concurrently with one almost done and two nearing write up stage. On top of that I have two blog posts already submitted. All things considered, my goal of trying to complete every blog post was rather ambitious but I still want to keep it as a goal even if I don’t reach it in order to keep me motivated. One of the things I wish I did was work on my blog posts a bit over Spring Break but alas, the beach got to me unfortunately.\n\n\n\nBesides being sick a few days, I have went to class and completed the warm ups. While yes, theoretically I could have went back and did the warm ups for the classes I was sick for, I think this is a good benchmark of where I set out to be at the beginning of the semester.\nLooking forward, as the semester comes to a close, inevitably there will be deadlines and other projects that may take away from this, but I hope to not let that be the case and keep continuing onward.\n\n\n\nWell, since our project proposals were only due today, the stage of the project is still very early. That being said, I met with four individuals that seem to have a similar interest/idea to mine and I hope to work with them all in order to create something truly awesome!\nFrom talking with them, we all seem to recognize that the task will be hard and more unlike the work we have done currently in class but I believe that is part of the allure of the project. Hopefully by the end of the semester I can look back at our project and be proud of the model we created!\n\n\n\nIs there anything else that you want to share with me about what you have learned, how you have participated, or what you have achieved in CSCI 0451?\nI haven’t learned/had this much fun/struggled at times with a class in a pretty long time. I’m excited I was able to take it and look forward to developing my skills further!\n\n\n\nFrom your experience in CSCI 0451 and your other classes this semester, you may feel moved to make modifications to your goals. Are they still feasible? Too ambitious? Not ambitious enough? If you would like to revise any of your goals from your reflective goal-setting, you can do so below. For each goal you want to modify:\n\nClearly state what the goal was.\nClearly state how you’ve done on that goal so far.\nClearly state your proposed revised goal for the remainder of the course.\n\nI want to maintain goals that I had at the beginning of the semester. I feel as though it would be a disservice to my past self if I stopped pressing the go button. I want to learn as much as possible! I’ll keep trying to do so unless something drastic comes up."
  },
  {
    "objectID": "posts/mid-course-reflection/mid-course.html#grade-and-goals",
    "href": "posts/mid-course-reflection/mid-course.html#grade-and-goals",
    "title": "Mid-Course Reflection",
    "section": "",
    "text": "Take 15 minutes to look back on your responses in each of the sections above. Then, state the letter grade that you feel reflects your learning, participation, and achievement in CSCI 0451 so far, and contextualize it against some of the soundbytes below.\n\n\nAn A sounds like:\n\n“I am very proud of my time in this course.”\n“I have grown significantly in multiple ways that matter to me.”\n“I am ready to take the theory, techniques, and ideas of this course into my future classes, projects, hobbies, or career.”\n\nA B sounds like:\n\n“I had some opportunities to learn more, overall I feel good about my time in this course.”\n“I am able to explain some new things or achieve new tasks.”\n“I can see a few ideas from this course that will be relevant for my future classes, projects, hobbies, or career.”\n\nA C sounds like:\n\n“I often made a good effort, but I missed many opportunities to get more out of my time in this course.”\n“I might be able to complete some new tasks related to the course content, but only with significant further guidance.”\n“I don’t see any ways to take the contents of this course into my future classes, projects, hobbies, or career.”\n\nYou might find that some of these soundbytes resonate and other’s don’t! Take some time, see what feels right, and don’t be afraid to celebrate your achievements.\n\nUpon reflection, I feel that my learning, participation, and achievement in CSCI 0451 (so far) are best reflected by a grade of A-\n\n\nA way in which I resonate with the soundbytes for that grade above is…\n\nI truly am proud of my time spent so far in the course. It’s been a lot of time and effort but I’m proud of myself for not burning out and staying excited. I have learned a lot so far, but more importantly was actively seeking additional information that was interesting to me. I often find myself just ‘doing’ class so I’m excited that I actually WANT to do extra and learn. Lastly, I’m hoping this project will teach me a lot about this area of computing and be something I can look back on proudly!"
  },
  {
    "objectID": "posts/bias-post/index.html",
    "href": "posts/bias-post/index.html",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "",
    "text": "This essay critically examines Arvind Narayanan’s assertion that quantitative methods used to assess bias and discrimination “primarily justify the status quo and do more harm than good.” By thoroughly exploring Narayanan’s arguments alongside related scholarly works and additional research, this analysis investigates the complex nature of quantitative methods in both perpetuating and addressing systemic biases."
  },
  {
    "objectID": "posts/bias-post/index.html#abstract",
    "href": "posts/bias-post/index.html#abstract",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "",
    "text": "This essay critically examines Arvind Narayanan’s assertion that quantitative methods used to assess bias and discrimination “primarily justify the status quo and do more harm than good.” By thoroughly exploring Narayanan’s arguments alongside related scholarly works and additional research, this analysis investigates the complex nature of quantitative methods in both perpetuating and addressing systemic biases."
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#part-e-evaluating-the-model-from-the-banks-perspective",
    "href": "posts/New-test-thing-post/index.html#part-e-evaluating-the-model-from-the-banks-perspective",
    "title": "Whose Costs?",
    "section": "Part E: Evaluating the Model from the Bank’s Perspective",
    "text": "Part E: Evaluating the Model from the Bank’s Perspective\nAfter finding an optimal threshold value of 0.25, let’s see how our model compares to the test set."
  },
  {
    "objectID": "Lecture Notes/stochastic-gradient.html",
    "href": "Lecture Notes/stochastic-gradient.html",
    "title": "Donovan's Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import torch\nfrom matplotlib import pyplot as plt \nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef regression_data(n = 100, w = torch.Tensor([-0.7, 0.5]), x_max = 1):\n\n    x = torch.rand(n)*x_max\n    y = x*w[1] + w[0] + 0.05*torch.randn(n)\n    return x, y\n\nx, y = regression_data()\n\nplt.scatter(x, y, facecolors = \"none\", edgecolors = \"steelblue\")\nlabs = plt.gca().set(xlabel = r\"$x$\", ylabel = r\"$y$\")\n\n\n\n\n\n\n\n\n\n# Empirical risk calculation function\ndef empirical_risk(w0, w1, x, y):\n    return ((y - w1 * x - w0)**2).mean()\n\n# Weight update function for SGD\ndef update_weights(w0, w1, x, y, alpha, t, idx):\n    i = idx\n    err = y[i] - w1 * x[i] - w0\n    w0 -= (2 * alpha / t) * w0 * err\n    w1 -= (2 * alpha / t) * w1 * err * x[i]\n    return w0, w1\n\n# Stochastic Gradient Descent function for regression\ndef sgd_regression(x, y, alpha, t_max, plot_single_epoch=False):\n    w0, w1 = torch.randn(1), torch.randn(1)  # Initial random weights\n    risk_over_time = []\n\n    for t in range(1, t_max + 1):\n        perm = torch.randperm(x.size(0))\n        risks = []\n\n        for i in perm:\n            w0, w1 = update_weights(w0, w1, x, y, alpha, t, i)\n            if plot_single_epoch and t == 1:\n                current_risk = empirical_risk(w0, w1, x, y)\n                risks.append(current_risk)\n\n        end_epoch_risk = empirical_risk(w0, w1, x, y)\n        risk_over_time.append(end_epoch_risk)\n\n        if plot_single_epoch and t == 1:\n            plt.figure(figsize=(10, 4))\n            plt.plot(risks, label='Empirical Risk During First Epoch')\n            plt.xlabel('Iteration')\n            plt.ylabel('Empirical Risk')\n            plt.title('Empirical Risk Evolution During the First Epoch')\n            plt.legend()\n            plt.show()\n\n    return w0, w1, risk_over_time\n\n# Parameters for SGD\nalpha = 0.01\nt_max = 100\n\n# Generate data\nx, y = regression_data()\n\n# Run the SGD regression\nfinal_w0, final_w1, risks = sgd_regression(x, y, alpha, t_max, plot_single_epoch=True)\n\n# Plot the empirical risk over 100 epochs\nplt.figure(figsize=(10, 4))\nplt.plot(risks, label='Empirical Risk at the End of Each Epoch')\nplt.xlabel('Epoch')\nplt.ylabel('Empirical Risk')\nplt.title('Empirical Risk Evolution Over 100 Epochs')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/perceptron-post/index.html",
    "href": "posts/perceptron-post/index.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "In this blog post, I implement the Perceptron algorithm from scratch in Python. The Perceptron algorithm is a simple algorithm for learning a binary classifier. It is the simplest type of artificial neural network. It is a model of a single neuron that can be used for two-class classification problems. The Perceptron algorithm is a linear classifier, which means it can only be used for linearly separable data. We find that it does not work well on non-linearly separable data. We implement a Mini-batch Perceptron algorithm to improve the convergence speed of the Perceptron algorithm and to better deal with non-linearly separable data. We perform various experiments to evaluate the performance of the Perceptron algorithm and the Mini-batch Perceptron algorithm on synthetic datasets and real-world datasets. We find that the Mini-batch Perceptron algorithm converges faster than the Perceptron algorithm and is more robust to non-linearly separable data."
  },
  {
    "objectID": "posts/perceptron-post/index.html#abstract",
    "href": "posts/perceptron-post/index.html#abstract",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "In this blog post, I implement the Perceptron algorithm from scratch in Python. The Perceptron algorithm is a simple algorithm for learning a binary classifier. It is the simplest type of artificial neural network. It is a model of a single neuron that can be used for two-class classification problems. The Perceptron algorithm is a linear classifier, which means it can only be used for linearly separable data. We find that it does not work well on non-linearly separable data. We implement a Mini-batch Perceptron algorithm to improve the convergence speed of the Perceptron algorithm and to better deal with non-linearly separable data. We perform various experiments to evaluate the performance of the Perceptron algorithm and the Mini-batch Perceptron algorithm on synthetic datasets and real-world datasets. We find that the Mini-batch Perceptron algorithm converges faster than the Perceptron algorithm and is more robust to non-linearly separable data."
  },
  {
    "objectID": "posts/perceptron-post/index.html#part-a-implement-perceptron",
    "href": "posts/perceptron-post/index.html#part-a-implement-perceptron",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Part A: Implement Perceptron",
    "text": "Part A: Implement Perceptron\nIn this section of the blog post, I will be implementing the Perceptron algorithm from scratch. The Perceptron algorithm is a type of linear classifier, which is used to classify data points into one of two classes. The algorithm works by finding a hyperplane that separates the data points into two classes. The hyperplane is defined by a set of weights and a bias term. The weights are used to determine the orientation of the hyperplane, while the bias term is used to determine the position of the hyperplane in the feature space.\nThe Perceptron algorithm works by iteratively updating the weights and bias term in order to minimize the classification error. The algorithm starts with an initial set of weights and bias term, and then iterates over the training data points. For each data point, the algorithm computes the predicted class label based on the current weights and bias term, and then updates the weights and bias term based on the error between the predicted class label and the true class label.\nThe Perceptron algorithm is a simple and efficient algorithm for binary classification tasks. It is a type of online learning algorithm, which means that it updates the weights and bias term based on each data point in the training set. The algorithm is guaranteed to converge if the data is linearly separable, and it can be extended to handle non-linearly separable data by using a kernel function.\nIn the following sections, I will implement the Perceptron algorithm in Python and test it on a synthetic dataset. I will also visualize the decision boundary learned by the Perceptron algorithm to see how well it separates the data points into two classes.\nLet’s get started by implementing the Perceptron algorithm in Python.\nThis is done in perceptron.py file. I will not cover the entirety of the coding file, but will touch on the perceptron.grad() portion of the code:\n\n# grad() function from perceptron.py\n\ndef grad(self, X, y):\n       # Computes the vector to add to the weights to minimize the loss\n       s = X@self.w\n       return (s*y &lt; 0)*X*y\n\nIn the first line of grad() function, we calculate the inner product:\n\\(s_i = \\langle \\mathbf{w}^{(t)}, \\mathbf{x}_i \\rangle\\)\nX and self.w are used to compute this inner product with the @ operator from torch\nIn the final line of the grad() function we use the result from the first line to calculate:\n\\([s_i y_i &lt; 0] y_i \\mathbf{x}_i\\)\nWith that said, let’s begin testing our implementation\nIn order to test the implementation, I will run the “minimal training loop”. We need some data to first test this on, so below we will implement some testing data.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\nimport torch\nfrom matplotlib import pyplot as plt\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\n# define function to plot data\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n# set seed\ntorch.manual_seed(1234)\n\n# create linearly separable data\nX_ls, y_ls = perceptron_data(n_points = 50, noise = 0.3)\n\n# plot linearly separable data\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X_ls, y_ls, ax)\nax.set_title(\"Our Linearly Separable Data\");\n\n\n\n\n\n\n\n\nWith our data generated, we can now test the Perceptron algorithm by running the minimal training loop. From what we have seen so far, there is a clear separation between the two classes, so the Perceptron algorithm should be able to learn a decision boundary that separates the data points into two classes. Let’s run the minimal training loop and visualize the decision boundary learned by the Perceptron algorithm.\n\n# set seed\ntorch.manual_seed(1234567)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X_ls, y_ls) \n    loss_vec.append(loss)\n    \n    # perform a perceptron update using the random data point\n    opt.step(X_ls, y_ls)\n\nWe can track the progress of our training by checking the values of the loss function over time:\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\nFigure 1: Loss as a function of the number of perceptron updates\n\n\n\n\n\nFrom this, we can see that training completed with the achievement of zero loss; that is, perfect training accuracy. With this in mind, we can now move on to the next section of the blog post, where we will implement the Perceptron algorithm further."
  },
  {
    "objectID": "posts/perceptron-post/index.html#part-b-experiments",
    "href": "posts/perceptron-post/index.html#part-b-experiments",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Part B: Experiments",
    "text": "Part B: Experiments\nWith a functional implementation of the perceptron algorithm, we can now move on to the next section of the blog post, where we will experiment with the algorithm."
  },
  {
    "objectID": "posts/perceptron-post/index.html#part-c-minibatch-perceptron",
    "href": "posts/perceptron-post/index.html#part-c-minibatch-perceptron",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Part C: MiniBatch Perceptron",
    "text": "Part C: MiniBatch Perceptron\nIn this section of the blog post, I will be implementing the MiniBatch Perceptron algorithm from scratch. The MiniBatch Perceptron algorithm is a variant of the Perceptron algorithm that updates the weights and bias term based on a mini-batch of data points, rather than updating the weights and bias term based on each data point in the training set. This can lead to faster convergence and better generalization performance, especially when the training data is large.\nMathematically, the MiniBatch Perceptron algorithm works as follows:\n\nRandomly select an initial decision boundary \\(\\mathbf{w}^{(0)}\\)\nIteratively:\nSample \\(k\\) random integers \\(i_1, i_2, ..., i_k \\in \\{1,\\ldots,n\\}\\) without replacement.\nUpdate the decision boundary:\n\\(\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + \\frac{\\alpha}{k} \\sum_{j=1}^k \\mathbb{1} [\\langle \\mathbf{w}^{(t)}, \\mathbf{x}_{i_j} \\rangle y_{i_j} &lt; 0] y_{i_j} \\mathbf{x}_{i_j}\\)\n\nMy implementation can be found at minibatch_perceptron.py. Fitting the MiniBatch perceptron model is the same as the normal perceptron model just with the addition of \\(k\\) and \\(\\alpha\\) as hyperparameters.\nThe \\(k\\) paramater represents the number of data points to sample in each iteration, while the \\(\\alpha\\) parameter represents the learning rate of the algorithm. The learning rate controls how much the weights and bias term are updated in each iteration. A higher learning rate will result in larger updates to the weights and bias term, while a lower learning rate will result in smaller updates.\nWe will perform 3 experiments, C1, C2, and C3 to test the MiniBatch Perceptron algorithm on different datasets.\nLet’s begin by defining the experiment code for all of the experiments:\n\nfrom minibatch_perceptron import MiniBatchPerceptron, MiniBatchPerceptronOptimizer\n\ndef experiment(X, y, k, alpha):  \n    # set seed\n    torch.manual_seed(1234567)\n\n   # MiniBatch Perceptron\n    mb_p = MiniBatchPerceptron() \n    mb_opt = MiniBatchPerceptronOptimizer(mb_p)\n\n    # define loss variable\n    mb_loss = 1.0\n\n    # for keeping track of loss values\n    mb_loss_vec = []\n\n    # for recording iteration number\n    iter = 0\n\n    while mb_loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n        \n        # not part of the update: just for tracking our progress    \n        mb_loss = mb_p.loss(X, y) \n        mb_loss_vec.append(mb_loss)\n        \n        # perform a perceptron update using the random data point\n        mb_opt.step(X, y, k, alpha)\n\n        # update iter\n        iter += 1\n\n        # maxiter condition\n        if iter &gt;= 1000:\n            break\n    \n    # set seed\n    torch.manual_seed(1234567)\n\n    # Normal Perceptron\n    p = Perceptron() \n    opt = PerceptronOptimizer(p)\n\n    # define loss variable\n    loss = 1.0\n\n    # for keeping track of loss values\n    loss_vec = []\n\n    # for recording iteration number\n    iter = 0\n\n    while loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n        \n        # not part of the update: just for tracking our progress    \n        loss = p.loss(X, y) \n        loss_vec.append(loss)\n        \n        # perform a perceptron update using the random data point\n        opt.step(X, y)\n\n        # update iter\n        iter += 1\n\n        # maxiter condition\n        if iter &gt;= 1000:\n            break\n    \n    return loss_vec, mb_loss_vec, p.w, mb_p.w"
  },
  {
    "objectID": "posts/perceptron-post/index.html#part-d-runtime-implications",
    "href": "posts/perceptron-post/index.html#part-d-runtime-implications",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Part D: Runtime Implications",
    "text": "Part D: Runtime Implications\nIn this section, we will discuss the runtime implications of the Perceptron and MiniBatch Perceptron algorithms.\nWhen considering the runtime implications of the Perceptron and MiniBatch Perceptron algorithms, we need to evaluate the runtime complexity of opt.step(X,y)\nLet’s first go over perceptron,py. opt.step() involves the following line by line:\n\nn = X.size()[0] Determines the amount of rows in Constant time\ni = torch.randint(n, size = (1,)) Selects a random number between 0 and \\(n\\) in Constant time\nx_i = X[[i],:] and y_i = y[i] Creates subsets of the data in Constant time\ncurrent_loss = self.model.loss(X, y) Calculates the current loss using dot product in Linear time\nself.model.w += torch.reshape(self.model.grad(x_i, y_i),(self.model.w.size()[0],)) Updates the weights in Linear time\nnew_loss = self.model.loss(X, y) Calculates the new loss in Linear time through dot product\nreturn i, abs(current_loss - new_loss) Returns the index and the difference in loss in Constant time\n\nSince the largest time complexity is Linear, we can say that the time complexity of opt.step() is Linear or \\(O(n)\\).\nNow let’s go over minibatch_perceptron.py. opt.step()\nThe two are actually very similar, the only difference in runtime is in the grad() function. In the normal Perceptron algorithm, the grad() function is called on a single data point, while in the MiniBatch Perceptron algorithm, the grad() function is called on a mini-batch of data points. This means in the normal Perceptron algorithm, a single dot product between 1 x \\(p\\) vectors is computed whereas in the MiniBatch perceptron algorithm a matrix product between \\(X^{k \\times p}\\) and \\(w^{p \\times 1}\\) is computed. Calculating the matrix product is equal to calculating \\(k\\) dot products, which are both \\(O(p)\\) operations. Meaning that the runtime of the Matrix product is \\(O(k \\cdot p)\\)."
  },
  {
    "objectID": "posts/perceptron-post/index.html#conclusion",
    "href": "posts/perceptron-post/index.html#conclusion",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, I have implemented the Perceptron algorithm from scratch and tested it on a synthetic dataset. I have also implemented the MiniBatch Perceptron algorithm and tested it on different datasets. I have shown that the Perceptron algorithm can learn a decision boundary that separates the data points into two classes, and that the MiniBatch Perceptron algorithm can converge faster than the Perceptron algorithm by updating the weights and bias term based on a mini-batch of data points in each iteration. I have also discussed the runtime implications of the Perceptron and MiniBatch Perceptron algorithms, and shown that the time complexity of the opt.step() function is Linear for the normal Perceptron algorithm and \\(O(k \\cdot p)\\) for the MiniBatch algorithm. Overall, the Perceptron and MiniBatch Perceptron algorithms are simple and efficient algorithms for binary classification tasks, and can be extended to handle data with more than two dimensions. It was great to be able to implement a simple machine learning algorithm from scratch and experiment with it on different datasets. I hope you enjoyed reading this blog post and learned something new about the Perceptron and MiniBatch Perceptron algorithms. Thank you for reading!"
  },
  {
    "objectID": "posts/logistic-regression-post/index.html",
    "href": "posts/logistic-regression-post/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer"
  },
  {
    "objectID": "posts/logistic-regression-post/index.html#abstract",
    "href": "posts/logistic-regression-post/index.html#abstract",
    "title": "Implementing Logistic Regression",
    "section": "Abstract:",
    "text": "Abstract:\nIn this Blog Post, we implement logistic regression. We will run three experiments to understand the working of logistic regression. In the first experiment, we will implement logistic regression using vanilla gradient descent. In the second experiment, we will implement logistic regression using gradient descent with momentum. In the third experiment, we will examine the issues of overfitting. We find that when \\(\\beta\\) is higher (momentum), the model converges faster. We also find that the model overfits when the the number of features is larger than the normal of observations\nIn this blog post we will:\n\nImplement gradient descent for logistic regression in an object-oriented paradigm.\nImplement a key variant of gradient descent with momentum in order to achieve faster convergence.\nPerform experiments to test our implementations."
  },
  {
    "objectID": "posts/logistic-regression-post/index.html#part-a-implement-logistic-regression",
    "href": "posts/logistic-regression-post/index.html#part-a-implement-logistic-regression",
    "title": "Implementing Logistic Regression",
    "section": "Part A: Implement Logistic Regression",
    "text": "Part A: Implement Logistic Regression\nTo view my implementation of logistic regression, please refer to logistic.py"
  },
  {
    "objectID": "posts/logistic-regression-post/index.html#part-b-experiments",
    "href": "posts/logistic-regression-post/index.html#part-b-experiments",
    "title": "Implementing Logistic Regression",
    "section": "Part B: Experiments",
    "text": "Part B: Experiments"
  },
  {
    "objectID": "posts/logistic-regression-post/index.html#conclusion",
    "href": "posts/logistic-regression-post/index.html#conclusion",
    "title": "Implementing Logistic Regression",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we implemented logistic regression in an object-oriented paradigm and tested our implementation with various experiments. We trained a logistic regression model using vanilla gradient descent and gradient descent with momentum. We also explored the concept of overfitting by generating data where the number of features is greater than the number of data points. Through these experiments, I gained a better understanding of logistic regression and gradient descent optimization techniques."
  }
]