[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Donovan’s Blog Post",
    "section": "",
    "text": "About this blog\nTesting this"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Whose Costs?",
    "section": "",
    "text": "The dataset below will be the one I use for this blog post\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf = pd.read_csv(url)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\n\nFor the dataset, I created two plots and one summary table in order to contextualize the data.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\nplt.figure(figsize=(12, 8))\nsns.scatterplot(x='person_age', y='person_emp_length', hue='loan_intent', data=df, palette='viridis')\nplt.title('Loan Intent vs. Age and Employment Length')\nplt.xlabel('Age')\nplt.ylabel('Employment Length (Years)')\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Loan Intent\n\n\n\n\n\n\nplt.figure(figsize=(10, 6))\nsns.countplot(x='person_home_ownership', hue='loan_grade', data=df, palette='Set2')\nplt.title('Loan Grade Distribution by Home Ownership')\nplt.xlabel('Home Ownership')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\n\nFigure 2: Loan Grade Distribution by Home Ownership\n\n\n\n\n\n\nsummary_table = df.groupby('loan_intent').agg({\n    'loan_int_rate': 'mean',\n    'loan_amnt': 'mean'\n}).reset_index()\n\nsummary_table.rename(columns={\n    'loan_int_rate': 'Average Interest Rate',\n    'loan_amnt': 'Average Loan Amount'\n}, inplace=True)\n\nprint(summary_table)\n\n         loan_intent  Average Interest Rate  Average Loan Amount\n0  DEBTCONSOLIDATION              10.983305          9620.901149\n1          EDUCATION              10.965465          9460.015604\n2    HOMEIMPROVEMENT              11.160075         10348.725017\n3            MEDICAL              11.051946          9242.269907\n4           PERSONAL              11.009814          9549.427178\n5            VENTURE              10.940866          9516.417425\n\n\n\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import Pipeline\n\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_all = pd.read_csv(url)\n\ndf = df_all.drop(['loan_grade', 'loan_status'], axis=1)  \ndf = pd.get_dummies(df, columns=['person_home_ownership', 'loan_intent', 'cb_person_default_on_file'], drop_first=True)\ndf.dropna(inplace=True)\n\ny = df_all.loc[df.index, 'loan_status']  \n\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=123)\n\n# Scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlogreg = LogisticRegression(max_iter=1000)\n\nselector = RFECV(estimator=logreg, step=1, cv=5)\nselector = selector.fit(X_train_scaled, y_train)  # Fitting RFECV to the scaled training data\n\n\nX_train_selected = selector.transform(X_train_scaled)\nX_test_selected = selector.transform(X_test_scaled)\n\nmodel_selected = LogisticRegression(max_iter=1000)\nmodel_selected.fit(X_train_selected, y_train)\n\nselected_features = X_train.columns[selector.support_]\n\n# Prediction\ny_pred_selected = model_selected.predict(X_test_selected)\nprint(f\"Optimal number of features: {selector.n_features_}\")\nprint(classification_report(y_test, y_pred_selected))\n\nOptimal number of features: 8\n              precision    recall  f1-score   support\n\n           0       0.87      0.94      0.90      3614\n           1       0.68      0.48      0.56       968\n\n    accuracy                           0.84      4582\n   macro avg       0.77      0.71      0.73      4582\nweighted avg       0.83      0.84      0.83      4582\n\n\n\n\nfeature_names = df.columns\n\nselected_features = feature_names[selector.support_]\n\nprint(\"Selected features:\", selected_features)\n\nSelected features: Index(['loan_amnt', 'loan_int_rate', 'loan_percent_income',\n       'person_home_ownership_OWN', 'person_home_ownership_RENT',\n       'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE'],\n      dtype='object')\n\n\n\n\nX = df[selected_features]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n## MODEL HERE\nmodel.fit(X_train_scaled, y_train)\n\nNameError: name 'model' is not defined\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef linear_score(X, w):\n    return X @ w\n\n\n# Extracting weights \nw = model.coef_.flatten()  \n\n\n\nscores = linear_score(X_train_selected, w) \n\n\n\n\nplt.hist(scores, bins=30, edgecolor='k', alpha=0.7)\nplt.xlabel(r\"Score $s$\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of Computed Scores\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnum_thresholds = 101\nFPR = np.zeros(num_thresholds)\nTPR = np.zeros(num_thresholds)\n\nT = np.linspace(scores.min() - 0.1, scores.max() + 0.1, num_thresholds)\n\nfor i in range(num_thresholds):\n    t = T[i]\n    preds = scores &gt;= t\n    FPR[i] = ((preds == 1) & (y_train == 0)).sum() / (y_train == 0).sum()\n    TPR[i] = ((preds == 1) & (y_train == 1)).sum() / (y_train == 1).sum()\n\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 4))\n\nax.plot(FPR, TPR, color=\"black\")\nax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"grey\")  \nax.set_aspect('equal')\n\nax.set_xlabel(\"False Positive Rate\")\nax.set_ylabel(\"True Positive Rate\")\nax.set_title(\"ROC Curve\")\n\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\ndef profit_repaid(loan_amnt, loan_int_rate):\n    return loan_amnt * (1 + 0.25 * loan_int_rate)**10 - loan_amnt\n\ndef profit_default(loan_amnt, loan_int_rate):\n    return loan_amnt * (1 + 0.25 * loan_int_rate)**3 - 2 * loan_amnt\n\n\n\nprobabilities = model.predict_proba(X_train_scaled)[:, 1]\n\n\nexpected_gains = []\nT = np.linspace(0, 1, 101)  \n\nfor t in T:\n    preds = probabilities &gt;= t  # Prediction based on threshold: whether the loan would be issued\n\n    # Initialize gains and losses\n    total_gain = 0\n\n    # Loop over each loan\n    for amt, rate, pred, actual in zip(X_train['loan_amnt'], X_train['loan_int_rate'], preds, y_train):\n        if pred:  # Loan is predicted to be issued\n            if actual == 1:  # Loan is repaid\n                total_gain += profit_repaid(amt, rate)\n            else:  # Loan defaults\n                total_gain += loss_default(amt, rate)\n\n    # Average gain per loan\n    avg_gain = total_gain / len(X_train)\n    expected_gains.append(avg_gain)\n\n\n\n\nplt.figure(figsize=(10, 6))\nplt.plot(T, expected_gains, label='Expected Gain')\nplt.xlabel(r\"Threshold $t$\")\nplt.ylabel(\"Expected Profit per Loan\")\nplt.title(\"Expected Profit per Loan vs. Threshold\")\nplt.xlim(0, 1)\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\nselected_features = ['loan_amnt', 'loan_int_rate', 'loan_percent_income', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE']\n\n\nfeature_combinations = [\n    ['loan_amnt', 'loan_int_rate'],  \n    ['loan_percent_income', 'person_home_ownership_RENT'],  \n    selected_features  \n]\n\n\nresults = {}\n\nfor features in feature_combinations:\n    # Scale features\n    X_scaled = scaler.fit_transform(df[features])\n    \n    LR = LogisticRegression(max_iter=10000)\n    \n    cv_scores_LR = cross_val_score(LR, X_scaled, y, cv=5)\n    results[str(features)] = cv_scores_LR.mean()\n\nfor combo, score in results.items():\n    print(f\"Features: {combo}\\nCV Score (mean accuracy): {score}\\n\")\n\nFeatures: ['loan_amnt', 'loan_int_rate']\nCV Score (mean accuracy): 0.7973108709793388\n\nFeatures: ['loan_percent_income', 'person_home_ownership_RENT']\nCV Score (mean accuracy): 0.8471203482091736\n\nFeatures: ['loan_amnt', 'loan_int_rate', 'loan_percent_income', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE']\nCV Score (mean accuracy): 0.848342893535451"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\n\nImportError: cannot import name 'Perceptron' from 'source' (/Users/donovanwood/itsdwood.github.io-1/posts/example-blog-post/source.py)\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Warmup Exercise Penquin",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Warmup Exercise Penquin",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "CSCI 0451: Reflective Goal-Setting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Women in Data Science (WiDS) Conference at Middlebury College\n\n\n\n\n\nHighlighting the Middlebury Women in Data Science Conference that took place on March 4th, 2024\n\n\n\n\n\nMar 14, 2024\n\n\nDonovan Wood\n\n\n\n\n\n\n\n\n\n\n\n\nWhose Costs?\n\n\n\n\n\nMy second blog post, creating an automated decision system for a hypothetical bank extending credit\n\n\n\n\n\nFeb 28, 2024\n\n\nDonovan Wood\n\n\n\n\n\n\n\n\n\n\n\n\nWhose Costs?\n\n\n\n\n\nMy second blog post, creating an automated decision system for a hypothetical bank extending credit\n\n\n\n\n\nFeb 28, 2024\n\n\nDonovan Wood\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nMy first blog post, detailing my model for classifying Palmer Penguins\n\n\n\n\n\nFeb 19, 2024\n\n\nDonovan Wood\n\n\n\n\n\n\n\n\n\n\n\n\nWarmup Exercise Penquin\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html",
    "href": "posts/New-test-thing-post/index.html",
    "title": "Whose Costs?",
    "section": "",
    "text": "The dataset below will be the one I use for this blog post\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf = pd.read_csv(url)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\n\nFor the dataset, I created two plots and one summary table in order to contextualize the data.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\nplt.figure(figsize=(12, 8))\nsns.scatterplot(x='person_age', y='person_emp_length', hue='loan_intent', data=df, palette='viridis')\nplt.title('Loan Intent vs. Age and Employment Length')\nplt.xlabel('Age')\nplt.ylabel('Employment Length (Years)')\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Loan Intent\n\n\n\n\n\nRegarding Figure 1, we can see some patterns within the dataset among loan intentions.\nMost notably in the bottom left corner of the plot, which indicates both a young age and limited employment history, there is a culmination of both Educational and Venture loans being used.\nThis makes sense given the current context of the world, as young individuals would most likely be in college or other forms of education at this time. On a similar note, venture capital is often used for newly formed or aspiring companies. Hence, the youthful age and lack of employment history match.\nConversely, as age increases, loans for medical, home improvement, and debt consolidation become more likely. This evidence is runs concurrent with today’s societal concerns from our more elderly population.\n\nplt.figure(figsize=(10, 6))\nsns.countplot(x='person_home_ownership', hue='loan_grade', data=df, palette='Set2')\nplt.title('Loan Grade Distribution by Home Ownership')\nplt.xlabel('Home Ownership')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\n\nFigure 2: Loan Grade Distribution by Home Ownership\n\n\n\n\n\nFigure 2 illustrates the Loan grade distribution by home ownership. Loan Grade is viewed A-G with A being the highest loan grade and G being the lowest.\nAn interesting observation to make here is that for individuals in both the ‘Mortgage’ and ‘Home’ groups, the highest frequency of loans is rated as an A whereas for individuals in the ‘Rent’ group the highest frequency of loans is rated as an B.\nThis could give context to the economic state of home ownership and bank trust. Those with a mortgage/own property could perhaps be linked to being more trustworthy due to having a longer term connection with a property. Banks may view those in the ‘Rent’ category as a bit more risky due to the short term ability of renting.\n\nsummary_table = df.groupby('loan_intent').agg({\n    'loan_int_rate': 'mean',\n    'loan_amnt': 'mean'\n}).reset_index()\n\nsummary_table.rename(columns={\n    'loan_int_rate': 'Average Interest Rate',\n    'loan_amnt': 'Average Loan Amount'\n}, inplace=True)\n\nprint(summary_table)\n\n         loan_intent  Average Interest Rate  Average Loan Amount\n0  DEBTCONSOLIDATION              10.983305          9620.901149\n1          EDUCATION              10.965465          9460.015604\n2    HOMEIMPROVEMENT              11.160075         10348.725017\n3            MEDICAL              11.051946          9242.269907\n4           PERSONAL              11.009814          9549.427178\n5            VENTURE              10.940866          9516.417425\n\n\nThis summary table above displays the relative average interest rates and average loan amounts for each category of loan intent.\nHome improvement has the greatest average loan amount while similarly having the highest interest rate. On the flip side, education has the lowest average loan amount but the second lowest interest rate.\n\n\n\nI will be utilizing a Logistic Regression Model for predicting whether a prospective borrower is likely to default on a given loan.\nI will also be utilizing the RFECV class from sklearn in order to automatically find the most useful features to use in the model. RFE is a feature selection method that fits a model and removes the weakest feature(s) until the specified number of features is reached. Using RFE, you can automate the process of finding an effective subset of features.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import Pipeline\n\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_all = pd.read_csv(url)\n\ndf = df_all.drop(['loan_grade', 'loan_status'], axis=1)  \ndf = pd.get_dummies(df, columns=['person_home_ownership', 'loan_intent', 'cb_person_default_on_file'], drop_first=True)\ndf.dropna(inplace=True)\n\ny = df_all.loc[df.index, 'loan_status']  \n\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=123)\n\n# Scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlogreg = LogisticRegression(max_iter=1000)\n\nselector = RFECV(estimator=logreg, step=1, cv=5)\nselector = selector.fit(X_train_scaled, y_train)  # Fitting RFECV to the scaled training data\n\n\nX_train_selected = selector.transform(X_train_scaled)\nX_test_selected = selector.transform(X_test_scaled)\n\nmodel_selected = LogisticRegression(max_iter=1000)\nmodel_selected.fit(X_train_selected, y_train)\n\nselected_features = X_train.columns[selector.support_]\n\n# Prediction\ny_pred_selected = model_selected.predict(X_test_selected)\nprint(f\"Optimal number of features: {selector.n_features_}\")\nprint(classification_report(y_test, y_pred_selected))\n\nOptimal number of features: 8\n              precision    recall  f1-score   support\n\n           0       0.87      0.94      0.90      3614\n           1       0.68      0.48      0.56       968\n\n    accuracy                           0.84      4582\n   macro avg       0.77      0.71      0.73      4582\nweighted avg       0.83      0.84      0.83      4582\n\n\n\nThe code below is used to identify which features will be used for the most optimal model prediction.\n\nfeature_names = df.columns\n\nselected_features = feature_names[selector.support_]\n\nprint(\"Selected features:\", selected_features)\n\nSelected features: Index(['loan_amnt', 'loan_int_rate', 'loan_percent_income',\n       'person_home_ownership_OWN', 'person_home_ownership_RENT',\n       'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE'],\n      dtype='object')\n\n\n\n\nX = df[selected_features]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n## MODEL HERE\nmodel.fit(X_train_scaled, y_train)\n\nLogisticRegression(max_iter=10000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=10000)\n\n\n\n\n\nI did not want to limit myself to only one model however, so I used cross_val_score from the sklearn.model_selection in order to compare how my ‘Optimal’ LR model did against other LR models with a different combination of features.\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\nselected_features = ['loan_amnt', 'loan_int_rate', 'loan_percent_income', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE']\n\n\nfeature_combinations = [\n    ['loan_amnt', 'loan_int_rate'],  \n    ['loan_percent_income', 'person_home_ownership_RENT'],  \n    selected_features  \n]\n\n\nresults = {}\n\nfor features in feature_combinations:\n    # Scale features\n    X_scaled = scaler.fit_transform(df[features])\n    \n    LR = LogisticRegression(max_iter=10000)\n    \n    cv_scores_LR = cross_val_score(LR, X_scaled, y, cv=5)\n    results[str(features)] = cv_scores_LR.mean()\n\nfor combo, score in results.items():\n    print(f\"Features: {combo}\\nCV Score (mean accuracy): {score}\\n\")\n\nFeatures: ['loan_amnt', 'loan_int_rate']\nCV Score (mean accuracy): 0.7973108709793388\n\nFeatures: ['loan_percent_income', 'person_home_ownership_RENT']\nCV Score (mean accuracy): 0.8471203482091736\n\nFeatures: ['loan_amnt', 'loan_int_rate', 'loan_percent_income', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE']\nCV Score (mean accuracy): 0.848342893535451\n\n\n\nAs seen above, the ‘Optimal’ feature LR model performed the best in predicting the likelihood of a default.\n\n\n\nIn this section, we will be exploring how I came up with the threshold t that will be used for our model.\nI used the numpy package to compute linear scores across all n of our training points.\nI plotted the scores below in order to easily visualize this process. By gathering scores, we can easily simulate decision-making with a given threshold.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef linear_score(X, w):\n    return X @ w\n\n\n# Extracting weights \nw = model.coef_.flatten()  \n\n\n\nscores = linear_score(X_train_selected, w) \n\n\nplt.hist(scores, bins=30, edgecolor='k', alpha=0.7)\nplt.xlabel(r\"Score $s$\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of Computed Scores\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure 3: Histogram of Computed Scores"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#classifying-the-palmer-penguins",
    "href": "posts/New-test-thing-post/index.html#classifying-the-palmer-penguins",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Image source: @allisonhorst\n\n\nOur data set for these notes is Palmer Penguins. This data set contains physiological measurements and species labels for several populations of Adelie, Chinstrap, and Gentoo penguins."
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#abstract",
    "href": "posts/New-test-thing-post/index.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "Abstract",
    "text": "Abstract\nIn this blog post we use a logistic regression model in order to classify Palmer penguins according to their Species. We find that using information such as the Island of the penguin as well as Culmen Depth (mm) and Culmen Length are sufficient features in order to provide an accurate classifying model. Within this blog post, we examine multiple data visualizations and tables in order to better understand the data and develop our reasoning for choosing the features of the model.\nBelow we will import the dataset we will use for this Blog Post\n\nimport pandas as pd\nimport numpy as np\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#data-preparation",
    "href": "posts/New-test-thing-post/index.html#data-preparation",
    "title": "Classifying Palmer Penguins",
    "section": "Data Preparation",
    "text": "Data Preparation\nIn order to prepare the qualitative columns in the data, we must convert categorical feature columns like Sex and Island into 0-1 columns using pd.get_dummies function.\nThe label column Species will also needed to be coded differently, using a LabelEncoder\nThe following function will take care of these processes:\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n# Replace the column with the first word in each entry \ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#visualizations-of-the-data",
    "href": "posts/New-test-thing-post/index.html#visualizations-of-the-data",
    "title": "Whose Costs?",
    "section": "",
    "text": "For the dataset, I created two plots and one summary table in order to contextualize the data.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\nplt.figure(figsize=(12, 8))\nsns.scatterplot(x='person_age', y='person_emp_length', hue='loan_intent', data=df, palette='viridis')\nplt.title('Loan Intent vs. Age and Employment Length')\nplt.xlabel('Age')\nplt.ylabel('Employment Length (Years)')\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Loan Intent\n\n\n\n\n\nRegarding Figure 1, we can see some patterns within the dataset among loan intentions.\nMost notably in the bottom left corner of the plot, which indicates both a young age and limited employment history, there is a culmination of both Educational and Venture loans being used.\nThis makes sense given the current context of the world, as young individuals would most likely be in college or other forms of education at this time. On a similar note, venture capital is often used for newly formed or aspiring companies. Hence, the youthful age and lack of employment history match.\nConversely, as age increases, loans for medical, home improvement, and debt consolidation become more likely. This evidence is runs concurrent with today’s societal concerns from our more elderly population.\n\nplt.figure(figsize=(10, 6))\nsns.countplot(x='person_home_ownership', hue='loan_grade', data=df, palette='Set2')\nplt.title('Loan Grade Distribution by Home Ownership')\nplt.xlabel('Home Ownership')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\n\nFigure 2: Loan Grade Distribution by Home Ownership\n\n\n\n\n\nFigure 2 illustrates the Loan grade distribution by home ownership. Loan Grade is viewed A-G with A being the highest loan grade and G being the lowest.\nAn interesting observation to make here is that for individuals in both the ‘Mortgage’ and ‘Home’ groups, the highest frequency of loans is rated as an A whereas for individuals in the ‘Rent’ group the highest frequency of loans is rated as an B.\nThis could give context to the economic state of home ownership and bank trust. Those with a mortgage/own property could perhaps be linked to being more trustworthy due to having a longer term connection with a property. Banks may view those in the ‘Rent’ category as a bit more risky due to the short term ability of renting.\n\nsummary_table = df.groupby('loan_intent').agg({\n    'loan_int_rate': 'mean',\n    'loan_amnt': 'mean'\n}).reset_index()\n\nsummary_table.rename(columns={\n    'loan_int_rate': 'Average Interest Rate',\n    'loan_amnt': 'Average Loan Amount'\n}, inplace=True)\n\nprint(summary_table)\n\n         loan_intent  Average Interest Rate  Average Loan Amount\n0  DEBTCONSOLIDATION              10.983305          9620.901149\n1          EDUCATION              10.965465          9460.015604\n2    HOMEIMPROVEMENT              11.160075         10348.725017\n3            MEDICAL              11.051946          9242.269907\n4           PERSONAL              11.009814          9549.427178\n5            VENTURE              10.940866          9516.417425\n\n\nThis summary table above displays the relative average interest rates and average loan amounts for each category of loan intent.\nHome improvement has the greatest average loan amount while similarly having the highest interest rate. On the flip side, education has the lowest average loan amount but the second lowest interest rate."
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#model-choices",
    "href": "posts/New-test-thing-post/index.html#model-choices",
    "title": "Classifying Palmer Penguins",
    "section": "Model Choices",
    "text": "Model Choices\nFor this blog, I used the combinations function from the itertools package in order to go through each variation of both qualitative and quantitative features.\n\nfrom itertools import combinations\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Region\", ]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n\nI then utilized the LoigisticRegression model from sklearn in order to run tests for each iteration.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\n\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\nLR = LogisticRegression(max_iter=10000)\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n0.99609375\n\n\nGoing through each iteration, I scored each of them found below:\n[‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Culmen Length (mm)’, ‘Body Mass (g)’] : .91  [‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Culmen Depth (mm)’, ‘Flipper Length (mm)’]: .91  [‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Culmen Depth (mm)’, ‘Body Mass (g)’] : .76  [‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Flipper Length (mm)’, ‘Body Mass (g)’] : .63  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, ‘Culmen Depth (mm)’] : .992  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, ‘Flipper Length (mm)’] : .98  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, ‘Body Mass (g)’] : .98  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Depth (mm)’, ‘Flipper Length (mm)’] : .82  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Depth (mm)’, ‘Body Mass (g)’] : .77  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Flipper Length (mm)’, ‘Body Mass (g)’] : .71  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Length (mm)’, ‘Culmen Depth (mm)’] : .996  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Length (mm)’, ‘Flipper Length (mm)’]: .97  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Length (mm)’, ‘Body Mass (g)’]: .97  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Depth (mm)’, ‘Flipper Length (mm)’]:.88  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Depth (mm)’, ‘Body Mass (g)’]: .83  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Flipper Length (mm)’, ‘Body Mass (g)’] : .75"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#testing",
    "href": "posts/New-test-thing-post/index.html#testing",
    "title": "Classifying Palmer Penguins",
    "section": "Testing",
    "text": "Testing\nIn order to test the model, I am downloading the dataset and preparing it via the prepare_data function\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n1.0\n\n\nAwesome! Our model is able to classify the three species of penguins with 100 percent accuracy.\n\n\n\n\n\n\nFigure 3: Image"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#plotting-decision-regions",
    "href": "posts/New-test-thing-post/index.html#plotting-decision-regions",
    "title": "Classifying Palmer Penguins",
    "section": "PLotting Decision Regions",
    "text": "PLotting Decision Regions\nI will be using the plot_regions function below in order to plot my decision regions for my model.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Gentoo\", \"Chinstrap\", \"Adelie\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nFor the purposes of this blog post, we will look at the decision regions for both our training data and our testing data\n\nplot_regions(LR, X_train[cols], y_train)\n\n\n\n\n\n\n\nFigure 4: Decision Regions for each island based on training data\n\n\n\n\n\nThe three plots above correspond to the decision regions for each island. As previously mentioned in the data visualization section, we see that for the Biscoe and Dream islands, there are only two species of penguins. For the Torgersen island, only the Adelie penguins reside there. As predicted, this results in comparing at most only two species of penguins against one another, allowing for easier classification.\nThe following section displays the decision regions our model makes for the testing data.\n\nplot_regions(LR, X_test[cols], y_test)\n\n\n\n\n\n\n\nFigure 5: Decision Regions for each island based on testing data\n\n\n\n\n\nOur testing data yielded a smaller number of penguins to be classified, but our classifier was able to accurately predict each species of penguin based on our three features of Island, Culmen Depth (mm), and Culmen Length (mm)\nWhile the penguins on the Torgersen would always be Adelie, and were classified as such. The closest decision region our model came to for classification of the test data was a penguin on the Dream Island. This was anticipated however as in Figure 2, Adelie and Chinstrap penguins share some overlapping points in regards to Culmen Depth (mm) and Culmen Length(mm)"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#confusion-matrix",
    "href": "posts/New-test-thing-post/index.html#confusion-matrix",
    "title": "Classifying Palmer Penguins",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nI will be using a confusion matrix for my model, evaluated on the test set.\nIn order to do this I will be using confusion_matrix from the sklearn package.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test[cols])\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nIn this confusion matrix, due to our 100 percent prediction accuracy, we do not see any errors within our matrix. As a reference, the confusion matrix just gives the number of data points that have the correct label.\nLogically, since our model predicts each penguin correctly, there will be disparities in the matrix.\nThat being said, as previously mentioned, the most likely error to occur within our model is classifying a penguin from the Dream island. This is due to the Adelie and Chinstrap penguins sharing the most similar distributions of Culmen Depth (mm) and Culmen Length (mm) which depending on the penguin’s dimensions, has the highest likelihood out of all the species to overlap in the incorrect species decision region."
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#discussion",
    "href": "posts/New-test-thing-post/index.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Discussion",
    "text": "Discussion\nIn this blog post, I have found that the using Island, Culmen Depth (mm), and Culmen Length (mm) features in a logistic regression model are useful for accurately classifying species of penguins pertaining to Adelie, Gentoo, and Chinstrap.\nI have found that Adelie penguins reside in all three islands (Dream, Biscoe, Torgersen), while Gentoo penguins only reside on the Biscoe island and Chinstrap penguins only inhabit the Biscoe island.\nIn terms of Culmen Length (mm) and Culmen Depth (mm) we found that:\nGentoo penguins seem to exhbit a longer culmen length but shorter culmen depth.  Adelie penguins have a shorter culmen length compared to Gentoo penguins but a higher culmen depth.  Chinstrap penguins appear to have the longest average combination of culmen depth/length out of all the penguins.\nIn terms of Body Mass (g) we found that:\nGentoo penguins, both male and female, have the highest mean body mass out of all species of penguins.\nAdelie and Chinstrap penguins exhibit similar mean body mass to one another for both male and female."
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html",
    "href": "posts/classifying-penguin-post/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Image source: @allisonhorst\n\n\nOur data set for these notes is Palmer Penguins. This data set contains physiological measurements and species labels for several populations of Adelie, Chinstrap, and Gentoo penguins."
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#classifying-the-palmer-penguins",
    "href": "posts/classifying-penguin-post/index.html#classifying-the-palmer-penguins",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Image source: @allisonhorst\n\n\nOur data set for these notes is Palmer Penguins. This data set contains physiological measurements and species labels for several populations of Adelie, Chinstrap, and Gentoo penguins."
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#abstract",
    "href": "posts/classifying-penguin-post/index.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "Abstract",
    "text": "Abstract\nIn this blog post we use a logistic regression model in order to classify Palmer penguins according to their Species. We find that using information such as the Island of the penguin as well as Culmen Depth (mm) and Culmen Length are sufficient features in order to provide an accurate classifying model. Within this blog post, we examine multiple data visualizations and tables in order to better understand the data and develop our reasoning for choosing the features of the model.\nBelow we will import the dataset we will use for this Blog Post\n\nimport pandas as pd\nimport numpy as np\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)"
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#data-preparation",
    "href": "posts/classifying-penguin-post/index.html#data-preparation",
    "title": "Classifying Palmer Penguins",
    "section": "Data Preparation",
    "text": "Data Preparation\nIn order to prepare the qualitative columns in the data, we must convert categorical feature columns like Sex and Island into 0-1 columns using pd.get_dummies function.\nThe label column Species will also needed to be coded differently, using a LabelEncoder\nThe following function will take care of these processes:\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n# Replace the column with the first word in each entry \ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)"
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#visualizations-of-the-data",
    "href": "posts/classifying-penguin-post/index.html#visualizations-of-the-data",
    "title": "Classifying Palmer Penguins",
    "section": "Visualizations of the Data",
    "text": "Visualizations of the Data\nFor the dataset, I created two plots and one summary table in order to contextualize the data.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\nsns.scatterplot(train, x = \"Island\", y = \"Species\", hue = \"Species\", style = \"Species\")\n\n\n\n\n\n\n\nFigure 1: Island Distribution for Each Penguin Species\n\n\n\n\n\nAfter running through a multitude of different plots with varying sets of data, one combination that particularly stood out to me is shown in Figure 1. The plot shows the distribution of each species of Penguin on the three islands within the dataset. Now while it seems rather plain and simple at first glance, take note that for one of these islands (Torgersen), only one species of penguins resides there, that being Adeilie. Upon further investigation, for the remaining two islands, at most, only two differing penguin species reside there. Adelie inhabit all three islands while Chinstrap penguins reside only on the Dream island and Gentoo Penguins are only found on the Biscoe Island.\nThe reason as to why this rather simple analysis is so interesting as it presents a way in which we no longer have to compare all three species against one another at a given point. Rather, we can now determine whether the penguin presented to us is one of two species if the island is Dream or Biscoe. In the case of Torgersen, we automatically know the penguin will be Adelie as they are the only species of Penguins that reside there!\nThis knowledge, paired with two more quantitative features, should allow the model to classify a penguin more accurately!\n\nsns.scatterplot(train, x = \"Culmen Depth (mm)\", y = \"Culmen Length (mm)\", hue = \"Species\", style = \"Species\")\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nWith that idea now set. Let’s move on to the second visualization, shown by Figure 2. This is a more traditional scatterplot showing the distributions of Culmen_Depth and Culmen_Length for all three species of penguin.\nNow recalling Figure 1, the main relationships we want to focus on are Chinstrap vs Adelie and Gentoo vs Adelie as these are the only relationships where penguins coexist on the same island.\nAs we can see from the Figure 2. Gentoo penguins seem to exhbit a longer culmen length but shorter culmen depth.  Adelie penguins have a shorter culmen length compared to Gentoo penguins but a higher culmen depth.  Chinstrap penguins appear to have the longest average combination of culmen depth/length out of all the penguins.\nReferring back to those relationships as mentioned previously, Gentoo and Adelie penguins seem to have a clear divide amongst one another in terms of culmen length/depth. Chinstrap and Adelie penguins seem to overlap at some points in the graph but fairly slightly.\nAll things considered, given the properties of both Figure 1 and Figure 2, the combination of these three features will be useful as a starting reference for my model.\n\n## Summary table for the mean body mass of each species of penguin by sex\n\nsummary_table = train.groupby(['Species', 'Sex']).agg({'Body Mass (g)': 'mean'})\n\nprint(summary_table)\n\n                  Body Mass (g)\nSpecies   Sex                  \nAdelie    FEMALE    3350.471698\n          MALE      4052.868852\nChinstrap FEMALE    3523.387097\n          MALE      4005.769231\nGentoo    .         4875.000000\n          FEMALE    4684.693878\n          MALE      5476.704545\n\n\nMy last visualization was a simple summary table of the mean body mass (g) of each penguin species by sex. I wanted to ensure that I was covering a large portion of the data, so by doing so, I can cover two features of the dataset I did not cover in the two previous data plots.\nAs a quick analysis:  Gentoo Males exhibit the largest mean body mass (g) out of all the male species  Similarly Gentoo females also exhibit the largest mean body mass (g) out of all the female species  In regards to Adelie and Chinstrap penguins, both males and females exhibit a similar mean mass (g) in relation to the other species.\nWith that in mind, the combination of body mass (g) and sex do not seem like a good pairing to use for predicition due to the similar nature of both Adelie and Chinstrap penguins."
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#model-choices",
    "href": "posts/classifying-penguin-post/index.html#model-choices",
    "title": "Classifying Palmer Penguins",
    "section": "Model Choices",
    "text": "Model Choices\nFor this blog, I used the combinations function from the itertools package in order to go through each variation of both qualitative and quantitative features.\n\nfrom itertools import combinations\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Region\", ]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n\nI then utilized the LoigisticRegression model from sklearn in order to run tests for each iteration.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\n\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\nLR = LogisticRegression(max_iter=10000)\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n0.99609375\n\n\nGoing through each iteration, I scored each of them found below:\n[‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Culmen Length (mm)’, ‘Body Mass (g)’] : .91  [‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Culmen Depth (mm)’, ‘Flipper Length (mm)’]: .91  [‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Culmen Depth (mm)’, ‘Body Mass (g)’] : .76  [‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Flipper Length (mm)’, ‘Body Mass (g)’] : .63  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, ‘Culmen Depth (mm)’] : .992  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, ‘Flipper Length (mm)’] : .98  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, ‘Body Mass (g)’] : .98  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Depth (mm)’, ‘Flipper Length (mm)’] : .82  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Depth (mm)’, ‘Body Mass (g)’] : .77  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Flipper Length (mm)’, ‘Body Mass (g)’] : .71  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Length (mm)’, ‘Culmen Depth (mm)’] : .996  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Length (mm)’, ‘Flipper Length (mm)’]: .97  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Length (mm)’, ‘Body Mass (g)’]: .97  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Depth (mm)’, ‘Flipper Length (mm)’]:.88  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Depth (mm)’, ‘Body Mass (g)’]: .83  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Flipper Length (mm)’, ‘Body Mass (g)’] : .75"
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#testing",
    "href": "posts/classifying-penguin-post/index.html#testing",
    "title": "Classifying Palmer Penguins",
    "section": "Testing",
    "text": "Testing\nIn order to test the model, I am downloading the dataset and preparing it via the prepare_data function\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n1.0\n\n\nAwesome! Our model is able to classify the three species of penguins with 100 percent accuracy.\n\n\n\n\n\n\nFigure 3: Happy Penguins"
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#plotting-decision-regions",
    "href": "posts/classifying-penguin-post/index.html#plotting-decision-regions",
    "title": "Classifying Palmer Penguins",
    "section": "PLotting Decision Regions",
    "text": "PLotting Decision Regions\nI will be using the plot_regions function below in order to plot my decision regions for my model.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Gentoo\", \"Chinstrap\", \"Adelie\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nFor the purposes of this blog post, we will look at the decision regions for both our training data and our testing data\n\nplot_regions(LR, X_train[cols], y_train)\n\n\n\n\n\n\n\nFigure 4: Decision Regions for each island based on training data\n\n\n\n\n\nThe three plots above correspond to the decision regions for each island. As previously mentioned in the data visualization section, we see that for the Biscoe and Dream islands, there are only two species of penguins. For the Torgersen island, only the Adelie penguins reside there. As predicted, this results in comparing at most only two species of penguins against one another, allowing for easier classification.\nThe following section displays the decision regions our model makes for the testing data.\n\nplot_regions(LR, X_test[cols], y_test)\n\n\n\n\n\n\n\nFigure 5: Decision Regions for each island based on testing data\n\n\n\n\n\nOur testing data yielded a smaller number of penguins to be classified, but our classifier was able to accurately predict each species of penguin based on our three features of Island, Culmen Depth (mm), and Culmen Length (mm)\nWhile the penguins on the Torgersen would always be Adelie, and were classified as such. The closest decision region our model came to for classification of the test data was a penguin on the Dream Island. This was anticipated however as in Figure 2, Adelie and Chinstrap penguins share some overlapping points in regards to Culmen Depth (mm) and Culmen Length(mm)"
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#confusion-matrix",
    "href": "posts/classifying-penguin-post/index.html#confusion-matrix",
    "title": "Classifying Palmer Penguins",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nI will be using a confusion matrix for my model, evaluated on the test set.\nIn order to do this I will be using confusion_matrix from the sklearn package.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test[cols])\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nIn this confusion matrix, due to our 100 percent prediction accuracy, we do not see any errors within our matrix. As a reference, the confusion matrix just gives the number of data points that have the correct label.\nLogically, since our model predicts each penguin correctly, there will be disparities in the matrix.\nThat being said, as previously mentioned, the most likely error to occur within our model is classifying a penguin from the Dream island. This is due to the Adelie and Chinstrap penguins sharing the most similar distributions of Culmen Depth (mm) and Culmen Length (mm) which depending on the penguin’s dimensions, has the highest likelihood out of all the species to overlap in the incorrect species decision region."
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#discussion",
    "href": "posts/classifying-penguin-post/index.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Discussion",
    "text": "Discussion\nIn this blog post, I have found that the using Island, Culmen Depth (mm), and Culmen Length (mm) features in a logistic regression model are useful for accurately classifying species of penguins pertaining to Adelie, Gentoo, and Chinstrap as seen in Figure 5\nI have found that Adelie penguins reside in all three islands (Dream, Biscoe, Torgersen), while Gentoo penguins only reside on the Biscoe island and Chinstrap penguins only inhabit the Biscoe island.\nIn terms of Culmen Length (mm) and Culmen Depth (mm) we found that:\nGentoo penguins seem to exhbit a longer culmen length but shorter culmen depth.  Adelie penguins have a shorter culmen length compared to Gentoo penguins but a higher culmen depth.  Chinstrap penguins appear to have the longest average combination of culmen depth/length out of all the penguins.\nIn terms of Body Mass (g) we found that:\nGentoo penguins, both male and female, have the highest mean body mass out of all species of penguins.\nAdelie and Chinstrap penguins exhibit similar mean body mass to one another for both male and female."
  },
  {
    "objectID": "posts/warmup-exercise-penquin/Palmer-Penguins-Warmup.html",
    "href": "posts/warmup-exercise-penquin/Palmer-Penguins-Warmup.html",
    "title": "Donovan's Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/palmer-penguins.csv\"\ndf = pd.read_csv(url)\n\ndf\n\nsummary_table = df.groupby(['Species', 'Sex']).agg({'Body Mass (g)': 'mean'})\n\nprint(summary_table)\n\n# Create a scatterplot\nsns.scatterplot(data=df, x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", hue=\"Species\")\n\n                                                  Body Mass (g)\nSpecies                                   Sex                  \nAdelie Penguin (Pygoscelis adeliae)       FEMALE    3368.835616\n                                          MALE      4043.493151\nChinstrap penguin (Pygoscelis antarctica) FEMALE    3527.205882\n                                          MALE      3938.970588\nGentoo penguin (Pygoscelis papua)         .         4875.000000\n                                          FEMALE    4679.741379\n                                          MALE      5484.836066"
  },
  {
    "objectID": "posts/warmup-exercise-penquin/index.html",
    "href": "posts/warmup-exercise-penquin/index.html",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "",
    "text": "In this blog post, we explore the pivotal role of women in the fields of computing, math, and engineering, and the significant impact of conferences like WiDS (Women in Data Science) in fostering change. We delve into the historical context of women’s participation in computing, highlight the current challenges and barriers they face, and celebrate the achievements of notable women scholars who are pushing the boundaries of data science and its application in diverse fields. Through the insights from Professors Amy Yuen, Jessica L’Roe, Laura Biester, and Sarah Brown, we learn about the transformative power of data science in political science, geography, mental health, and the quest for fairness in machine learning. This post aims to illuminate the importance of diversity in STEM and inspire action towards a more inclusive future."
  },
  {
    "objectID": "posts/warmup-exercise-penquin/index.html#abstract",
    "href": "posts/warmup-exercise-penquin/index.html#abstract",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "",
    "text": "In this blog post, we explore the pivotal role of women in the fields of computing, math, and engineering, and the significant impact of conferences like WiDS (Women in Data Science) in fostering change. We delve into the historical context of women’s participation in computing, highlight the current challenges and barriers they face, and celebrate the achievements of notable women scholars who are pushing the boundaries of data science and its application in diverse fields. Through the insights from Professors Amy Yuen, Jessica L’Roe, Laura Biester, and Sarah Brown, we learn about the transformative power of data science in political science, geography, mental health, and the quest for fairness in machine learning. This post aims to illuminate the importance of diversity in STEM and inspire action towards a more inclusive future."
  },
  {
    "objectID": "posts/warmup-exercise-penquin/index.html#part-1-why-spotlight-women-in-data-science",
    "href": "posts/warmup-exercise-penquin/index.html#part-1-why-spotlight-women-in-data-science",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Part 1: Why Spotlight Women in Data Science?",
    "text": "Part 1: Why Spotlight Women in Data Science?"
  },
  {
    "objectID": "posts/warmup-exercise-penquin/index.html#professor-amy-yuen",
    "href": "posts/warmup-exercise-penquin/index.html#professor-amy-yuen",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Professor Amy Yuen",
    "text": "Professor Amy Yuen\nProfessor Amy Yuen is a Professor of Political Science at Middlebury College. Her research focused on the United Nations Security Council and aimed to answer if it is truly a democratic institution. Within the United Nations Security Council, elected members do not have veto power, Professor Yuen hoped to answer whether these members continue to run if this is the case, and if there is a lack of representation within the council. In her research, she measured the number of resolutions, formal meetings and consultations, presidential statements, resolution Co-sponsorships, and Aria-Formula Meetings. She found that sponsorships showed the biggest differences in who is doing what. Another aspect of her research was investigating the members that sit on the council, and the frequency of which. She found that the data showed that countries were geared more towards inclusion over time. She concluded by finding that while the council was not entirely representative, it could be worse. She plans to continue with this research to see how this finding plays into the output and effectiveness of the council. From her talk, I learned how data science can be applied to essentially any aspect of our lives. Before this talk, data science seemed strictly technical and I would have never thought about how it can be used in the context of politics and policy. The talk opened my eyes to the realm of possibilities that data science presents in so many varying fields."
  },
  {
    "objectID": "posts/warmup-exercise-penquin/index.html#professor-jessica-lroe",
    "href": "posts/warmup-exercise-penquin/index.html#professor-jessica-lroe",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Professor Jessica L’Roe",
    "text": "Professor Jessica L’Roe\nProfessor Jessica L’Roe is an assistant professor of Geography at Middlebury College. Professor Roe spoke about her research into monitoring change in tropical forest landscapes within East Africa. Within her research, she found that mothers were more willing to invest in education rather than land because it was a better bet for the success of their children. Although her research was an important crux of her talk, one of the main messages she wanted to convey was that all kinds of things can be “data”. Regardless of whether something is abstract or physical, it can often be used to analyze certain behaviors or outcomes. She concluded her talk by emphasizing how women are making large contributions in her field and to just “Go for it”! After listening to her talk, I learned the importance of varying types of data and utilization, but more importantly how it was collected. Her data was collected using the help of locals and traveling from settlement to settlement. It was so interesting to hear this type of data collection as before this, most of the data collection I was used to was simply clicking on a link or downloading a dataset. It showed me that data science can transcend the keyboard and has real implications for REAL people."
  },
  {
    "objectID": "posts/warmup-exercise-penquin/index.html#professor-laura-biester",
    "href": "posts/warmup-exercise-penquin/index.html#professor-laura-biester",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Professor Laura Biester",
    "text": "Professor Laura Biester\nProfessor Laura Biester is a Computer Science Professor at Middlebury College. Professor Biester spoke about her research regarding the connection between language and mental health. She used a corpus of Reddit posts and comments from 2006-2019 to predict whether or not language could be used to identify those who were diagnosed with depression before their diagnosis. Through multiple models such as logistic regression and MentalBeRT, she was able to successfully produce a model that was relatively accurate in predicting depression in users using only language. Like Professor L’Roe’s talk, although much of Professor Biester’s talk was focused on her research, a huge point she emphasized was that data collection is a huge part of the data science process - it’s not just about building models. This part struck true to me as I learned that no matter how good your model is, it is only as good as the data it is being supplied with. Data preprocessing and determining which parts of data to use or not use is crucial in the data science process."
  },
  {
    "objectID": "posts/warmup-exercise-penquin/index.html#professor-sarah-brown",
    "href": "posts/warmup-exercise-penquin/index.html#professor-sarah-brown",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Professor Sarah Brown",
    "text": "Professor Sarah Brown\nProfessor Sarah Brown is an assistant professor of Computer Science at the University of Rhode Island. In her talk, her research focused on how to make machine learning more fair. More broadly, however, she discussed the implications of data science through the use of “keys”. These keys were: Disciplines are communities, meet people where they are, and understand data’s context. She mentioned three projects and the “lock” for each, which is something she needed to figure out for it all to work and flow.\nThese keys were used to unlock these locks and culminate a successful data science process. The keys were picked up often from something that had no explicit relation to data science itself but were collected from other aspects of her life. For example, one of the skills she picked up from social studies was the ability to use context to understand primary sources. She mentions that oftentimes, data scientists fall into the trap of only thinking like a data scientist and approaching the problem from one particular angle. By using other disciplines/individuals’ perspectives, you may be able to figure out a problem in a more efficient/effective way than you would have otherwise. She concluded by stressing the importance of these three keys and emphasizing the idea of understanding “why” rather than just “doing” when it comes to data science. Through this, I learned key principles that will influence the future of my computer science journey."
  },
  {
    "objectID": "posts/warmup-exercise-penquin/index.html#conclusion",
    "href": "posts/warmup-exercise-penquin/index.html#conclusion",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Conclusion",
    "text": "Conclusion\nThis exploration into the status of women in computing and the supportive role of initiatives like WiDS has been enlightening. It has underscored the importance of diversity for innovation and the vast potential of data science to impact various aspects of society positively. From the historical context of women’s evolving role in computing to the cutting-edge research conducted by leading female data scientists, this journey has highlighted both the challenges and the incredible opportunities that lie ahead. As we look forward to a more inclusive and equitable future in STEM, the lessons learned here inspire continued advocacy and action. My next steps involve deepening my understanding of how to support diversity in tech and exploring more about how data science can be leveraged for social good."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#visualizations-of-the-data",
    "href": "posts/new-new-test-post/index.html#visualizations-of-the-data",
    "title": "Whose Costs?",
    "section": "",
    "text": "For the dataset, I created two plots and one summary table in order to contextualize the data.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\nplt.figure(figsize=(12, 8))\nsns.scatterplot(x='person_age', y='person_emp_length', hue='loan_intent', data=df, palette='viridis')\nplt.title('Loan Intent vs. Age and Employment Length')\nplt.xlabel('Age')\nplt.ylabel('Employment Length (Years)')\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Loan Intent\n\n\n\n\n\n\nplt.figure(figsize=(10, 6))\nsns.countplot(x='person_home_ownership', hue='loan_grade', data=df, palette='Set2')\nplt.title('Loan Grade Distribution by Home Ownership')\nplt.xlabel('Home Ownership')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\n\nFigure 2: Loan Grade Distribution by Home Ownership\n\n\n\n\n\n\nsummary_table = df.groupby('loan_intent').agg({\n    'loan_int_rate': 'mean',\n    'loan_amnt': 'mean'\n}).reset_index()\n\nsummary_table.rename(columns={\n    'loan_int_rate': 'Average Interest Rate',\n    'loan_amnt': 'Average Loan Amount'\n}, inplace=True)\n\nprint(summary_table)\n\n         loan_intent  Average Interest Rate  Average Loan Amount\n0  DEBTCONSOLIDATION              10.983305          9620.901149\n1          EDUCATION              10.965465          9460.015604\n2    HOMEIMPROVEMENT              11.160075         10348.725017\n3            MEDICAL              11.051946          9242.269907\n4           PERSONAL              11.009814          9549.427178\n5            VENTURE              10.940866          9516.417425"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#part-c-build-a-model",
    "href": "posts/new-new-test-post/index.html#part-c-build-a-model",
    "title": "Whose Costs?",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import Pipeline\n\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_all = pd.read_csv(url)\n\ndf = df_all.drop(['loan_grade', 'loan_status'], axis=1)  \ndf = pd.get_dummies(df, columns=['person_home_ownership', 'loan_intent', 'cb_person_default_on_file'], drop_first=True)\ndf.dropna(inplace=True)\n\ny = df_all.loc[df.index, 'loan_status']  \n\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=123)\n\n# Scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlogreg = LogisticRegression(max_iter=1000)\n\nselector = RFECV(estimator=logreg, step=1, cv=5)\nselector = selector.fit(X_train_scaled, y_train)  # Fitting RFECV to the scaled training data\n\n\nX_train_selected = selector.transform(X_train_scaled)\nX_test_selected = selector.transform(X_test_scaled)\n\nmodel_selected = LogisticRegression(max_iter=1000)\nmodel_selected.fit(X_train_selected, y_train)\n\nselected_features = X_train.columns[selector.support_]\n\n# Prediction\ny_pred_selected = model_selected.predict(X_test_selected)\nprint(f\"Optimal number of features: {selector.n_features_}\")\nprint(classification_report(y_test, y_pred_selected))\n\nOptimal number of features: 8\n              precision    recall  f1-score   support\n\n           0       0.87      0.94      0.90      3614\n           1       0.68      0.48      0.56       968\n\n    accuracy                           0.84      4582\n   macro avg       0.77      0.71      0.73      4582\nweighted avg       0.83      0.84      0.83      4582\n\n\n\n\nfeature_names = df.columns\n\nselected_features = feature_names[selector.support_]\n\nprint(\"Selected features:\", selected_features)\n\nSelected features: Index(['loan_amnt', 'loan_int_rate', 'loan_percent_income',\n       'person_home_ownership_OWN', 'person_home_ownership_RENT',\n       'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE'],\n      dtype='object')\n\n\n\n\nX = df[selected_features]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n## MODEL HERE\nmodel.fit(X_train_scaled, y_train)\n\nNameError: name 'model' is not defined"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#trying-to-do-threshold-stuff-here",
    "href": "posts/new-new-test-post/index.html#trying-to-do-threshold-stuff-here",
    "title": "Whose Costs?",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef linear_score(X, w):\n    return X @ w\n\n\n# Extracting weights \nw = model.coef_.flatten()  \n\n\n\nscores = linear_score(X_train_selected, w) \n\n\n\n\nplt.hist(scores, bins=30, edgecolor='k', alpha=0.7)\nplt.xlabel(r\"Score $s$\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of Computed Scores\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnum_thresholds = 101\nFPR = np.zeros(num_thresholds)\nTPR = np.zeros(num_thresholds)\n\nT = np.linspace(scores.min() - 0.1, scores.max() + 0.1, num_thresholds)\n\nfor i in range(num_thresholds):\n    t = T[i]\n    preds = scores &gt;= t\n    FPR[i] = ((preds == 1) & (y_train == 0)).sum() / (y_train == 0).sum()\n    TPR[i] = ((preds == 1) & (y_train == 1)).sum() / (y_train == 1).sum()\n\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 4))\n\nax.plot(FPR, TPR, color=\"black\")\nax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"grey\")  \nax.set_aspect('equal')\n\nax.set_xlabel(\"False Positive Rate\")\nax.set_ylabel(\"True Positive Rate\")\nax.set_title(\"ROC Curve\")\n\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\ndef profit_repaid(loan_amnt, loan_int_rate):\n    return loan_amnt * (1 + 0.25 * loan_int_rate)**10 - loan_amnt\n\ndef profit_default(loan_amnt, loan_int_rate):\n    return loan_amnt * (1 + 0.25 * loan_int_rate)**3 - 2 * loan_amnt\n\n\n\nprobabilities = model.predict_proba(X_train_scaled)[:, 1]\n\n\nexpected_gains = []\nT = np.linspace(0, 1, 101)  \n\nfor t in T:\n    preds = probabilities &gt;= t  # Prediction based on threshold: whether the loan would be issued\n\n    # Initialize gains and losses\n    total_gain = 0\n\n    # Loop over each loan\n    for amt, rate, pred, actual in zip(X_train['loan_amnt'], X_train['loan_int_rate'], preds, y_train):\n        if pred:  # Loan is predicted to be issued\n            if actual == 1:  # Loan is repaid\n                total_gain += profit_repaid(amt, rate)\n            else:  # Loan defaults\n                total_gain += loss_default(amt, rate)\n\n    # Average gain per loan\n    avg_gain = total_gain / len(X_train)\n    expected_gains.append(avg_gain)\n\n\n\n\nplt.figure(figsize=(10, 6))\nplt.plot(T, expected_gains, label='Expected Gain')\nplt.xlabel(r\"Threshold $t$\")\nplt.ylabel(\"Expected Profit per Loan\")\nplt.title(\"Expected Profit per Loan vs. Threshold\")\nplt.xlim(0, 1)\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\nselected_features = ['loan_amnt', 'loan_int_rate', 'loan_percent_income', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE']\n\n\nfeature_combinations = [\n    ['loan_amnt', 'loan_int_rate'],  \n    ['loan_percent_income', 'person_home_ownership_RENT'],  \n    selected_features  \n]\n\n\nresults = {}\n\nfor features in feature_combinations:\n    # Scale features\n    X_scaled = scaler.fit_transform(df[features])\n    \n    LR = LogisticRegression(max_iter=10000)\n    \n    cv_scores_LR = cross_val_score(LR, X_scaled, y, cv=5)\n    results[str(features)] = cv_scores_LR.mean()\n\nfor combo, score in results.items():\n    print(f\"Features: {combo}\\nCV Score (mean accuracy): {score}\\n\")\n\nFeatures: ['loan_amnt', 'loan_int_rate']\nCV Score (mean accuracy): 0.7973108709793388\n\nFeatures: ['loan_percent_income', 'person_home_ownership_RENT']\nCV Score (mean accuracy): 0.8471203482091736\n\nFeatures: ['loan_amnt', 'loan_int_rate', 'loan_percent_income', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE']\nCV Score (mean accuracy): 0.848342893535451"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#evaluate-different-thresholds",
    "href": "posts/new-new-test-post/index.html#evaluate-different-thresholds",
    "title": "Whose Costs?",
    "section": "Evaluate Different Thresholds",
    "text": "Evaluate Different Thresholds"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#part-c-build-a-model",
    "href": "posts/New-test-thing-post/index.html#part-c-build-a-model",
    "title": "Whose Costs?",
    "section": "",
    "text": "I will be utilizing a Logistic Regression Model for predicting whether a prospective borrower is likely to default on a given loan.\nI will also be utilizing the RFECV class from sklearn in order to automatically find the most useful features to use in the model. RFE is a feature selection method that fits a model and removes the weakest feature(s) until the specified number of features is reached. Using RFE, you can automate the process of finding an effective subset of features.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import Pipeline\n\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_all = pd.read_csv(url)\n\ndf = df_all.drop(['loan_grade', 'loan_status'], axis=1)  \ndf = pd.get_dummies(df, columns=['person_home_ownership', 'loan_intent', 'cb_person_default_on_file'], drop_first=True)\ndf.dropna(inplace=True)\n\ny = df_all.loc[df.index, 'loan_status']  \n\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=123)\n\n# Scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlogreg = LogisticRegression(max_iter=1000)\n\nselector = RFECV(estimator=logreg, step=1, cv=5)\nselector = selector.fit(X_train_scaled, y_train)  # Fitting RFECV to the scaled training data\n\n\nX_train_selected = selector.transform(X_train_scaled)\nX_test_selected = selector.transform(X_test_scaled)\n\nmodel_selected = LogisticRegression(max_iter=1000)\nmodel_selected.fit(X_train_selected, y_train)\n\nselected_features = X_train.columns[selector.support_]\n\n# Prediction\ny_pred_selected = model_selected.predict(X_test_selected)\nprint(f\"Optimal number of features: {selector.n_features_}\")\nprint(classification_report(y_test, y_pred_selected))\n\nOptimal number of features: 8\n              precision    recall  f1-score   support\n\n           0       0.87      0.94      0.90      3614\n           1       0.68      0.48      0.56       968\n\n    accuracy                           0.84      4582\n   macro avg       0.77      0.71      0.73      4582\nweighted avg       0.83      0.84      0.83      4582\n\n\n\nThe code below is used to identify which features will be used for the most optimal model prediction.\n\nfeature_names = df.columns\n\nselected_features = feature_names[selector.support_]\n\nprint(\"Selected features:\", selected_features)\n\nSelected features: Index(['loan_amnt', 'loan_int_rate', 'loan_percent_income',\n       'person_home_ownership_OWN', 'person_home_ownership_RENT',\n       'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE'],\n      dtype='object')\n\n\n\n\nX = df[selected_features]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n## MODEL HERE\nmodel.fit(X_train_scaled, y_train)\n\nLogisticRegression(max_iter=10000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=10000)"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#cross-validation",
    "href": "posts/New-test-thing-post/index.html#cross-validation",
    "title": "Whose Costs?",
    "section": "",
    "text": "I did not want to limit myself to only one model however, so I used cross_val_score from the sklearn.model_selection in order to compare how my ‘Optimal’ LR model did against other LR models with a different combination of features.\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\nselected_features = ['loan_amnt', 'loan_int_rate', 'loan_percent_income', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE']\n\n\nfeature_combinations = [\n    ['loan_amnt', 'loan_int_rate'],  \n    ['loan_percent_income', 'person_home_ownership_RENT'],  \n    selected_features  \n]\n\n\nresults = {}\n\nfor features in feature_combinations:\n    # Scale features\n    X_scaled = scaler.fit_transform(df[features])\n    \n    LR = LogisticRegression(max_iter=10000)\n    \n    cv_scores_LR = cross_val_score(LR, X_scaled, y, cv=5)\n    results[str(features)] = cv_scores_LR.mean()\n\nfor combo, score in results.items():\n    print(f\"Features: {combo}\\nCV Score (mean accuracy): {score}\\n\")\n\nFeatures: ['loan_amnt', 'loan_int_rate']\nCV Score (mean accuracy): 0.7973108709793388\n\nFeatures: ['loan_percent_income', 'person_home_ownership_RENT']\nCV Score (mean accuracy): 0.8471203482091736\n\nFeatures: ['loan_amnt', 'loan_int_rate', 'loan_percent_income', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE']\nCV Score (mean accuracy): 0.848342893535451\n\n\n\nAs seen above, the ‘Optimal’ feature LR model performed the best in predicting the likelihood of a default."
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#part-d-finding-a-threshold",
    "href": "posts/New-test-thing-post/index.html#part-d-finding-a-threshold",
    "title": "Whose Costs?",
    "section": "",
    "text": "In this section, we will be exploring how I came up with the threshold t that will be used for our model.\nI used the numpy package to compute linear scores across all n of our training points.\nI plotted the scores below in order to easily visualize this process. By gathering scores, we can easily simulate decision-making with a given threshold.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef linear_score(X, w):\n    return X @ w\n\n\n# Extracting weights \nw = model.coef_.flatten()  \n\n\n\nscores = linear_score(X_train_selected, w) \n\n\nplt.hist(scores, bins=30, edgecolor='k', alpha=0.7)\nplt.xlabel(r\"Score $s$\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of Computed Scores\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure 3: Histogram of Computed Scores"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#evaluate-different-thresholds",
    "href": "posts/New-test-thing-post/index.html#evaluate-different-thresholds",
    "title": "Whose Costs?",
    "section": "Evaluate Different Thresholds",
    "text": "Evaluate Different Thresholds\nAlthough the previous data from Figure 3 and ?@fig-ROC are useful, often times banks want to set a threshold that maximizes profit-loss.\nWe use a two assumptions below to account for the profit and loss of a bank if a loan is repaid in full and if a loan is defaulted on.\nWe then calculate the expected profit per loan for each threshold, and find the threshold that is the most optimal for generating overall profit.\nThis relationship is illustrated in Figure 5.\n\ndef profit_repaid(loan_amnt, loan_int_rate):\n    return loan_amnt * (1 + 0.25 * loan_int_rate)**10 - loan_amnt\n\ndef loss_default(loan_amnt, loan_int_rate):\n    return loan_amnt * (1 + 0.25 * loan_int_rate)**3 - 20 * loan_amnt\n\n\n\nprobabilities = model.predict_proba(X_train_scaled)[:, 1]\n\nexpected_gains = []\nT = np.linspace(0, 1, 101)  \n\nfor t in T:\n    total_gain = 0\n    loans_issued = 0\n    \n    for actual, pred_prob, amt, rate in zip(y_train, probabilities, X_train['loan_amnt'], X_train['loan_int_rate']):\n        is_issued = pred_prob &gt;= t\n        if is_issued:\n            loans_issued += 1\n            if actual == 1:  # Loan is repaid\n                total_gain += profit_repaid(amt, rate)\n            else:  # Loan defaults\n                total_gain += loss_default(amt, rate)\n    \n    avg_gain = total_gain / loans_issued if loans_issued &gt; 0 else 0\n    expected_gains.append(avg_gain)\n\n\n\nplt.figure(figsize=(10, 6))\nplt.plot(T, expected_gains, label='Expected Gain')\nplt.xlabel(r\"Threshold $t$\")\nplt.ylabel(\"Expected Profit per Loan\")\nplt.title(\"Expected Profit per Loan vs. Threshold\")\nplt.xlim(0, 1)\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure 5: Expected Profit per Loan vs Threshold\n\n\n\n\n\n\n# Identify the refined optimal threshold\noptimal_index = np.argmax(expected_gains)\noptimal_threshold = T[optimal_index]\noptimal_gain = expected_gains[optimal_index]\n\nprint(f\"Optimal Threshold: {optimal_threshold}\")\nprint(f\"Maximum Expected Gain per Loan at Optimal Threshold: {optimal_gain}\")\n\nOptimal Threshold: 0.9500000000000001\nMaximum Expected Gain per Loan at Optimal Threshold: 248286039253.16788"
  },
  {
    "objectID": "posts/new-test-post/goal-setting.html",
    "href": "posts/new-test-post/goal-setting.html",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "Donovan Wood\n\n\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nAs mentioned, I want to obviously grow in each of these areas, but in particular I hope to expand my knowledge in the Theory and Experimentation areas.\nI want to know why things work, not just “They work, and here is how to use them”. Throughout my college experience I oftentimes found myself just doing the motions and I really want to try to emphasize fully grasping the material.\nIn terms of relevance to my future, I want to join a quantitative firm in the future, in order to research further in the field and develop systems that work efficiently in the market. Being an Economics and Computer Science Major, my interest lies in both and looking forward, this path seems to be the most interesting to me. That being said, machine learning is a huge component of that job field and so I want to be adept in my learnings/experimentations in the course so that I can use some of what I learned in real-world applications.\n\n\n\n\n\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\nCurrently, I plan to complete as many blog posts as I can, not only for the obvious grade, but also in order to access my learning accordingly. By doing as many as I can, I can directly apply the most recent class teachings and hope to build on more fundamental skills.\nThat is my current goal, however I do realize I am taking five courses, three of which are computer science courses, so I acknowledge that there will be times in which I have too much on my plate in order to complete every one. Still, I hope to at least keep this mentality throughout the course of the semester and do the best job I can.\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\nPreparing for the warmups is a task that I believe I can achieve every week. Not only does it help with the understandings of the class material that day, but also reinforces the idea of completion due to societal and class pressure ha! All joking aside, I hope to continue to complete all the warmups throughout the semester and get help from my peers if I’m stuck on one.\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\nEver since I joined the class, I always wanted to develop something that directly applies to my future career aspirations.\nThis may seem a bit ambitious, but I want to develop an machine learning algorithm that can create a positive return rate trading on some good (stock, etf, futures, options) in the market consistently.\nObviously things that come to mind are data limitations (real-time data), effective implementation, and non bias results but I still want to try to achieve this goal.\nAs of now, we are only beginning week 3 of the class, so I am unsure as to if that is possible given the course teachings, but I am willing to learn additional things outside of the class in order to achieve this. Even if I am unsuccessful in developing a model that achieves all my goals, I believe that the learning experience and process will be useful for my future endeavors."
  },
  {
    "objectID": "posts/new-test-post/goal-setting.html#what-youll-learn",
    "href": "posts/new-test-post/goal-setting.html#what-youll-learn",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "The knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nAs mentioned, I want to obviously grow in each of these areas, but in particular I hope to expand my knowledge in the Theory and Experimentation areas.\nI want to know why things work, not just “They work, and here is how to use them”. Throughout my college experience I oftentimes found myself just doing the motions and I really want to try to emphasize fully grasping the material.\nIn terms of relevance to my future, I want to join a quantitative firm in the future, in order to research further in the field and develop systems that work efficiently in the market. Being an Economics and Computer Science Major, my interest lies in both and looking forward, this path seems to be the most interesting to me. That being said, machine learning is a huge component of that job field and so I want to be adept in my learnings/experimentations in the course so that I can use some of what I learned in real-world applications."
  },
  {
    "objectID": "posts/new-test-post/goal-setting.html#what-youll-achieve",
    "href": "posts/new-test-post/goal-setting.html#what-youll-achieve",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "Most blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\nCurrently, I plan to complete as many blog posts as I can, not only for the obvious grade, but also in order to access my learning accordingly. By doing as many as I can, I can directly apply the most recent class teachings and hope to build on more fundamental skills.\nThat is my current goal, however I do realize I am taking five courses, three of which are computer science courses, so I acknowledge that there will be times in which I have too much on my plate in order to complete every one. Still, I hope to at least keep this mentality throughout the course of the semester and do the best job I can.\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\nPreparing for the warmups is a task that I believe I can achieve every week. Not only does it help with the understandings of the class material that day, but also reinforces the idea of completion due to societal and class pressure ha! All joking aside, I hope to continue to complete all the warmups throughout the semester and get help from my peers if I’m stuck on one.\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\nEver since I joined the class, I always wanted to develop something that directly applies to my future career aspirations.\nThis may seem a bit ambitious, but I want to develop an machine learning algorithm that can create a positive return rate trading on some good (stock, etf, futures, options) in the market consistently.\nObviously things that come to mind are data limitations (real-time data), effective implementation, and non bias results but I still want to try to achieve this goal.\nAs of now, we are only beginning week 3 of the class, so I am unsure as to if that is possible given the course teachings, but I am willing to learn additional things outside of the class in order to achieve this. Even if I am unsuccessful in developing a model that achieves all my goals, I believe that the learning experience and process will be useful for my future endeavors."
  },
  {
    "objectID": "Lecture Notes/10-compas.html",
    "href": "Lecture Notes/10-compas.html",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Today we are going to study an extremely famous investigation into algorithmic decision-making in the sphere of criminal justice by @angwin2022machine, originally written for ProPublica in 2016. This investigation significantly accelerated the pace of research into bias and fairness in machine learning, due in combination to its simple message and publicly-available data.\nIt’s helpful to look at a sample form used for feature collection in the COMPAS risk assessment.\nYou may have already read about the COMPAS algorithm in the original article at ProPublica. Our goal today is to reproduce some of the main findings of this article and set the stage for a more systematic treatment of bias and fairness in machine learning.\nParts of these lecture notes are inspired by the original ProPublica analysis and Allen Downey’s expository case study on the same data.\n\n\n Let’s first obtain the data. I’ve hosted a copy on the course website, so we can download it using a URL.This data set was obtained by @angwin2022machine through a public records request. The data comprises two years worth of COMPAS scoring in Broward County, Florida.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nsns.set_style(\"whitegrid\")\nnp.set_printoptions(precision = 3)\npd.set_option('display.precision', 3)\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/compas/compas.csv\"\ncompas = pd.read_csv(url)\n\nFor today we are only going to consider a subset of columns.\n\ncols = [\"sex\", \"race\", \"decile_score\", \"two_year_recid\"]\ncompas = compas[cols]\n\nWe are also only going to consider white (Caucasian) and Black (African-American) defendants:\n\nis_white = compas[\"race\"] == \"Caucasian\"\nis_black = compas[\"race\"] == \"African-American\"\n\ncompas = compas[is_white | is_black]\ncompas = compas.copy()\n\nOur data now looks like this:\n\ncompas.head()\n\n\n\n\n\n\n\n\nsex\nrace\ndecile_score\ntwo_year_recid\n\n\n\n\n1\nMale\nAfrican-American\n3\n1\n\n\n2\nMale\nAfrican-American\n4\n1\n\n\n3\nMale\nAfrican-American\n8\n0\n\n\n6\nMale\nCaucasian\n6\n1\n\n\n8\nFemale\nCaucasian\n1\n0\n\n\n\n\n\n\n\n\n\n\nLet’s do some quick exploration of our data. How many defendants are present in this data of each sex?\n\ncompas.groupby(\"sex\").size()\n\nsex\nFemale    1219\nMale      4931\ndtype: int64\n\n\nWhat about race?\n\ncompas.groupby(\"race\").size()\n\nrace\nAfrican-American    3696\nCaucasian           2454\ndtype: int64\n\n\nThe decile score is the algorithm’s prediction. Higher decile scores indicate that, according to the COMPAS model, the defendant has higher likelihood to be charged with a crime within the next two years. In the framework we’ve developed in this class, you can think of the decile score as being produced by computing a score like \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\) for each defendant \\(i\\), and then dividing these into the lowest 10% (decile score 1), the next 10% (decile score 2), the next 10% (decile score 3) and so on.\nThe easiest way to see how this looks is with a bar chart, which we can make efficiently using the seaborn (sns) package.\n\ncounts = compas.groupby([\"race\", \"decile_score\"]).size().reset_index(name = \"n\")\np = sns.barplot(data = counts, \n                x = \"decile_score\", \n                y = \"n\", \n                hue = \"race\", \n                palette = \"BuPu\", \n                saturation = 0.5)\n\n\n\n\n\n\n\n\nYou may notice that the number of white defendants who receive a given decile score tends to decrease as the score increases, whereas the number of Black defendants remains relatively constant.\nLet’s also take a look at the recidivism rate in the data:\n\ncompas[\"two_year_recid\"].mean()\n\n0.4661788617886179\n\n\nSo, in these data, approximately 47% of all defendants went on to be charged of another crime within the next two years. This is sometimes called the prevalence of the outcome. Although this is not a “good” outcome, it is labeled 1 in the target data and so we refer to this as the “positive” outcome. Prevalence without further specification usually refers to prevalence of the positive outcome.\nThe base rate of prediction accuracy in this problem is 53%: if we always guessed that the defendant was not arrested within two years, we would be right 53% of the time.\nWe can also compute the prevalence broken down by race of the defendant:\n\ncompas.groupby(\"race\")[\"two_year_recid\"].mean()\n\nrace\nAfrican-American    0.514\nCaucasian           0.394\nName: two_year_recid, dtype: float64\n\n\n\n\nWhen interpreting these different prevalences, it is important to remember that\n\nRace is itself a socially-constructed system of human categorization invented by humans with political and economic motives to describe other humans as property [@bonilla-silvaRacismRacistsColorblind2018].\n\nThe relation between arrest and actual criminal offense can display racial bias, with effects varying by geography [@fogliatoValidityArrestProxy2021].\nDecisions about which behaviors are criminal are contingent political decisions which have, historically, fallen hardest on Black Americans [@yusefCriminalizingRaceRacializing2017].\n\nThe prevalences between the two groups are substantially different. This difference will have major consequences later on for the possibility of different kinds of fairness in classifiers.\nWe’re going to treat the COMPAS algorithm as a binary classifier, but you might notice a problem: the algorithm’s prediction is the decile_score column, which is not actually a 0-1 label. Following the analysis of @angwin2022machine, we are going to construct a new binary column in which we say that a defendant is predicted_high_risk if their decile_score is larger than 4.\n\ncompas[\"predicted_high_risk\"] = compas[\"decile_score\"] &gt; 4 \n\nNow that we’ve done that, we can ask: how likely are Black and white defendants to receive positive predictions in this data?\n\ncompas.groupby(\"race\")[[\"two_year_recid\", \"predicted_high_risk\"]].mean()\n\n\n\n\n\n\n\n\ntwo_year_recid\npredicted_high_risk\n\n\nrace\n\n\n\n\n\n\nAfrican-American\n0.514\n0.588\n\n\nCaucasian\n0.394\n0.348\n\n\n\n\n\n\n\nBlack defendants are substantially more likely to receive a positive prediction than white defendants, and the disparity is larger than the observed prevalence of the positive outcome.\n\n\n\n\n\n\nFairness (Part 1)\n\n\n\nIs this fair? What is your gut telling you? Yes, no, possibly? What information would you need in order to make a judgment? What is the principle on which your judgment rests?\n\n\n\n\n\nLet’s now ask a few questions about the the predictive accuracy of this algorithm. First, how accurate it is it overall?\n\ncompas[\"correct_prediction\"] = (compas[\"predicted_high_risk\"] == compas[\"two_year_recid\"])\ncompas[\"correct_prediction\"].mean()\n\n0.6508943089430894\n\n\nRecall that the base rate in this problem is 53%, so our accuracy is somewhat better than random guessing.\nWhat about the accuracy on Black and white defendants separately?\n\ncompas.groupby([\"race\"])[\"correct_prediction\"].mean()\n\nrace\nAfrican-American    0.638\nCaucasian           0.670\nName: correct_prediction, dtype: float64\n\n\nThe overall accuracies for Black and white defendants are comparable, and both are somewhat higher than the base rate of 53%.\nWhat about the error rates? Here is a simple calculation which computes the false positive rate (FPR) in the first row and the true positive rate (TPR) on the bottom row:\n\ncompas.groupby([\"two_year_recid\"])[\"predicted_high_risk\"].mean().reset_index()\n\n\n\n\n\n\n\n\ntwo_year_recid\npredicted_high_risk\n\n\n\n\n0\n0\n0.352\n\n\n1\n1\n0.654\n\n\n\n\n\n\n\nHowever, and this was the main finding of the ProPublica study, the FPR and FNR are very different when we break down the data by race:\n\ncompas.groupby([\"two_year_recid\", \"race\"])[\"predicted_high_risk\"].mean().reset_index()\n\n\n\n\n\n\n\n\ntwo_year_recid\nrace\npredicted_high_risk\n\n\n\n\n0\n0\nAfrican-American\n0.448\n\n\n1\n0\nCaucasian\n0.235\n\n\n2\n1\nAfrican-American\n0.720\n\n\n3\n1\nCaucasian\n0.523\n\n\n\n\n\n\n\nThe false positive rate for Black defendants is much higher than the false positive rate for white defendants. This was the main finding of @angwin2022machine. The FPR of 44% for Black defendants means that, out of every 100 Black defendants who in fact will not commit another crime, the algorithm nevertheless predicts that 44 of them will. In contrast, the FPR of 23% for white defendants indicates that only 23 out of 100 non-recidivating white defendants would be predicted to recidivate.\nThere are a few ways in which we can think of this result as reflecting bias:\n\nThe algorithm has learned an implicit pattern wherein Black defendants are intrinsically more “criminal” than white defendants, even among people who factually never committed another crime. This is a bias in the patterns that the algorithm has learned in order to formulate its predictions. This is related to the idea of representational bias, in which algorithms learn and reproduce toxic stereotypes about certain groups of people.\nRegardless of how the algorithm forms its predictions, the impact of the algorithm being used in the penal system is that more Black defendants will be classified as high-risk, resulting in more denials of parole, bail, early release, or other forms of freedom from the penal system. So, the algorithm has disparate impact on people. This is sometimes called allocative or distributional bias: bias in how resources or opportunities (in this case, freedom) are allocated or distributed between groups.\n\nSometimes predictive equality is also defined to require that the false negative rates (FNRs) be equal across the two groups as well.\nWe can think about the argument of @angwin2022machine as a two-step argument:\n\n\nThe COMPAS algorithm has disparate error rates by race.\nTherefore, the COMPAS algorithm is unjustly biased with respect to race.\n\n\nThis argument implicitly equates equality of error rates with lack of bias.\n\n\n\n\n\n\nFairness (Part 2)\n\n\n\n\nSuppose that we developed an alternative algorithm in which the false positive rates were equal, but there were still more positive predictions for Black defendants overall. Would that be enough to ensure fairness?\nSuppose that we developed an alternative prediction algorithm in which the rate of positive prediction was the same across racial groups, but the false positive rates were different. Would that be to ensure fairness?\n\n\n\n\n\n\n@angwin2022machine kicked off a vigorous discussion about what it means for an algorithm to fair and how to measure deviations from bias. In particular, Northpointe, the company that developed COMPAS, issued a report @flores2016false in which they argued that their algorithm was fair. Their argument is based on an idea of fairness which is sometimes called sufficiency @corbett-daviesAlgorithmicDecisionMaking2017.\nHere’s the intuition expressed by sufficiency. Imagine that you and your friend both received an A- in Data Structures. Suppose, however, that the instructor says different things to each of you:\n\nTo you, the instructor says: “You did fine in this class, but I don’t think that you are prepared to take Computer Architecture. I gave you a higher grade than I would normally because you wear cool hats in class.”\nTo your friend, the instructor says: “*You did fine in this class and I think you are prepared to take Computer Architecture. Some students got a bump in their grade because they are cool-hat-wearers, but you didn’t get that benefit.”\n\nFeels unfair, right? The instructor is saying that:\n\nWhat a grade means for you in terms of your future success depends on your identity group.\n\n\n\n\n\n\n\nNote\n\n\n\nSuppose that you heard this, but instead of cool hats it was because you are a member of an identity group that “needs some help” in order to achieve equitable representation in the CS major. How would you feel? Would that feel fair to you?\n\n\nWe’ll formally define sufficiency in a future lecture. For now, let’s use an informal definition:\n\nSufficiency means that a positive prediction means the same thing for future outcomes for each racial group.\n\nTo operationalize this idea, we are looking for the rate of re-arrest to be the same between (a) Black defendants who received a positive prediction and (b) white defendants who received a positive prediction.\nLet’s check this:\nThe rates of rearrest are relatively similar between groups when controlling for the predictions they collectively received. Formal statistical hypothesis tests are typically used to determine whether this difference is sufficiently “real” to warrant correction. In most of the published literature, scholars have considered that the two rates are sufficiently close that we should instead simply say that COMPAS appears to be relatively close to satisfying sufficiency.\nIndeed, in a rejoinder article published by affiliates of the company Northpointe which produced COMPAS, the fact that COMPAS satisfies sufficiency is one of the primary arguments [@flores2016false].\n\n\n\nIn these notes, we replicated the data analysis of @angwin2022machine, finding that the COMPAS algorithm has disparate error rates between Black and white defendants. We introduced the idea that fairness actually has several different facets in our moral intuitions, and found that the COMPAS algorithm satisfies one of them (sufficiency: equal scores mean the same thing regardless of your group membership) but not the others (equal prediction rates and equal error rates).\n\n\n\n\nCan we have it all? Could we modify the COMPAS algorithm in such a way that it satisfies all the ideas of fairness that we discussed above? Could we then call it “fair” or “unbiased?”\nAre there other ways to define fairness? Which ones are most compelling to us? Does the right idea of fairness depend on the context in which we apply it?\nHow did this happen? The COMPAS algorithm was never trained on race data about the defendant. How did it happen that this algorithm nevertheless made recommendations at different rates across groups?\nIs automated decision-making legitimate in this setting? Can it be legitimate (just, fair) to use an automated decision-system for making recommendations about parole and sentencing decisions at all? What safeguards and forms of recourse are necessary for the legitimate use of automated decision-making in criminal justice?\nWhat are the systemic impacts? Disparate sentencing decisions can have downstream impacts on communities and institutions. How could application of the COMPAS algorithm exacerbate systemic inequalities?"
  },
  {
    "objectID": "Lecture Notes/10-compas.html#data-preparation",
    "href": "Lecture Notes/10-compas.html#data-preparation",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Let’s first obtain the data. I’ve hosted a copy on the course website, so we can download it using a URL.This data set was obtained by @angwin2022machine through a public records request. The data comprises two years worth of COMPAS scoring in Broward County, Florida.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nsns.set_style(\"whitegrid\")\nnp.set_printoptions(precision = 3)\npd.set_option('display.precision', 3)\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/compas/compas.csv\"\ncompas = pd.read_csv(url)\n\nFor today we are only going to consider a subset of columns.\n\ncols = [\"sex\", \"race\", \"decile_score\", \"two_year_recid\"]\ncompas = compas[cols]\n\nWe are also only going to consider white (Caucasian) and Black (African-American) defendants:\n\nis_white = compas[\"race\"] == \"Caucasian\"\nis_black = compas[\"race\"] == \"African-American\"\n\ncompas = compas[is_white | is_black]\ncompas = compas.copy()\n\nOur data now looks like this:\n\ncompas.head()\n\n\n\n\n\n\n\n\nsex\nrace\ndecile_score\ntwo_year_recid\n\n\n\n\n1\nMale\nAfrican-American\n3\n1\n\n\n2\nMale\nAfrican-American\n4\n1\n\n\n3\nMale\nAfrican-American\n8\n0\n\n\n6\nMale\nCaucasian\n6\n1\n\n\n8\nFemale\nCaucasian\n1\n0"
  },
  {
    "objectID": "Lecture Notes/10-compas.html#preliminary-explorations",
    "href": "Lecture Notes/10-compas.html#preliminary-explorations",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Let’s do some quick exploration of our data. How many defendants are present in this data of each sex?\n\ncompas.groupby(\"sex\").size()\n\nsex\nFemale    1219\nMale      4931\ndtype: int64\n\n\nWhat about race?\n\ncompas.groupby(\"race\").size()\n\nrace\nAfrican-American    3696\nCaucasian           2454\ndtype: int64\n\n\nThe decile score is the algorithm’s prediction. Higher decile scores indicate that, according to the COMPAS model, the defendant has higher likelihood to be charged with a crime within the next two years. In the framework we’ve developed in this class, you can think of the decile score as being produced by computing a score like \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\) for each defendant \\(i\\), and then dividing these into the lowest 10% (decile score 1), the next 10% (decile score 2), the next 10% (decile score 3) and so on.\nThe easiest way to see how this looks is with a bar chart, which we can make efficiently using the seaborn (sns) package.\n\ncounts = compas.groupby([\"race\", \"decile_score\"]).size().reset_index(name = \"n\")\np = sns.barplot(data = counts, \n                x = \"decile_score\", \n                y = \"n\", \n                hue = \"race\", \n                palette = \"BuPu\", \n                saturation = 0.5)\n\n\n\n\n\n\n\n\nYou may notice that the number of white defendants who receive a given decile score tends to decrease as the score increases, whereas the number of Black defendants remains relatively constant.\nLet’s also take a look at the recidivism rate in the data:\n\ncompas[\"two_year_recid\"].mean()\n\n0.4661788617886179\n\n\nSo, in these data, approximately 47% of all defendants went on to be charged of another crime within the next two years. This is sometimes called the prevalence of the outcome. Although this is not a “good” outcome, it is labeled 1 in the target data and so we refer to this as the “positive” outcome. Prevalence without further specification usually refers to prevalence of the positive outcome.\nThe base rate of prediction accuracy in this problem is 53%: if we always guessed that the defendant was not arrested within two years, we would be right 53% of the time.\nWe can also compute the prevalence broken down by race of the defendant:\n\ncompas.groupby(\"race\")[\"two_year_recid\"].mean()\n\nrace\nAfrican-American    0.514\nCaucasian           0.394\nName: two_year_recid, dtype: float64\n\n\n\n\nWhen interpreting these different prevalences, it is important to remember that\n\nRace is itself a socially-constructed system of human categorization invented by humans with political and economic motives to describe other humans as property [@bonilla-silvaRacismRacistsColorblind2018].\n\nThe relation between arrest and actual criminal offense can display racial bias, with effects varying by geography [@fogliatoValidityArrestProxy2021].\nDecisions about which behaviors are criminal are contingent political decisions which have, historically, fallen hardest on Black Americans [@yusefCriminalizingRaceRacializing2017].\n\nThe prevalences between the two groups are substantially different. This difference will have major consequences later on for the possibility of different kinds of fairness in classifiers.\nWe’re going to treat the COMPAS algorithm as a binary classifier, but you might notice a problem: the algorithm’s prediction is the decile_score column, which is not actually a 0-1 label. Following the analysis of @angwin2022machine, we are going to construct a new binary column in which we say that a defendant is predicted_high_risk if their decile_score is larger than 4.\n\ncompas[\"predicted_high_risk\"] = compas[\"decile_score\"] &gt; 4 \n\nNow that we’ve done that, we can ask: how likely are Black and white defendants to receive positive predictions in this data?\n\ncompas.groupby(\"race\")[[\"two_year_recid\", \"predicted_high_risk\"]].mean()\n\n\n\n\n\n\n\n\ntwo_year_recid\npredicted_high_risk\n\n\nrace\n\n\n\n\n\n\nAfrican-American\n0.514\n0.588\n\n\nCaucasian\n0.394\n0.348\n\n\n\n\n\n\n\nBlack defendants are substantially more likely to receive a positive prediction than white defendants, and the disparity is larger than the observed prevalence of the positive outcome.\n\n\n\n\n\n\nFairness (Part 1)\n\n\n\nIs this fair? What is your gut telling you? Yes, no, possibly? What information would you need in order to make a judgment? What is the principle on which your judgment rests?"
  },
  {
    "objectID": "Lecture Notes/10-compas.html#the-propublica-findings",
    "href": "Lecture Notes/10-compas.html#the-propublica-findings",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Let’s now ask a few questions about the the predictive accuracy of this algorithm. First, how accurate it is it overall?\n\ncompas[\"correct_prediction\"] = (compas[\"predicted_high_risk\"] == compas[\"two_year_recid\"])\ncompas[\"correct_prediction\"].mean()\n\n0.6508943089430894\n\n\nRecall that the base rate in this problem is 53%, so our accuracy is somewhat better than random guessing.\nWhat about the accuracy on Black and white defendants separately?\n\ncompas.groupby([\"race\"])[\"correct_prediction\"].mean()\n\nrace\nAfrican-American    0.638\nCaucasian           0.670\nName: correct_prediction, dtype: float64\n\n\nThe overall accuracies for Black and white defendants are comparable, and both are somewhat higher than the base rate of 53%.\nWhat about the error rates? Here is a simple calculation which computes the false positive rate (FPR) in the first row and the true positive rate (TPR) on the bottom row:\n\ncompas.groupby([\"two_year_recid\"])[\"predicted_high_risk\"].mean().reset_index()\n\n\n\n\n\n\n\n\ntwo_year_recid\npredicted_high_risk\n\n\n\n\n0\n0\n0.352\n\n\n1\n1\n0.654\n\n\n\n\n\n\n\nHowever, and this was the main finding of the ProPublica study, the FPR and FNR are very different when we break down the data by race:\n\ncompas.groupby([\"two_year_recid\", \"race\"])[\"predicted_high_risk\"].mean().reset_index()\n\n\n\n\n\n\n\n\ntwo_year_recid\nrace\npredicted_high_risk\n\n\n\n\n0\n0\nAfrican-American\n0.448\n\n\n1\n0\nCaucasian\n0.235\n\n\n2\n1\nAfrican-American\n0.720\n\n\n3\n1\nCaucasian\n0.523\n\n\n\n\n\n\n\nThe false positive rate for Black defendants is much higher than the false positive rate for white defendants. This was the main finding of @angwin2022machine. The FPR of 44% for Black defendants means that, out of every 100 Black defendants who in fact will not commit another crime, the algorithm nevertheless predicts that 44 of them will. In contrast, the FPR of 23% for white defendants indicates that only 23 out of 100 non-recidivating white defendants would be predicted to recidivate.\nThere are a few ways in which we can think of this result as reflecting bias:\n\nThe algorithm has learned an implicit pattern wherein Black defendants are intrinsically more “criminal” than white defendants, even among people who factually never committed another crime. This is a bias in the patterns that the algorithm has learned in order to formulate its predictions. This is related to the idea of representational bias, in which algorithms learn and reproduce toxic stereotypes about certain groups of people.\nRegardless of how the algorithm forms its predictions, the impact of the algorithm being used in the penal system is that more Black defendants will be classified as high-risk, resulting in more denials of parole, bail, early release, or other forms of freedom from the penal system. So, the algorithm has disparate impact on people. This is sometimes called allocative or distributional bias: bias in how resources or opportunities (in this case, freedom) are allocated or distributed between groups.\n\nSometimes predictive equality is also defined to require that the false negative rates (FNRs) be equal across the two groups as well.\nWe can think about the argument of @angwin2022machine as a two-step argument:\n\n\nThe COMPAS algorithm has disparate error rates by race.\nTherefore, the COMPAS algorithm is unjustly biased with respect to race.\n\n\nThis argument implicitly equates equality of error rates with lack of bias.\n\n\n\n\n\n\nFairness (Part 2)\n\n\n\n\nSuppose that we developed an alternative algorithm in which the false positive rates were equal, but there were still more positive predictions for Black defendants overall. Would that be enough to ensure fairness?\nSuppose that we developed an alternative prediction algorithm in which the rate of positive prediction was the same across racial groups, but the false positive rates were different. Would that be to ensure fairness?"
  },
  {
    "objectID": "Lecture Notes/10-compas.html#the-rebuttal",
    "href": "Lecture Notes/10-compas.html#the-rebuttal",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "@angwin2022machine kicked off a vigorous discussion about what it means for an algorithm to fair and how to measure deviations from bias. In particular, Northpointe, the company that developed COMPAS, issued a report @flores2016false in which they argued that their algorithm was fair. Their argument is based on an idea of fairness which is sometimes called sufficiency @corbett-daviesAlgorithmicDecisionMaking2017.\nHere’s the intuition expressed by sufficiency. Imagine that you and your friend both received an A- in Data Structures. Suppose, however, that the instructor says different things to each of you:\n\nTo you, the instructor says: “You did fine in this class, but I don’t think that you are prepared to take Computer Architecture. I gave you a higher grade than I would normally because you wear cool hats in class.”\nTo your friend, the instructor says: “*You did fine in this class and I think you are prepared to take Computer Architecture. Some students got a bump in their grade because they are cool-hat-wearers, but you didn’t get that benefit.”\n\nFeels unfair, right? The instructor is saying that:\n\nWhat a grade means for you in terms of your future success depends on your identity group.\n\n\n\n\n\n\n\nNote\n\n\n\nSuppose that you heard this, but instead of cool hats it was because you are a member of an identity group that “needs some help” in order to achieve equitable representation in the CS major. How would you feel? Would that feel fair to you?\n\n\nWe’ll formally define sufficiency in a future lecture. For now, let’s use an informal definition:\n\nSufficiency means that a positive prediction means the same thing for future outcomes for each racial group.\n\nTo operationalize this idea, we are looking for the rate of re-arrest to be the same between (a) Black defendants who received a positive prediction and (b) white defendants who received a positive prediction.\nLet’s check this:\nThe rates of rearrest are relatively similar between groups when controlling for the predictions they collectively received. Formal statistical hypothesis tests are typically used to determine whether this difference is sufficiently “real” to warrant correction. In most of the published literature, scholars have considered that the two rates are sufficiently close that we should instead simply say that COMPAS appears to be relatively close to satisfying sufficiency.\nIndeed, in a rejoinder article published by affiliates of the company Northpointe which produced COMPAS, the fact that COMPAS satisfies sufficiency is one of the primary arguments [@flores2016false]."
  },
  {
    "objectID": "Lecture Notes/10-compas.html#recap",
    "href": "Lecture Notes/10-compas.html#recap",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "In these notes, we replicated the data analysis of @angwin2022machine, finding that the COMPAS algorithm has disparate error rates between Black and white defendants. We introduced the idea that fairness actually has several different facets in our moral intuitions, and found that the COMPAS algorithm satisfies one of them (sufficiency: equal scores mean the same thing regardless of your group membership) but not the others (equal prediction rates and equal error rates)."
  },
  {
    "objectID": "Lecture Notes/10-compas.html#some-questions-moving-forward",
    "href": "Lecture Notes/10-compas.html#some-questions-moving-forward",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Can we have it all? Could we modify the COMPAS algorithm in such a way that it satisfies all the ideas of fairness that we discussed above? Could we then call it “fair” or “unbiased?”\nAre there other ways to define fairness? Which ones are most compelling to us? Does the right idea of fairness depend on the context in which we apply it?\nHow did this happen? The COMPAS algorithm was never trained on race data about the defendant. How did it happen that this algorithm nevertheless made recommendations at different rates across groups?\nIs automated decision-making legitimate in this setting? Can it be legitimate (just, fair) to use an automated decision-system for making recommendations about parole and sentencing decisions at all? What safeguards and forms of recourse are necessary for the legitimate use of automated decision-making in criminal justice?\nWhat are the systemic impacts? Disparate sentencing decisions can have downstream impacts on communities and institutions. How could application of the COMPAS algorithm exacerbate systemic inequalities?"
  }
]