[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Donovan’s Blog Post",
    "section": "",
    "text": "About this blog\nTesting this"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Whose Costs?",
    "section": "",
    "text": "The dataset below will be the one I use for this blog post\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf = pd.read_csv(url)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\n\nFor the dataset, I created two plots and one summary table in order to contextualize the data.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\nplt.figure(figsize=(12, 8))\nsns.scatterplot(x='person_age', y='person_emp_length', hue='loan_intent', data=df, palette='viridis')\nplt.title('Loan Intent vs. Age and Employment Length')\nplt.xlabel('Age')\nplt.ylabel('Employment Length (Years)')\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Loan Intent\n\n\n\n\n\n\nplt.figure(figsize=(10, 6))\nsns.countplot(x='person_home_ownership', hue='loan_grade', data=df, palette='Set2')\nplt.title('Loan Grade Distribution by Home Ownership')\nplt.xlabel('Home Ownership')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\n\nFigure 2: Loan Grade Distribution by Home Ownership\n\n\n\n\n\n\nsummary_table = df.groupby('loan_intent').agg({\n    'loan_int_rate': 'mean',\n    'loan_amnt': 'mean'\n}).reset_index()\n\nsummary_table.rename(columns={\n    'loan_int_rate': 'Average Interest Rate',\n    'loan_amnt': 'Average Loan Amount'\n}, inplace=True)\n\nprint(summary_table)\n\n         loan_intent  Average Interest Rate  Average Loan Amount\n0  DEBTCONSOLIDATION              10.983305          9620.901149\n1          EDUCATION              10.965465          9460.015604\n2    HOMEIMPROVEMENT              11.160075         10348.725017\n3            MEDICAL              11.051946          9242.269907\n4           PERSONAL              11.009814          9549.427178\n5            VENTURE              10.940866          9516.417425\n\n\n\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import Pipeline\n\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_all = pd.read_csv(url)\n\ndf = df_all.drop(['loan_grade', 'loan_status'], axis=1)  \ndf = pd.get_dummies(df, columns=['person_home_ownership', 'loan_intent', 'cb_person_default_on_file'], drop_first=True)\ndf.dropna(inplace=True)\n\ny = df_all.loc[df.index, 'loan_status']  \n\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=123)\n\n# Scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlogreg = LogisticRegression(max_iter=1000)\n\nselector = RFECV(estimator=logreg, step=1, cv=5)\nselector = selector.fit(X_train_scaled, y_train)  # Fitting RFECV to the scaled training data\n\n\nX_train_selected = selector.transform(X_train_scaled)\nX_test_selected = selector.transform(X_test_scaled)\n\nmodel_selected = LogisticRegression(max_iter=1000)\nmodel_selected.fit(X_train_selected, y_train)\n\nselected_features = X_train.columns[selector.support_]\n\n# Prediction\ny_pred_selected = model_selected.predict(X_test_selected)\nprint(f\"Optimal number of features: {selector.n_features_}\")\nprint(classification_report(y_test, y_pred_selected))\n\nOptimal number of features: 8\n              precision    recall  f1-score   support\n\n           0       0.87      0.94      0.90      3614\n           1       0.68      0.48      0.56       968\n\n    accuracy                           0.84      4582\n   macro avg       0.77      0.71      0.73      4582\nweighted avg       0.83      0.84      0.83      4582\n\n\n\n\nfeature_names = df.columns\n\nselected_features = feature_names[selector.support_]\n\nprint(\"Selected features:\", selected_features)\n\nSelected features: Index(['loan_amnt', 'loan_int_rate', 'loan_percent_income',\n       'person_home_ownership_OWN', 'person_home_ownership_RENT',\n       'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE'],\n      dtype='object')\n\n\n\n\nX = df[selected_features]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n## MODEL HERE\nmodel.fit(X_train_scaled, y_train)\n\nNameError: name 'model' is not defined\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef linear_score(X, w):\n    return X @ w\n\n\n# Extracting weights \nw = model.coef_.flatten()  \n\n\n\nscores = linear_score(X_train_selected, w) \n\n\n\n\nplt.hist(scores, bins=30, edgecolor='k', alpha=0.7)\nplt.xlabel(r\"Score $s$\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of Computed Scores\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnum_thresholds = 101\nFPR = np.zeros(num_thresholds)\nTPR = np.zeros(num_thresholds)\n\nT = np.linspace(scores.min() - 0.1, scores.max() + 0.1, num_thresholds)\n\nfor i in range(num_thresholds):\n    t = T[i]\n    preds = scores &gt;= t\n    FPR[i] = ((preds == 1) & (y_train == 0)).sum() / (y_train == 0).sum()\n    TPR[i] = ((preds == 1) & (y_train == 1)).sum() / (y_train == 1).sum()\n\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 4))\n\nax.plot(FPR, TPR, color=\"black\")\nax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"grey\")  \nax.set_aspect('equal')\n\nax.set_xlabel(\"False Positive Rate\")\nax.set_ylabel(\"True Positive Rate\")\nax.set_title(\"ROC Curve\")\n\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\ndef profit_repaid(loan_amnt, loan_int_rate):\n    return loan_amnt * (1 + 0.25 * loan_int_rate)**10 - loan_amnt\n\ndef profit_default(loan_amnt, loan_int_rate):\n    return loan_amnt * (1 + 0.25 * loan_int_rate)**3 - 2 * loan_amnt\n\n\n\nprobabilities = model.predict_proba(X_train_scaled)[:, 1]\n\n\nexpected_gains = []\nT = np.linspace(0, 1, 101)  \n\nfor t in T:\n    preds = probabilities &gt;= t  # Prediction based on threshold: whether the loan would be issued\n\n    # Initialize gains and losses\n    total_gain = 0\n\n    # Loop over each loan\n    for amt, rate, pred, actual in zip(X_train['loan_amnt'], X_train['loan_int_rate'], preds, y_train):\n        if pred:  # Loan is predicted to be issued\n            if actual == 1:  # Loan is repaid\n                total_gain += profit_repaid(amt, rate)\n            else:  # Loan defaults\n                total_gain += loss_default(amt, rate)\n\n    # Average gain per loan\n    avg_gain = total_gain / len(X_train)\n    expected_gains.append(avg_gain)\n\n\n\n\nplt.figure(figsize=(10, 6))\nplt.plot(T, expected_gains, label='Expected Gain')\nplt.xlabel(r\"Threshold $t$\")\nplt.ylabel(\"Expected Profit per Loan\")\nplt.title(\"Expected Profit per Loan vs. Threshold\")\nplt.xlim(0, 1)\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\nselected_features = ['loan_amnt', 'loan_int_rate', 'loan_percent_income', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE']\n\n\nfeature_combinations = [\n    ['loan_amnt', 'loan_int_rate'],  \n    ['loan_percent_income', 'person_home_ownership_RENT'],  \n    selected_features  \n]\n\n\nresults = {}\n\nfor features in feature_combinations:\n    # Scale features\n    X_scaled = scaler.fit_transform(df[features])\n    \n    LR = LogisticRegression(max_iter=10000)\n    \n    cv_scores_LR = cross_val_score(LR, X_scaled, y, cv=5)\n    results[str(features)] = cv_scores_LR.mean()\n\nfor combo, score in results.items():\n    print(f\"Features: {combo}\\nCV Score (mean accuracy): {score}\\n\")\n\nFeatures: ['loan_amnt', 'loan_int_rate']\nCV Score (mean accuracy): 0.7973108709793388\n\nFeatures: ['loan_percent_income', 'person_home_ownership_RENT']\nCV Score (mean accuracy): 0.8471203482091736\n\nFeatures: ['loan_amnt', 'loan_int_rate', 'loan_percent_income', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE']\nCV Score (mean accuracy): 0.848342893535451"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\n\nImportError: cannot import name 'Perceptron' from 'source' (/Users/donovanwood/itsdwood.github.io-1/posts/example-blog-post/source.py)\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Warmup Exercise Penquin",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Warmup Exercise Penquin",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Donovan test code just to get started\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLSTM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApplying Machine Learning to Stock Price Prediction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSCI 0451: Reflective Goal-Setting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNewton’s Method for Logistic Regression\n\n\n\n\n\nThis Blog Post will implement Newton’s Method for Logistic Regression in Python.\n\n\n\n\n\nMay 15, 2024\n\n\nDonovan Wood\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\nThis Blog Post will cover the implementation of logistic regression in Python.\n\n\n\n\n\nMay 12, 2024\n\n\nDonovan Wood\n\n\n\n\n\n\n\n\n\n\n\n\nQuantitative Trading Model Using LSTM\n\n\n\n\n\nFinal Project for CS451\n\n\n\n\n\nApr 22, 2024\n\n\nAndre Xiao, James Ohr, Donovan Wood\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing the Perceptron Algorithm\n\n\n\n\n\nA simple implementation of the Perceptron Algorithm in Python.\n\n\n\n\n\nApr 2, 2024\n\n\nDonovan Wood\n\n\n\n\n\n\n\n\n\n\n\n\nMid-Course Reflection\n\n\n\n\n\nWe reflect on our learning, engagement, and achievement in the first part of the semester. \n\n\n\n\n\nApr 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLimits of the Quantitative Approach to Bias and Fairness\n\n\n\n\n\nMy Blog Post Discussing the Limits of the Quantitative Approach to Bias and Fairness in Machine Learning.\n\n\n\n\n\nMar 27, 2024\n\n\nDonovan Wood\n\n\n\n\n\n\n\n\n\n\n\n\nReplication Study, Dissecting racial bias in an algorithm used to manage the health of populations\n\n\n\n\n\nReplicating the results of the paper ‘Dissecting racial bias in an algorithm used to manage the health of populations’ by Obermeyer et al. (2019).\n\n\n\n\n\nMar 19, 2024\n\n\nDonovan Wood\n\n\n\n\n\n\n\n\n\n\n\n\nThe Women in Data Science (WiDS) Conference at Middlebury College\n\n\n\n\n\nHighlighting the Middlebury Women in Data Science Conference that took place on March 4th, 2024\n\n\n\n\n\nMar 14, 2024\n\n\nDonovan Wood\n\n\n\n\n\n\n\n\n\n\n\n\nWhose Costs?\n\n\n\n\n\nMy second blog post, creating an automated decision system for a hypothetical bank extending credit\n\n\n\n\n\nFeb 28, 2024\n\n\nDonovan Wood\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nMy first blog post, detailing my model for classifying Palmer Penguins\n\n\n\n\n\nFeb 19, 2024\n\n\nDonovan Wood\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html",
    "href": "posts/New-test-thing-post/index.html",
    "title": "Whose Costs?",
    "section": "",
    "text": "The dataset below will be the one I use for this blog post\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf = pd.read_csv(url)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\n\nFor the dataset, I created two plots and one summary table in order to contextualize the data.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\nplt.figure(figsize=(12, 8))\nsns.scatterplot(x='person_age', y='person_emp_length', hue='loan_intent', data=df, palette='viridis')\nplt.title('Loan Intent vs. Age and Employment Length')\nplt.xlabel('Age')\nplt.ylabel('Employment Length (Years)')\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Loan Intent\n\n\n\n\n\nRegarding Figure 1, we can see some patterns within the dataset among loan intentions.\nMost notably in the bottom left corner of the plot, which indicates both a young age and limited employment history, there is a culmination of both Educational and Venture loans being used.\nThis makes sense given the current context of the world, as young individuals would most likely be in college or other forms of education at this time. On a similar note, venture capital is often used for newly formed or aspiring companies. Hence, the youthful age and lack of employment history match.\nConversely, as age increases, loans for medical, home improvement, and debt consolidation become more likely. This evidence is runs concurrent with today’s societal concerns from our more elderly population.\n\nplt.figure(figsize=(10, 6))\nsns.countplot(x='person_home_ownership', hue='loan_grade', data=df, palette='Set2')\nplt.title('Loan Grade Distribution by Home Ownership')\nplt.xlabel('Home Ownership')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\n\nFigure 2: Loan Grade Distribution by Home Ownership\n\n\n\n\n\nFigure 2 illustrates the Loan grade distribution by home ownership. Loan Grade is viewed A-G with A being the highest loan grade and G being the lowest.\nAn interesting observation to make here is that for individuals in both the ‘Mortgage’ and ‘Home’ groups, the highest frequency of loans is rated as an A whereas for individuals in the ‘Rent’ group the highest frequency of loans is rated as an B.\nThis could give context to the economic state of home ownership and bank trust. Those with a mortgage/own property could perhaps be linked to being more trustworthy due to having a longer term connection with a property. Banks may view those in the ‘Rent’ category as a bit more risky due to the short term ability of renting.\n\nsummary_table = df.groupby('loan_intent').agg({\n    'loan_int_rate': 'mean',\n    'loan_amnt': 'mean'\n}).reset_index()\n\nsummary_table.rename(columns={\n    'loan_int_rate': 'Average Interest Rate',\n    'loan_amnt': 'Average Loan Amount'\n}, inplace=True)\n\nprint(summary_table)\n\n         loan_intent  Average Interest Rate  Average Loan Amount\n0  DEBTCONSOLIDATION              10.983305          9620.901149\n1          EDUCATION              10.965465          9460.015604\n2    HOMEIMPROVEMENT              11.160075         10348.725017\n3            MEDICAL              11.051946          9242.269907\n4           PERSONAL              11.009814          9549.427178\n5            VENTURE              10.940866          9516.417425\n\n\nThis summary table above displays the relative average interest rates and average loan amounts for each category of loan intent.\nHome improvement has the greatest average loan amount while similarly having the highest interest rate. On the flip side, education has the lowest average loan amount but the second lowest interest rate.\n\n\n\nI will be utilizing a Logistic Regression Model for predicting whether a prospective borrower is likely to default on a given loan.\nI will also be utilizing the RFECV class from sklearn in order to automatically find the most useful features to use in the model. RFE is a feature selection method that fits a model and removes the weakest feature(s) until the specified number of features is reached. Using RFE, you can automate the process of finding an effective subset of features.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import Pipeline\n\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_all = pd.read_csv(url)\n\ndf = df_all.drop(['loan_grade', 'loan_status'], axis=1)  \ndf = pd.get_dummies(df, columns=['person_home_ownership', 'loan_intent', 'cb_person_default_on_file'], drop_first=True)\ndf.dropna(inplace=True)\n\ny = df_all.loc[df.index, 'loan_status']  \n\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=123)\n\n# Scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlogreg = LogisticRegression(max_iter=1000)\n\nselector = RFECV(estimator=logreg, step=1, cv=5)\nselector = selector.fit(X_train_scaled, y_train)  # Fitting RFECV to the scaled training data\n\n\nX_train_selected = selector.transform(X_train_scaled)\nX_test_selected = selector.transform(X_test_scaled)\n\nmodel_selected = LogisticRegression(max_iter=1000)\nmodel_selected.fit(X_train_selected, y_train)\n\nselected_features = X_train.columns[selector.support_]\n\n# Prediction\ny_pred_selected = model_selected.predict(X_test_selected)\nprint(f\"Optimal number of features: {selector.n_features_}\")\nprint(classification_report(y_test, y_pred_selected))\n\n\n  \nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=123)\n\n\nX_train\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_Y\n\n\n\n\n7573\n26\n38000\n1.0\n8875\n7.51\n0.23\n2\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n15865\n25\n110004\n9.0\n15000\n7.66\n0.14\n4\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n18729\n29\n60000\n3.0\n7200\n7.88\n0.12\n7\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n1288\n24\n44000\n1.0\n3200\n8.00\n0.07\n2\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n12168\n65\n46000\n5.0\n10500\n16.32\n0.23\n24\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17438\n26\n48000\n10.0\n15000\n11.12\n0.31\n3\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n24566\n45\n118000\n15.0\n25000\n12.18\n0.21\n12\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n20105\n21\n26000\n5.0\n14500\n14.61\n0.56\n2\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\n\n\n17825\n35\n78000\n9.0\n9450\n10.99\n0.12\n8\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n22678\n38\n65000\n5.0\n15000\n10.36\n0.23\n17\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n18325 rows × 16 columns\n\n\n\n\n\n# Scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\nX_train_scaled\n\narray([[-0.26848589, -0.43778172, -0.91860701, ..., -0.45114679,\n        -0.4630992 , -0.46344676],\n       [-0.4252438 ,  0.65831624,  1.01527471, ..., -0.45114679,\n        -0.4630992 , -0.46344676],\n       [ 0.20178785, -0.10288151, -0.43513658, ...,  2.21657344,\n        -0.4630992 , -0.46344676],\n       ...,\n       [-1.05227545, -0.62045457,  0.04833385, ..., -0.45114679,\n        -0.4630992 ,  2.15774517],\n       [ 1.14233533,  0.17112776,  1.01527471, ..., -0.45114679,\n        -0.4630992 , -0.46344676],\n       [ 1.61260907, -0.02676782,  0.04833385, ..., -0.45114679,\n        -0.4630992 , -0.46344676]])\n\n\n\n\nlogreg = LogisticRegression(max_iter=1000)\n\nselector = RFECV(estimator=logreg, step=1, cv=5)\nselector = selector.fit(X_train_scaled, y_train)  # Fitting RFECV to the scaled training data\n\n\nX_train_selected = selector.transform(X_train_scaled)\nX_test_selected = selector.transform(X_test_scaled)\n\n\nX_train_selected\n\narray([[-0.12277222, -1.0852037 ,  0.57659278, ..., -0.49598922,\n        -0.45114679, -0.4630992 ],\n       [ 0.85178574, -1.03888993, -0.27169328, ..., -0.49598922,\n        -0.45114679, -0.4630992 ],\n       [-0.38928398, -0.97096305, -0.4602013 , ..., -0.49598922,\n         2.21657344, -0.4630992 ],\n       ...,\n       [ 0.77222999,  1.10698175,  3.68697501, ..., -0.49598922,\n        -0.45114679, -0.4630992 ],\n       [-0.0312831 , -0.01072407, -0.4602013 , ..., -0.49598922,\n        -0.45114679, -0.4630992 ],\n       [ 0.85178574, -0.20524194,  0.57659278, ...,  2.01617286,\n        -0.45114679, -0.4630992 ]])\n\n\n\nfrom sklearn.metrics import classification_report\n\nmodel_selected = LogisticRegression(max_iter=1000)\nmodel_selected.fit(X_train_selected, y_train)\n\nselected_features = X_train.columns[selector.support_]\n\n# Prediction\ny_pred_selected = model_selected.predict(X_test_selected)\nprint(f\"Optimal number of features: {selector.n_features_}\")\nprint(classification_report(y_test, y_pred_selected))\n\nOptimal number of features: 8\n              precision    recall  f1-score   support\n\n           0       0.87      0.94      0.90      3614\n           1       0.68      0.48      0.56       968\n\n    accuracy                           0.84      4582\n   macro avg       0.77      0.71      0.73      4582\nweighted avg       0.83      0.84      0.83      4582\n\n\n\nThe code below is used to identify which features will be used for the most optimal model prediction.\n\nfeature_names = df.columns\n\nselected_features = feature_names[selector.support_]\n\nprint(\"Selected features:\", selected_features)\n\nSelected features: Index(['loan_amnt', 'loan_int_rate', 'loan_percent_income',\n       'person_home_ownership_OWN', 'person_home_ownership_RENT',\n       'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE'],\n      dtype='object')\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nthresholds = np.linspace(0, 1, 101)\nissued_counts = []\n\nfor threshold in thresholds:\n    X_train['predicted_issued'] = X_train['probabilities'] &gt;= threshold\n    issued_count = X_train['predicted_issued'].sum()  \n    issued_counts.append(issued_count)\n\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, issued_counts)\nplt.title('Number of Loans Predicted to be Issued vs. Threshold')\nplt.xlabel('Threshold')\nplt.ylabel('Number of Loans Predicted to be Issued')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nX_train\n\n\n\n\n\n\n\n\nloan_amnt\nloan_int_rate\nloan_percent_income\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_EDUCATION\nloan_intent_PERSONAL\nloan_intent_VENTURE\nprobabilities\nactual\nprediction_probability\npredicted_issued\ngain_loss\n\n\n\n\n7573\n8875\n7.51\n0.23\nFalse\nFalse\nFalse\nFalse\nFalse\n0.105087\n0\n0.105087\nFalse\n0\n\n\n15865\n15000\n7.66\n0.14\nFalse\nFalse\nFalse\nFalse\nFalse\n0.023507\n0\n0.023507\nFalse\n0\n\n\n18729\n7200\n7.88\n0.12\nFalse\nFalse\nFalse\nTrue\nFalse\n0.022012\n0\n0.022012\nFalse\n0\n\n\n1288\n3200\n8.00\n0.07\nFalse\nTrue\nFalse\nFalse\nFalse\n0.061475\n0\n0.061475\nFalse\n0\n\n\n12168\n10500\n16.32\n0.23\nFalse\nTrue\nFalse\nFalse\nTrue\n0.600637\n0\n0.600637\nFalse\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17438\n15000\n11.12\n0.31\nTrue\nFalse\nFalse\nFalse\nTrue\n0.047207\n0\n0.047207\nFalse\n0\n\n\n24566\n25000\n12.18\n0.21\nFalse\nFalse\nFalse\nFalse\nFalse\n0.097552\n0\n0.097552\nFalse\n0\n\n\n20105\n14500\n14.61\n0.56\nFalse\nFalse\nFalse\nFalse\nFalse\n0.978011\n1\n0.978011\nFalse\n0\n\n\n17825\n9450\n10.99\n0.12\nFalse\nFalse\nFalse\nFalse\nFalse\n0.083158\n0\n0.083158\nFalse\n0\n\n\n22678\n15000\n10.36\n0.23\nFalse\nFalse\nTrue\nFalse\nFalse\n0.078296\n0\n0.078296\nFalse\n0\n\n\n\n\n18325 rows × 13 columns\n\n\n\n\n# Estimating the profit here into rows \n\nthresholds = np.linspace(0, 1, 101)\naverage_gains = []\n\nfor threshold in thresholds:\n    X_train['predicted_issued'] = X_train['prediction_probability'] &gt;= threshold\n    X_train['gain_loss'] = X_train.apply(calculate_gain_loss, axis=1)\n    average_gain_loss_per_issued_loan = X_train.loc[X_train['predicted_issued'], 'gain_loss'].mean()\n    average_gains.append(average_gain_loss_per_issued_loan)\n\n# Plotting the average gain per issued loan across thresholds\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, average_gains, label='Average Gain Per Issued Loan')\nplt.xlabel('Threshold')\nplt.ylabel('Average Gain Per Issued Loan')\nplt.title('Average Gain Per Issued Loan vs. Threshold')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\nX_train\n\n\n\n\n\n\n\n\nloan_amnt\nloan_int_rate\nloan_percent_income\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_EDUCATION\nloan_intent_PERSONAL\nloan_intent_VENTURE\nprobabilities\nactual\nprediction_probability\npredicted_issued\ngain_loss\n\n\n\n\n7573\n8875\n7.51\n0.23\nFalse\nFalse\nFalse\nFalse\nFalse\n0.105087\n0\n0.105087\nFalse\n0\n\n\n15865\n15000\n7.66\n0.14\nFalse\nFalse\nFalse\nFalse\nFalse\n0.023507\n0\n0.023507\nFalse\n0\n\n\n18729\n7200\n7.88\n0.12\nFalse\nFalse\nFalse\nTrue\nFalse\n0.022012\n0\n0.022012\nFalse\n0\n\n\n1288\n3200\n8.00\n0.07\nFalse\nTrue\nFalse\nFalse\nFalse\n0.061475\n0\n0.061475\nFalse\n0\n\n\n12168\n10500\n16.32\n0.23\nFalse\nTrue\nFalse\nFalse\nTrue\n0.600637\n0\n0.600637\nFalse\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17438\n15000\n11.12\n0.31\nTrue\nFalse\nFalse\nFalse\nTrue\n0.047207\n0\n0.047207\nFalse\n0\n\n\n24566\n25000\n12.18\n0.21\nFalse\nFalse\nFalse\nFalse\nFalse\n0.097552\n0\n0.097552\nFalse\n0\n\n\n20105\n14500\n14.61\n0.56\nFalse\nFalse\nFalse\nFalse\nFalse\n0.978011\n1\n0.978011\nFalse\n0\n\n\n17825\n9450\n10.99\n0.12\nFalse\nFalse\nFalse\nFalse\nFalse\n0.083158\n0\n0.083158\nFalse\n0\n\n\n22678\n15000\n10.36\n0.23\nFalse\nFalse\nTrue\nFalse\nFalse\n0.078296\n0\n0.078296\nFalse\n0\n\n\n\n\n18325 rows × 13 columns\n\n\n\n\n\n\nI did not want to limit myself to only one model however, so I used cross_val_score from the sklearn.model_selection in order to compare how my ‘Optimal’ LR model did against other LR models with a different combination of features.\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\nselected_features = ['loan_amnt', 'loan_int_rate', 'loan_percent_income', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE']\n\n\nfeature_combinations = [\n    ['loan_amnt', 'loan_int_rate'],  \n    ['loan_percent_income', 'person_home_ownership_RENT'],  \n    selected_features  \n]\n\n\nresults = {}\n\nfor features in feature_combinations:\n    # Scale features\n    X_scaled = scaler.fit_transform(df[features])\n    \n    LR = LogisticRegression(max_iter=10000)\n    \n    cv_scores_LR = cross_val_score(LR, X_scaled, y, cv=5)\n    results[str(features)] = cv_scores_LR.mean()\n\nfor combo, score in results.items():\n    print(f\"Features: {combo}\\nCV Score (mean accuracy): {score}\\n\")\n\nFeatures: ['loan_amnt', 'loan_int_rate']\nCV Score (mean accuracy): 0.7973108709793388\n\nFeatures: ['loan_percent_income', 'person_home_ownership_RENT']\nCV Score (mean accuracy): 0.8471203482091736\n\nFeatures: ['loan_amnt', 'loan_int_rate', 'loan_percent_income', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE']\nCV Score (mean accuracy): 0.848342893535451\n\n\n\nAs seen above, the ‘Optimal’ feature LR model performed the best in predicting the likelihood of a default.\n\n\n\nIn this section, we will be exploring how I came up with the threshold t that will be used for our model.\nI used the numpy package to compute linear scores across all n of our training points.\nI plotted the scores below in order to easily visualize this process. By gathering scores, we can easily simulate decision-making with a given threshold.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef linear_score(X, w):\n    return X @ w\n\n\n# Extracting weights \nw = model.coef_.flatten()  \n\n\n\nscores = linear_score(X_train_selected, w) \n\n\nplt.hist(scores, bins=30, edgecolor='k', alpha=0.7)\nplt.xlabel(r\"Score $s$\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of Computed Scores\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure 3: Histogram of Computed Scores"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#classifying-the-palmer-penguins",
    "href": "posts/New-test-thing-post/index.html#classifying-the-palmer-penguins",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Image source: @allisonhorst\n\n\nOur data set for these notes is Palmer Penguins. This data set contains physiological measurements and species labels for several populations of Adelie, Chinstrap, and Gentoo penguins."
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#abstract",
    "href": "posts/New-test-thing-post/index.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "Abstract",
    "text": "Abstract\nIn this blog post we use a logistic regression model in order to classify Palmer penguins according to their Species. We find that using information such as the Island of the penguin as well as Culmen Depth (mm) and Culmen Length are sufficient features in order to provide an accurate classifying model. Within this blog post, we examine multiple data visualizations and tables in order to better understand the data and develop our reasoning for choosing the features of the model.\nBelow we will import the dataset we will use for this Blog Post\n\nimport pandas as pd\nimport numpy as np\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#data-preparation",
    "href": "posts/New-test-thing-post/index.html#data-preparation",
    "title": "Classifying Palmer Penguins",
    "section": "Data Preparation",
    "text": "Data Preparation\nIn order to prepare the qualitative columns in the data, we must convert categorical feature columns like Sex and Island into 0-1 columns using pd.get_dummies function.\nThe label column Species will also needed to be coded differently, using a LabelEncoder\nThe following function will take care of these processes:\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n# Replace the column with the first word in each entry \ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#visualizations-of-the-data",
    "href": "posts/New-test-thing-post/index.html#visualizations-of-the-data",
    "title": "Whose Costs?",
    "section": "",
    "text": "For the dataset, I created two plots and one summary table in order to contextualize the data.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\nplt.figure(figsize=(12, 8))\nsns.scatterplot(x='person_age', y='person_emp_length', hue='loan_intent', data=df, palette='viridis')\nplt.title('Loan Intent vs. Age and Employment Length')\nplt.xlabel('Age')\nplt.ylabel('Employment Length (Years)')\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Loan Intent\n\n\n\n\n\nRegarding Figure 1, we can see some patterns within the dataset among loan intentions.\nMost notably in the bottom left corner of the plot, which indicates both a young age and limited employment history, there is a culmination of both Educational and Venture loans being used.\nThis makes sense given the current context of the world, as young individuals would most likely be in college or other forms of education at this time. On a similar note, venture capital is often used for newly formed or aspiring companies. Hence, the youthful age and lack of employment history match.\nConversely, as age increases, loans for medical, home improvement, and debt consolidation become more likely. This evidence is runs concurrent with today’s societal concerns from our more elderly population.\n\nplt.figure(figsize=(10, 6))\nsns.countplot(x='person_home_ownership', hue='loan_grade', data=df, palette='Set2')\nplt.title('Loan Grade Distribution by Home Ownership')\nplt.xlabel('Home Ownership')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\n\nFigure 2: Loan Grade Distribution by Home Ownership\n\n\n\n\n\nFigure 2 illustrates the Loan grade distribution by home ownership. Loan Grade is viewed A-G with A being the highest loan grade and G being the lowest.\nAn interesting observation to make here is that for individuals in both the ‘Mortgage’ and ‘Home’ groups, the highest frequency of loans is rated as an A whereas for individuals in the ‘Rent’ group the highest frequency of loans is rated as an B.\nThis could give context to the economic state of home ownership and bank trust. Those with a mortgage/own property could perhaps be linked to being more trustworthy due to having a longer term connection with a property. Banks may view those in the ‘Rent’ category as a bit more risky due to the short term ability of renting.\n\nsummary_table = df.groupby('loan_intent').agg({\n    'loan_int_rate': 'mean',\n    'loan_amnt': 'mean'\n}).reset_index()\n\nsummary_table.rename(columns={\n    'loan_int_rate': 'Average Interest Rate',\n    'loan_amnt': 'Average Loan Amount'\n}, inplace=True)\n\nprint(summary_table)\n\n         loan_intent  Average Interest Rate  Average Loan Amount\n0  DEBTCONSOLIDATION              10.983305          9620.901149\n1          EDUCATION              10.965465          9460.015604\n2    HOMEIMPROVEMENT              11.160075         10348.725017\n3            MEDICAL              11.051946          9242.269907\n4           PERSONAL              11.009814          9549.427178\n5            VENTURE              10.940866          9516.417425\n\n\nThis summary table above displays the relative average interest rates and average loan amounts for each category of loan intent.\nHome improvement has the greatest average loan amount while similarly having the highest interest rate. On the flip side, education has the lowest average loan amount but the second lowest interest rate."
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#model-choices",
    "href": "posts/New-test-thing-post/index.html#model-choices",
    "title": "Classifying Palmer Penguins",
    "section": "Model Choices",
    "text": "Model Choices\nFor this blog, I used the combinations function from the itertools package in order to go through each variation of both qualitative and quantitative features.\n\nfrom itertools import combinations\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Region\", ]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n\nI then utilized the LoigisticRegression model from sklearn in order to run tests for each iteration.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\n\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\nLR = LogisticRegression(max_iter=10000)\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n0.99609375\n\n\nGoing through each iteration, I scored each of them found below:\n[‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Culmen Length (mm)’, ‘Body Mass (g)’] : .91  [‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Culmen Depth (mm)’, ‘Flipper Length (mm)’]: .91  [‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Culmen Depth (mm)’, ‘Body Mass (g)’] : .76  [‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Flipper Length (mm)’, ‘Body Mass (g)’] : .63  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, ‘Culmen Depth (mm)’] : .992  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, ‘Flipper Length (mm)’] : .98  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, ‘Body Mass (g)’] : .98  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Depth (mm)’, ‘Flipper Length (mm)’] : .82  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Depth (mm)’, ‘Body Mass (g)’] : .77  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Flipper Length (mm)’, ‘Body Mass (g)’] : .71  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Length (mm)’, ‘Culmen Depth (mm)’] : .996  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Length (mm)’, ‘Flipper Length (mm)’]: .97  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Length (mm)’, ‘Body Mass (g)’]: .97  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Depth (mm)’, ‘Flipper Length (mm)’]:.88  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Depth (mm)’, ‘Body Mass (g)’]: .83  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Flipper Length (mm)’, ‘Body Mass (g)’] : .75"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#testing",
    "href": "posts/New-test-thing-post/index.html#testing",
    "title": "Classifying Palmer Penguins",
    "section": "Testing",
    "text": "Testing\nIn order to test the model, I am downloading the dataset and preparing it via the prepare_data function\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n1.0\n\n\nAwesome! Our model is able to classify the three species of penguins with 100 percent accuracy.\n\n\n\n\n\n\nFigure 3: Image"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#plotting-decision-regions",
    "href": "posts/New-test-thing-post/index.html#plotting-decision-regions",
    "title": "Classifying Palmer Penguins",
    "section": "PLotting Decision Regions",
    "text": "PLotting Decision Regions\nI will be using the plot_regions function below in order to plot my decision regions for my model.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Gentoo\", \"Chinstrap\", \"Adelie\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nFor the purposes of this blog post, we will look at the decision regions for both our training data and our testing data\n\nplot_regions(LR, X_train[cols], y_train)\n\n\n\n\n\n\n\nFigure 4: Decision Regions for each island based on training data\n\n\n\n\n\nThe three plots above correspond to the decision regions for each island. As previously mentioned in the data visualization section, we see that for the Biscoe and Dream islands, there are only two species of penguins. For the Torgersen island, only the Adelie penguins reside there. As predicted, this results in comparing at most only two species of penguins against one another, allowing for easier classification.\nThe following section displays the decision regions our model makes for the testing data.\n\nplot_regions(LR, X_test[cols], y_test)\n\n\n\n\n\n\n\nFigure 5: Decision Regions for each island based on testing data\n\n\n\n\n\nOur testing data yielded a smaller number of penguins to be classified, but our classifier was able to accurately predict each species of penguin based on our three features of Island, Culmen Depth (mm), and Culmen Length (mm)\nWhile the penguins on the Torgersen would always be Adelie, and were classified as such. The closest decision region our model came to for classification of the test data was a penguin on the Dream Island. This was anticipated however as in Figure 2, Adelie and Chinstrap penguins share some overlapping points in regards to Culmen Depth (mm) and Culmen Length(mm)"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#confusion-matrix",
    "href": "posts/New-test-thing-post/index.html#confusion-matrix",
    "title": "Classifying Palmer Penguins",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nI will be using a confusion matrix for my model, evaluated on the test set.\nIn order to do this I will be using confusion_matrix from the sklearn package.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test[cols])\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nIn this confusion matrix, due to our 100 percent prediction accuracy, we do not see any errors within our matrix. As a reference, the confusion matrix just gives the number of data points that have the correct label.\nLogically, since our model predicts each penguin correctly, there will be disparities in the matrix.\nThat being said, as previously mentioned, the most likely error to occur within our model is classifying a penguin from the Dream island. This is due to the Adelie and Chinstrap penguins sharing the most similar distributions of Culmen Depth (mm) and Culmen Length (mm) which depending on the penguin’s dimensions, has the highest likelihood out of all the species to overlap in the incorrect species decision region."
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#discussion",
    "href": "posts/New-test-thing-post/index.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Discussion",
    "text": "Discussion\nIn this blog post, I have found that the using Island, Culmen Depth (mm), and Culmen Length (mm) features in a logistic regression model are useful for accurately classifying species of penguins pertaining to Adelie, Gentoo, and Chinstrap.\nI have found that Adelie penguins reside in all three islands (Dream, Biscoe, Torgersen), while Gentoo penguins only reside on the Biscoe island and Chinstrap penguins only inhabit the Biscoe island.\nIn terms of Culmen Length (mm) and Culmen Depth (mm) we found that:\nGentoo penguins seem to exhbit a longer culmen length but shorter culmen depth.  Adelie penguins have a shorter culmen length compared to Gentoo penguins but a higher culmen depth.  Chinstrap penguins appear to have the longest average combination of culmen depth/length out of all the penguins.\nIn terms of Body Mass (g) we found that:\nGentoo penguins, both male and female, have the highest mean body mass out of all species of penguins.\nAdelie and Chinstrap penguins exhibit similar mean body mass to one another for both male and female."
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html",
    "href": "posts/classifying-penguin-post/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Image source: @allisonhorst\n\n\nOur data set for these notes is Palmer Penguins. This data set contains physiological measurements and species labels for several populations of Adelie, Chinstrap, and Gentoo penguins."
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#classifying-the-palmer-penguins",
    "href": "posts/classifying-penguin-post/index.html#classifying-the-palmer-penguins",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Image source: @allisonhorst\n\n\nOur data set for these notes is Palmer Penguins. This data set contains physiological measurements and species labels for several populations of Adelie, Chinstrap, and Gentoo penguins."
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#abstract",
    "href": "posts/classifying-penguin-post/index.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "Abstract",
    "text": "Abstract\nIn this blog post we use a logistic regression model in order to classify Palmer penguins according to their Species. We find that using information such as the Island of the penguin as well as Culmen Depth (mm) and Culmen Length are sufficient features in order to provide an accurate classifying model. Within this blog post, we examine multiple data visualizations and tables in order to better understand the data and develop our reasoning for choosing the features of the model.\nBelow we will import the dataset we will use for this Blog Post\n\nimport pandas as pd\nimport numpy as np\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)"
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#data-preparation",
    "href": "posts/classifying-penguin-post/index.html#data-preparation",
    "title": "Classifying Palmer Penguins",
    "section": "Data Preparation",
    "text": "Data Preparation\nIn order to prepare the qualitative columns in the data, we must convert categorical feature columns like Sex and Island into 0-1 columns using pd.get_dummies function.\nThe label column Species will also needed to be coded differently, using a LabelEncoder\nThe following function will take care of these processes:\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n# Replace the column with the first word in each entry \ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)"
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#visualizations-of-the-data",
    "href": "posts/classifying-penguin-post/index.html#visualizations-of-the-data",
    "title": "Classifying Palmer Penguins",
    "section": "Visualizations of the Data",
    "text": "Visualizations of the Data\nFor the dataset, I created two plots and one summary table in order to contextualize the data.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\n# Create a bar plot\nplt.figure(figsize=(10, 6))\nsns.countplot(data=train, x='Island', hue='Species')\n\n# Customize the plot\nplt.title('Island Distribution for Each Penguin Species')\nplt.xlabel('Island')\nplt.ylabel('Count')\nplt.legend(title='Species')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Island Distribution for Each Penguin Species\n\n\n\n\n\nAfter running through a multitude of different plots with varying sets of data, one combination that particularly stood out to me is shown in Figure 1. The plot shows the distribution of each species of Penguin on the three islands within the dataset. Now while it seems rather plain and simple at first glance, take note that for one of these islands (Torgersen), only one species of penguins resides there, that being Adeilie. Upon further investigation, for the remaining two islands, at most, only two differing penguin species reside there. Adelie inhabit all three islands while Chinstrap penguins reside only on the Dream island and Gentoo Penguins are only found on the Biscoe Island.\nThe reason as to why this rather simple analysis is so interesting as it presents a way in which we no longer have to compare all three species against one another at a given point. Rather, we can now determine whether the penguin presented to us is one of two species if the island is Dream or Biscoe. In the case of Torgersen, we automatically know the penguin will be Adelie as they are the only species of Penguins that reside there!\nThis knowledge, paired with two more quantitative features, should allow the model to classify a penguin more accurately!\n\nsns.scatterplot(train, x = \"Culmen Depth (mm)\", y = \"Culmen Length (mm)\", hue = \"Species\", style = \"Species\")\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nWith that idea now set. Let’s move on to the second visualization, shown by Figure 2. This is a more traditional scatterplot showing the distributions of Culmen_Depth and Culmen_Length for all three species of penguin.\nNow recalling ?@fig-scatter-island, the main relationships we want to focus on are Chinstrap vs Adelie and Gentoo vs Adelie as these are the only relationships where penguins coexist on the same island.\nAs we can see from the Figure 2. Gentoo penguins seem to exhbit a longer culmen length but shorter culmen depth.  Adelie penguins have a shorter culmen length compared to Gentoo penguins but a higher culmen depth.  Chinstrap penguins appear to have the longest average combination of culmen depth/length out of all the penguins.\nReferring back to those relationships as mentioned previously, Gentoo and Adelie penguins seem to have a clear divide amongst one another in terms of culmen length/depth. Chinstrap and Adelie penguins seem to overlap at some points in the graph but fairly slightly.\nAll things considered, given the properties of both ?@fig-scatter-island and Figure 2, the combination of these three features will be useful as a starting reference for my model.\n\n## Summary table for the mean body mass of each species of penguin by sex\n\nsummary_table = train.groupby(['Species', 'Sex']).agg({'Body Mass (g)': 'mean'})\n\nprint(summary_table)\n\n                  Body Mass (g)\nSpecies   Sex                  \nAdelie    FEMALE    3350.471698\n          MALE      4052.868852\nChinstrap FEMALE    3523.387097\n          MALE      4005.769231\nGentoo    .         4875.000000\n          FEMALE    4684.693878\n          MALE      5476.704545\n\n\nMy last visualization was a simple summary table of the mean body mass (g) of each penguin species by sex. I wanted to ensure that I was covering a large portion of the data, so by doing so, I can cover two features of the dataset I did not cover in the two previous data plots.\nAs a quick analysis:  Gentoo Males exhibit the largest mean body mass (g) out of all the male species  Similarly Gentoo females also exhibit the largest mean body mass (g) out of all the female species  In regards to Adelie and Chinstrap penguins, both males and females exhibit a similar mean mass (g) in relation to the other species.\nWith that in mind, the combination of body mass (g) and sex do not seem like a good pairing to use for predicition due to the similar nature of both Adelie and Chinstrap penguins."
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#model-choices",
    "href": "posts/classifying-penguin-post/index.html#model-choices",
    "title": "Classifying Palmer Penguins",
    "section": "Model Choices",
    "text": "Model Choices\nFor this blog, I used the combinations function from the itertools package in order to go through each variation of both qualitative and quantitative features.\n\nfrom itertools import combinations\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Region\", ]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n\nI then utilized the LoigisticRegression model from sklearn in order to run tests for each iteration.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\n\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\nLR = LogisticRegression(max_iter=10000)\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n0.99609375\n\n\nGoing through each iteration, I scored each of them found below:\n[‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Culmen Length (mm)’, ‘Body Mass (g)’] : .91  [‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Culmen Depth (mm)’, ‘Flipper Length (mm)’]: .91  [‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Culmen Depth (mm)’, ‘Body Mass (g)’] : .76  [‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Flipper Length (mm)’, ‘Body Mass (g)’] : .63  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, ‘Culmen Depth (mm)’] : .992  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, ‘Flipper Length (mm)’] : .98  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, ‘Body Mass (g)’] : .98  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Depth (mm)’, ‘Flipper Length (mm)’] : .82  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Depth (mm)’, ‘Body Mass (g)’] : .77  [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Flipper Length (mm)’, ‘Body Mass (g)’] : .71  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Length (mm)’, ‘Culmen Depth (mm)’] : .996  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Length (mm)’, ‘Flipper Length (mm)’]: .97  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Length (mm)’, ‘Body Mass (g)’]: .97  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Depth (mm)’, ‘Flipper Length (mm)’]:.88  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Depth (mm)’, ‘Body Mass (g)’]: .83  [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Flipper Length (mm)’, ‘Body Mass (g)’] : .75"
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#testing",
    "href": "posts/classifying-penguin-post/index.html#testing",
    "title": "Classifying Palmer Penguins",
    "section": "Testing",
    "text": "Testing\nIn order to test the model, I am downloading the dataset and preparing it via the prepare_data function\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n1.0\n\n\nAwesome! Our model is able to classify the three species of penguins with 100 percent accuracy.\n\n\n\n\n\n\nFigure 3: Happy Penguins"
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#plotting-decision-regions",
    "href": "posts/classifying-penguin-post/index.html#plotting-decision-regions",
    "title": "Classifying Palmer Penguins",
    "section": "PLotting Decision Regions",
    "text": "PLotting Decision Regions\nI will be using the plot_regions function below in order to plot my decision regions for my model.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Gentoo\", \"Chinstrap\", \"Adelie\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nFor the purposes of this blog post, we will look at the decision regions for both our training data and our testing data\n\nplot_regions(LR, X_train[cols], y_train)\n\n\n\n\n\n\n\nFigure 4: Decision Regions for each island based on training data\n\n\n\n\n\nThe three plots above correspond to the decision regions for each island. As previously mentioned in the data visualization section, we see that for the Biscoe and Dream islands, there are only two species of penguins. For the Torgersen island, only the Adelie penguins reside there. As predicted, this results in comparing at most only two species of penguins against one another, allowing for easier classification.\nThe following section displays the decision regions our model makes for the testing data.\n\nplot_regions(LR, X_test[cols], y_test)\n\n\n\n\n\n\n\nFigure 5: Decision Regions for each island based on testing data\n\n\n\n\n\nOur testing data yielded a smaller number of penguins to be classified, but our classifier was able to accurately predict each species of penguin based on our three features of Island, Culmen Depth (mm), and Culmen Length (mm)\nWhile the penguins on the Torgersen would always be Adelie, and were classified as such. The closest decision region our model came to for classification of the test data was a penguin on the Dream Island. This was anticipated however as in Figure 2, Adelie and Chinstrap penguins share some overlapping points in regards to Culmen Depth (mm) and Culmen Length(mm)"
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#confusion-matrix",
    "href": "posts/classifying-penguin-post/index.html#confusion-matrix",
    "title": "Classifying Palmer Penguins",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nI will be using a confusion matrix for my model, evaluated on the test set.\nIn order to do this I will be using confusion_matrix from the sklearn package.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test[cols])\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nIn this confusion matrix, due to our 100 percent prediction accuracy, we do not see any errors within our matrix. As a reference, the confusion matrix just gives the number of data points that have the correct label.\nLogically, since our model predicts each penguin correctly, there will be disparities in the matrix.\nThat being said, as previously mentioned, the most likely error to occur within our model is classifying a penguin from the Dream island. This is due to the Adelie and Chinstrap penguins sharing the most similar distributions of Culmen Depth (mm) and Culmen Length (mm) which depending on the penguin’s dimensions, has the highest likelihood out of all the species to overlap in the incorrect species decision region."
  },
  {
    "objectID": "posts/classifying-penguin-post/index.html#discussion",
    "href": "posts/classifying-penguin-post/index.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Discussion",
    "text": "Discussion\nIn this blog post, I have found that the using Island, Culmen Depth (mm), and Culmen Length (mm) features in a logistic regression model are useful for accurately classifying species of penguins pertaining to Adelie, Gentoo, and Chinstrap as seen in Figure 5\nI have found that Adelie penguins reside in all three islands (Dream, Biscoe, Torgersen), while Gentoo penguins only reside on the Biscoe island and Chinstrap penguins only inhabit the Biscoe island.\nIn terms of Culmen Length (mm) and Culmen Depth (mm) we found that:\nGentoo penguins seem to exhbit a longer culmen length but shorter culmen depth.  Adelie penguins have a shorter culmen length compared to Gentoo penguins but a higher culmen depth.  Chinstrap penguins appear to have the longest average combination of culmen depth/length out of all the penguins.\nIn terms of Body Mass (g) we found that:\nGentoo penguins, both male and female, have the highest mean body mass out of all species of penguins.\nAdelie and Chinstrap penguins exhibit similar mean body mass to one another for both male and female."
  },
  {
    "objectID": "posts/warmup-exercise-penquin/Palmer-Penguins-Warmup.html",
    "href": "posts/warmup-exercise-penquin/Palmer-Penguins-Warmup.html",
    "title": "Donovan's Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/palmer-penguins.csv\"\ndf = pd.read_csv(url)\n\ndf\n\nsummary_table = df.groupby(['Species', 'Sex']).agg({'Body Mass (g)': 'mean'})\n\nprint(summary_table)\n\n# Create a scatterplot\nsns.scatterplot(data=df, x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", hue=\"Species\")\n\n                                                  Body Mass (g)\nSpecies                                   Sex                  \nAdelie Penguin (Pygoscelis adeliae)       FEMALE    3368.835616\n                                          MALE      4043.493151\nChinstrap penguin (Pygoscelis antarctica) FEMALE    3527.205882\n                                          MALE      3938.970588\nGentoo penguin (Pygoscelis papua)         .         4875.000000\n                                          FEMALE    4679.741379\n                                          MALE      5484.836066"
  },
  {
    "objectID": "posts/warmup-exercise-penquin/index.html",
    "href": "posts/warmup-exercise-penquin/index.html",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "",
    "text": "In this blog post, we explore the pivotal role of women in the fields of computing, math, and engineering, and the significant impact of conferences like WiDS (Women in Data Science) in fostering change. We delve into the historical context of women’s participation in computing, highlight the current challenges and barriers they face, and celebrate the achievements of notable women scholars who are pushing the boundaries of data science and its application in diverse fields. Through the insights from Professors Amy Yuen, Jessica L’Roe, Laura Biester, and Sarah Brown, we learn about the transformative power of data science in political science, geography, mental health, and the quest for fairness in machine learning. This post aims to illuminate the importance of diversity in STEM and inspire action towards a more inclusive future."
  },
  {
    "objectID": "posts/warmup-exercise-penquin/index.html#abstract",
    "href": "posts/warmup-exercise-penquin/index.html#abstract",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "",
    "text": "In this blog post, we explore the pivotal role of women in the fields of computing, math, and engineering, and the significant impact of conferences like WiDS (Women in Data Science) in fostering change. We delve into the historical context of women’s participation in computing, highlight the current challenges and barriers they face, and celebrate the achievements of notable women scholars who are pushing the boundaries of data science and its application in diverse fields. Through the insights from Professors Amy Yuen, Jessica L’Roe, Laura Biester, and Sarah Brown, we learn about the transformative power of data science in political science, geography, mental health, and the quest for fairness in machine learning. This post aims to illuminate the importance of diversity in STEM and inspire action towards a more inclusive future."
  },
  {
    "objectID": "posts/warmup-exercise-penquin/index.html#part-1-why-spotlight-women-in-data-science",
    "href": "posts/warmup-exercise-penquin/index.html#part-1-why-spotlight-women-in-data-science",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Part 1: Why Spotlight Women in Data Science?",
    "text": "Part 1: Why Spotlight Women in Data Science?"
  },
  {
    "objectID": "posts/warmup-exercise-penquin/index.html#professor-amy-yuen",
    "href": "posts/warmup-exercise-penquin/index.html#professor-amy-yuen",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Professor Amy Yuen",
    "text": "Professor Amy Yuen\nProfessor Amy Yuen is a Professor of Political Science at Middlebury College. Her research focused on the United Nations Security Council and aimed to answer if it is truly a democratic institution. Within the United Nations Security Council, elected members do not have veto power, Professor Yuen hoped to answer whether these members continue to run if this is the case, and if there is a lack of representation within the council. In her research, she measured the number of resolutions, formal meetings and consultations, presidential statements, resolution Co-sponsorships, and Aria-Formula Meetings. She found that sponsorships showed the biggest differences in who is doing what. Another aspect of her research was investigating the members that sit on the council, and the frequency of which. She found that the data showed that countries were geared more towards inclusion over time. She concluded by finding that while the council was not entirely representative, it could be worse. She plans to continue with this research to see how this finding plays into the output and effectiveness of the council. From her talk, I learned how data science can be applied to essentially any aspect of our lives. Before this talk, data science seemed strictly technical and I would have never thought about how it can be used in the context of politics and policy. The talk opened my eyes to the realm of possibilities that data science presents in so many varying fields."
  },
  {
    "objectID": "posts/warmup-exercise-penquin/index.html#professor-jessica-lroe",
    "href": "posts/warmup-exercise-penquin/index.html#professor-jessica-lroe",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Professor Jessica L’Roe",
    "text": "Professor Jessica L’Roe\nProfessor Jessica L’Roe is an assistant professor of Geography at Middlebury College. Professor Roe spoke about her research into monitoring change in tropical forest landscapes within East Africa. Within her research, she found that mothers were more willing to invest in education rather than land because it was a better bet for the success of their children. Although her research was an important crux of her talk, one of the main messages she wanted to convey was that all kinds of things can be “data”. Regardless of whether something is abstract or physical, it can often be used to analyze certain behaviors or outcomes. She concluded her talk by emphasizing how women are making large contributions in her field and to just “Go for it”! After listening to her talk, I learned the importance of varying types of data and utilization, but more importantly how it was collected. Her data was collected using the help of locals and traveling from settlement to settlement. It was so interesting to hear this type of data collection as before this, most of the data collection I was used to was simply clicking on a link or downloading a dataset. It showed me that data science can transcend the keyboard and has real implications for REAL people."
  },
  {
    "objectID": "posts/warmup-exercise-penquin/index.html#professor-laura-biester",
    "href": "posts/warmup-exercise-penquin/index.html#professor-laura-biester",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Professor Laura Biester",
    "text": "Professor Laura Biester\nProfessor Laura Biester is a Computer Science Professor at Middlebury College. Professor Biester spoke about her research regarding the connection between language and mental health. She used a corpus of Reddit posts and comments from 2006-2019 to predict whether or not language could be used to identify those who were diagnosed with depression before their diagnosis. Through multiple models such as logistic regression and MentalBeRT, she was able to successfully produce a model that was relatively accurate in predicting depression in users using only language. Like Professor L’Roe’s talk, although much of Professor Biester’s talk was focused on her research, a huge point she emphasized was that data collection is a huge part of the data science process - it’s not just about building models. This part struck true to me as I learned that no matter how good your model is, it is only as good as the data it is being supplied with. Data preprocessing and determining which parts of data to use or not use is crucial in the data science process."
  },
  {
    "objectID": "posts/warmup-exercise-penquin/index.html#professor-sarah-brown",
    "href": "posts/warmup-exercise-penquin/index.html#professor-sarah-brown",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Professor Sarah Brown",
    "text": "Professor Sarah Brown\nProfessor Sarah Brown is an assistant professor of Computer Science at the University of Rhode Island. In her talk, her research focused on how to make machine learning more fair. More broadly, however, she discussed the implications of data science through the use of “keys”. These keys were: Disciplines are communities, meet people where they are, and understand data’s context. She mentioned three projects and the “lock” for each, which is something she needed to figure out for it all to work and flow.\nThese keys were used to unlock these locks and culminate a successful data science process. The keys were picked up often from something that had no explicit relation to data science itself but were collected from other aspects of her life. For example, one of the skills she picked up from social studies was the ability to use context to understand primary sources. She mentions that oftentimes, data scientists fall into the trap of only thinking like a data scientist and approaching the problem from one particular angle. By using other disciplines/individuals’ perspectives, you may be able to figure out a problem in a more efficient/effective way than you would have otherwise. She concluded by stressing the importance of these three keys and emphasizing the idea of understanding “why” rather than just “doing” when it comes to data science. Through this, I learned key principles that will influence the future of my computer science journey."
  },
  {
    "objectID": "posts/warmup-exercise-penquin/index.html#conclusion",
    "href": "posts/warmup-exercise-penquin/index.html#conclusion",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Conclusion",
    "text": "Conclusion\nThis exploration into the status of women in computing and the supportive role of initiatives like WiDS has been enlightening. It has underscored the importance of diversity for innovation and the vast potential of data science to impact various aspects of society positively. From the historical context of women’s evolving role in computing to the cutting-edge research conducted by leading female data scientists, this journey has highlighted both the challenges and the incredible opportunities that lie ahead. As we look forward to a more inclusive and equitable future in STEM, the lessons learned here inspire continued advocacy and action. My next steps involve deepening my understanding of how to support diversity in tech and exploring more about how data science can be leveraged for social good."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#visualizations-of-the-data",
    "href": "posts/new-new-test-post/index.html#visualizations-of-the-data",
    "title": "Whose Costs?",
    "section": "",
    "text": "For the dataset, I created two plots and one summary table in order to contextualize the data.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\nplt.figure(figsize=(12, 8))\nsns.scatterplot(x='person_age', y='person_emp_length', hue='loan_intent', data=df, palette='viridis')\nplt.title('Loan Intent vs. Age and Employment Length')\nplt.xlabel('Age')\nplt.ylabel('Employment Length (Years)')\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Loan Intent\n\n\n\n\n\n\nplt.figure(figsize=(10, 6))\nsns.countplot(x='person_home_ownership', hue='loan_grade', data=df, palette='Set2')\nplt.title('Loan Grade Distribution by Home Ownership')\nplt.xlabel('Home Ownership')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\n\nFigure 2: Loan Grade Distribution by Home Ownership\n\n\n\n\n\n\nsummary_table = df.groupby('loan_intent').agg({\n    'loan_int_rate': 'mean',\n    'loan_amnt': 'mean'\n}).reset_index()\n\nsummary_table.rename(columns={\n    'loan_int_rate': 'Average Interest Rate',\n    'loan_amnt': 'Average Loan Amount'\n}, inplace=True)\n\nprint(summary_table)\n\n         loan_intent  Average Interest Rate  Average Loan Amount\n0  DEBTCONSOLIDATION              10.983305          9620.901149\n1          EDUCATION              10.965465          9460.015604\n2    HOMEIMPROVEMENT              11.160075         10348.725017\n3            MEDICAL              11.051946          9242.269907\n4           PERSONAL              11.009814          9549.427178\n5            VENTURE              10.940866          9516.417425"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#part-c-build-a-model",
    "href": "posts/new-new-test-post/index.html#part-c-build-a-model",
    "title": "Whose Costs?",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import Pipeline\n\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_all = pd.read_csv(url)\n\ndf = df_all.drop(['loan_grade', 'loan_status'], axis=1)  \ndf = pd.get_dummies(df, columns=['person_home_ownership', 'loan_intent', 'cb_person_default_on_file'], drop_first=True)\ndf.dropna(inplace=True)\n\ny = df_all.loc[df.index, 'loan_status']  \n\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=123)\n\n# Scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlogreg = LogisticRegression(max_iter=1000)\n\nselector = RFECV(estimator=logreg, step=1, cv=5)\nselector = selector.fit(X_train_scaled, y_train)  # Fitting RFECV to the scaled training data\n\n\nX_train_selected = selector.transform(X_train_scaled)\nX_test_selected = selector.transform(X_test_scaled)\n\nmodel_selected = LogisticRegression(max_iter=1000)\nmodel_selected.fit(X_train_selected, y_train)\n\nselected_features = X_train.columns[selector.support_]\n\n# Prediction\ny_pred_selected = model_selected.predict(X_test_selected)\nprint(f\"Optimal number of features: {selector.n_features_}\")\nprint(classification_report(y_test, y_pred_selected))\n\nOptimal number of features: 8\n              precision    recall  f1-score   support\n\n           0       0.87      0.94      0.90      3614\n           1       0.68      0.48      0.56       968\n\n    accuracy                           0.84      4582\n   macro avg       0.77      0.71      0.73      4582\nweighted avg       0.83      0.84      0.83      4582\n\n\n\n\nfeature_names = df.columns\n\nselected_features = feature_names[selector.support_]\n\nprint(\"Selected features:\", selected_features)\n\nSelected features: Index(['loan_amnt', 'loan_int_rate', 'loan_percent_income',\n       'person_home_ownership_OWN', 'person_home_ownership_RENT',\n       'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE'],\n      dtype='object')\n\n\n\n\nX = df[selected_features]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n## MODEL HERE\nmodel.fit(X_train_scaled, y_train)\n\nNameError: name 'model' is not defined"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#trying-to-do-threshold-stuff-here",
    "href": "posts/new-new-test-post/index.html#trying-to-do-threshold-stuff-here",
    "title": "Whose Costs?",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef linear_score(X, w):\n    return X @ w\n\n\n# Extracting weights \nw = model.coef_.flatten()  \n\n\n\nscores = linear_score(X_train_selected, w) \n\n\n\n\nplt.hist(scores, bins=30, edgecolor='k', alpha=0.7)\nplt.xlabel(r\"Score $s$\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of Computed Scores\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnum_thresholds = 101\nFPR = np.zeros(num_thresholds)\nTPR = np.zeros(num_thresholds)\n\nT = np.linspace(scores.min() - 0.1, scores.max() + 0.1, num_thresholds)\n\nfor i in range(num_thresholds):\n    t = T[i]\n    preds = scores &gt;= t\n    FPR[i] = ((preds == 1) & (y_train == 0)).sum() / (y_train == 0).sum()\n    TPR[i] = ((preds == 1) & (y_train == 1)).sum() / (y_train == 1).sum()\n\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 4))\n\nax.plot(FPR, TPR, color=\"black\")\nax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"grey\")  \nax.set_aspect('equal')\n\nax.set_xlabel(\"False Positive Rate\")\nax.set_ylabel(\"True Positive Rate\")\nax.set_title(\"ROC Curve\")\n\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\ndef profit_repaid(loan_amnt, loan_int_rate):\n    return loan_amnt * (1 + 0.25 * loan_int_rate)**10 - loan_amnt\n\ndef profit_default(loan_amnt, loan_int_rate):\n    return loan_amnt * (1 + 0.25 * loan_int_rate)**3 - 2 * loan_amnt\n\n\n\nprobabilities = model.predict_proba(X_train_scaled)[:, 1]\n\n\nexpected_gains = []\nT = np.linspace(0, 1, 101)  \n\nfor t in T:\n    preds = probabilities &gt;= t  # Prediction based on threshold: whether the loan would be issued\n\n    # Initialize gains and losses\n    total_gain = 0\n\n    # Loop over each loan\n    for amt, rate, pred, actual in zip(X_train['loan_amnt'], X_train['loan_int_rate'], preds, y_train):\n        if pred:  # Loan is predicted to be issued\n            if actual == 1:  # Loan is repaid\n                total_gain += profit_repaid(amt, rate)\n            else:  # Loan defaults\n                total_gain += loss_default(amt, rate)\n\n    # Average gain per loan\n    avg_gain = total_gain / len(X_train)\n    expected_gains.append(avg_gain)\n\n\n\n\nplt.figure(figsize=(10, 6))\nplt.plot(T, expected_gains, label='Expected Gain')\nplt.xlabel(r\"Threshold $t$\")\nplt.ylabel(\"Expected Profit per Loan\")\nplt.title(\"Expected Profit per Loan vs. Threshold\")\nplt.xlim(0, 1)\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\nselected_features = ['loan_amnt', 'loan_int_rate', 'loan_percent_income', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE']\n\n\nfeature_combinations = [\n    ['loan_amnt', 'loan_int_rate'],  \n    ['loan_percent_income', 'person_home_ownership_RENT'],  \n    selected_features  \n]\n\n\nresults = {}\n\nfor features in feature_combinations:\n    # Scale features\n    X_scaled = scaler.fit_transform(df[features])\n    \n    LR = LogisticRegression(max_iter=10000)\n    \n    cv_scores_LR = cross_val_score(LR, X_scaled, y, cv=5)\n    results[str(features)] = cv_scores_LR.mean()\n\nfor combo, score in results.items():\n    print(f\"Features: {combo}\\nCV Score (mean accuracy): {score}\\n\")\n\nFeatures: ['loan_amnt', 'loan_int_rate']\nCV Score (mean accuracy): 0.7973108709793388\n\nFeatures: ['loan_percent_income', 'person_home_ownership_RENT']\nCV Score (mean accuracy): 0.8471203482091736\n\nFeatures: ['loan_amnt', 'loan_int_rate', 'loan_percent_income', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE']\nCV Score (mean accuracy): 0.848342893535451"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#evaluate-different-thresholds",
    "href": "posts/new-new-test-post/index.html#evaluate-different-thresholds",
    "title": "Whose Costs?",
    "section": "Evaluate Different Thresholds",
    "text": "Evaluate Different Thresholds"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#part-c-build-a-model",
    "href": "posts/New-test-thing-post/index.html#part-c-build-a-model",
    "title": "Whose Costs?",
    "section": "",
    "text": "I will be utilizing a Logistic Regression Model for predicting whether a prospective borrower is likely to default on a given loan.\nI will also be utilizing the RFECV class from sklearn in order to automatically find the most useful features to use in the model. RFE is a feature selection method that fits a model and removes the weakest feature(s) until the specified number of features is reached. Using RFE, you can automate the process of finding an effective subset of features.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import Pipeline\n\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_all = pd.read_csv(url)\n\ndf = df_all.drop(['loan_grade', 'loan_status'], axis=1)  \ndf = pd.get_dummies(df, columns=['person_home_ownership', 'loan_intent', 'cb_person_default_on_file'], drop_first=True)\ndf.dropna(inplace=True)\n\ny = df_all.loc[df.index, 'loan_status']  \n\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=123)\n\n# Scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlogreg = LogisticRegression(max_iter=1000)\n\nselector = RFECV(estimator=logreg, step=1, cv=5)\nselector = selector.fit(X_train_scaled, y_train)  # Fitting RFECV to the scaled training data\n\n\nX_train_selected = selector.transform(X_train_scaled)\nX_test_selected = selector.transform(X_test_scaled)\n\nmodel_selected = LogisticRegression(max_iter=1000)\nmodel_selected.fit(X_train_selected, y_train)\n\nselected_features = X_train.columns[selector.support_]\n\n# Prediction\ny_pred_selected = model_selected.predict(X_test_selected)\nprint(f\"Optimal number of features: {selector.n_features_}\")\nprint(classification_report(y_test, y_pred_selected))\n\n\n  \nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=123)\n\n\nX_train\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_Y\n\n\n\n\n7573\n26\n38000\n1.0\n8875\n7.51\n0.23\n2\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n15865\n25\n110004\n9.0\n15000\n7.66\n0.14\n4\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n18729\n29\n60000\n3.0\n7200\n7.88\n0.12\n7\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n1288\n24\n44000\n1.0\n3200\n8.00\n0.07\n2\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n12168\n65\n46000\n5.0\n10500\n16.32\n0.23\n24\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17438\n26\n48000\n10.0\n15000\n11.12\n0.31\n3\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n24566\n45\n118000\n15.0\n25000\n12.18\n0.21\n12\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n20105\n21\n26000\n5.0\n14500\n14.61\n0.56\n2\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\n\n\n17825\n35\n78000\n9.0\n9450\n10.99\n0.12\n8\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n22678\n38\n65000\n5.0\n15000\n10.36\n0.23\n17\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n18325 rows × 16 columns\n\n\n\n\n\n# Scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\nX_train_scaled\n\narray([[-0.26848589, -0.43778172, -0.91860701, ..., -0.45114679,\n        -0.4630992 , -0.46344676],\n       [-0.4252438 ,  0.65831624,  1.01527471, ..., -0.45114679,\n        -0.4630992 , -0.46344676],\n       [ 0.20178785, -0.10288151, -0.43513658, ...,  2.21657344,\n        -0.4630992 , -0.46344676],\n       ...,\n       [-1.05227545, -0.62045457,  0.04833385, ..., -0.45114679,\n        -0.4630992 ,  2.15774517],\n       [ 1.14233533,  0.17112776,  1.01527471, ..., -0.45114679,\n        -0.4630992 , -0.46344676],\n       [ 1.61260907, -0.02676782,  0.04833385, ..., -0.45114679,\n        -0.4630992 , -0.46344676]])\n\n\n\n\nlogreg = LogisticRegression(max_iter=1000)\n\nselector = RFECV(estimator=logreg, step=1, cv=5)\nselector = selector.fit(X_train_scaled, y_train)  # Fitting RFECV to the scaled training data\n\n\nX_train_selected = selector.transform(X_train_scaled)\nX_test_selected = selector.transform(X_test_scaled)\n\n\nX_train_selected\n\narray([[-0.12277222, -1.0852037 ,  0.57659278, ..., -0.49598922,\n        -0.45114679, -0.4630992 ],\n       [ 0.85178574, -1.03888993, -0.27169328, ..., -0.49598922,\n        -0.45114679, -0.4630992 ],\n       [-0.38928398, -0.97096305, -0.4602013 , ..., -0.49598922,\n         2.21657344, -0.4630992 ],\n       ...,\n       [ 0.77222999,  1.10698175,  3.68697501, ..., -0.49598922,\n        -0.45114679, -0.4630992 ],\n       [-0.0312831 , -0.01072407, -0.4602013 , ..., -0.49598922,\n        -0.45114679, -0.4630992 ],\n       [ 0.85178574, -0.20524194,  0.57659278, ...,  2.01617286,\n        -0.45114679, -0.4630992 ]])\n\n\n\nfrom sklearn.metrics import classification_report\n\nmodel_selected = LogisticRegression(max_iter=1000)\nmodel_selected.fit(X_train_selected, y_train)\n\nselected_features = X_train.columns[selector.support_]\n\n# Prediction\ny_pred_selected = model_selected.predict(X_test_selected)\nprint(f\"Optimal number of features: {selector.n_features_}\")\nprint(classification_report(y_test, y_pred_selected))\n\nOptimal number of features: 8\n              precision    recall  f1-score   support\n\n           0       0.87      0.94      0.90      3614\n           1       0.68      0.48      0.56       968\n\n    accuracy                           0.84      4582\n   macro avg       0.77      0.71      0.73      4582\nweighted avg       0.83      0.84      0.83      4582\n\n\n\nThe code below is used to identify which features will be used for the most optimal model prediction.\n\nfeature_names = df.columns\n\nselected_features = feature_names[selector.support_]\n\nprint(\"Selected features:\", selected_features)\n\nSelected features: Index(['loan_amnt', 'loan_int_rate', 'loan_percent_income',\n       'person_home_ownership_OWN', 'person_home_ownership_RENT',\n       'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE'],\n      dtype='object')\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nthresholds = np.linspace(0, 1, 101)\nissued_counts = []\n\nfor threshold in thresholds:\n    X_train['predicted_issued'] = X_train['probabilities'] &gt;= threshold\n    issued_count = X_train['predicted_issued'].sum()  \n    issued_counts.append(issued_count)\n\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, issued_counts)\nplt.title('Number of Loans Predicted to be Issued vs. Threshold')\nplt.xlabel('Threshold')\nplt.ylabel('Number of Loans Predicted to be Issued')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nX_train\n\n\n\n\n\n\n\n\nloan_amnt\nloan_int_rate\nloan_percent_income\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_EDUCATION\nloan_intent_PERSONAL\nloan_intent_VENTURE\nprobabilities\nactual\nprediction_probability\npredicted_issued\ngain_loss\n\n\n\n\n7573\n8875\n7.51\n0.23\nFalse\nFalse\nFalse\nFalse\nFalse\n0.105087\n0\n0.105087\nFalse\n0\n\n\n15865\n15000\n7.66\n0.14\nFalse\nFalse\nFalse\nFalse\nFalse\n0.023507\n0\n0.023507\nFalse\n0\n\n\n18729\n7200\n7.88\n0.12\nFalse\nFalse\nFalse\nTrue\nFalse\n0.022012\n0\n0.022012\nFalse\n0\n\n\n1288\n3200\n8.00\n0.07\nFalse\nTrue\nFalse\nFalse\nFalse\n0.061475\n0\n0.061475\nFalse\n0\n\n\n12168\n10500\n16.32\n0.23\nFalse\nTrue\nFalse\nFalse\nTrue\n0.600637\n0\n0.600637\nFalse\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17438\n15000\n11.12\n0.31\nTrue\nFalse\nFalse\nFalse\nTrue\n0.047207\n0\n0.047207\nFalse\n0\n\n\n24566\n25000\n12.18\n0.21\nFalse\nFalse\nFalse\nFalse\nFalse\n0.097552\n0\n0.097552\nFalse\n0\n\n\n20105\n14500\n14.61\n0.56\nFalse\nFalse\nFalse\nFalse\nFalse\n0.978011\n1\n0.978011\nFalse\n0\n\n\n17825\n9450\n10.99\n0.12\nFalse\nFalse\nFalse\nFalse\nFalse\n0.083158\n0\n0.083158\nFalse\n0\n\n\n22678\n15000\n10.36\n0.23\nFalse\nFalse\nTrue\nFalse\nFalse\n0.078296\n0\n0.078296\nFalse\n0\n\n\n\n\n18325 rows × 13 columns\n\n\n\n\n# Estimating the profit here into rows \n\nthresholds = np.linspace(0, 1, 101)\naverage_gains = []\n\nfor threshold in thresholds:\n    X_train['predicted_issued'] = X_train['prediction_probability'] &gt;= threshold\n    X_train['gain_loss'] = X_train.apply(calculate_gain_loss, axis=1)\n    average_gain_loss_per_issued_loan = X_train.loc[X_train['predicted_issued'], 'gain_loss'].mean()\n    average_gains.append(average_gain_loss_per_issued_loan)\n\n# Plotting the average gain per issued loan across thresholds\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, average_gains, label='Average Gain Per Issued Loan')\nplt.xlabel('Threshold')\nplt.ylabel('Average Gain Per Issued Loan')\nplt.title('Average Gain Per Issued Loan vs. Threshold')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\nX_train\n\n\n\n\n\n\n\n\nloan_amnt\nloan_int_rate\nloan_percent_income\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_EDUCATION\nloan_intent_PERSONAL\nloan_intent_VENTURE\nprobabilities\nactual\nprediction_probability\npredicted_issued\ngain_loss\n\n\n\n\n7573\n8875\n7.51\n0.23\nFalse\nFalse\nFalse\nFalse\nFalse\n0.105087\n0\n0.105087\nFalse\n0\n\n\n15865\n15000\n7.66\n0.14\nFalse\nFalse\nFalse\nFalse\nFalse\n0.023507\n0\n0.023507\nFalse\n0\n\n\n18729\n7200\n7.88\n0.12\nFalse\nFalse\nFalse\nTrue\nFalse\n0.022012\n0\n0.022012\nFalse\n0\n\n\n1288\n3200\n8.00\n0.07\nFalse\nTrue\nFalse\nFalse\nFalse\n0.061475\n0\n0.061475\nFalse\n0\n\n\n12168\n10500\n16.32\n0.23\nFalse\nTrue\nFalse\nFalse\nTrue\n0.600637\n0\n0.600637\nFalse\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17438\n15000\n11.12\n0.31\nTrue\nFalse\nFalse\nFalse\nTrue\n0.047207\n0\n0.047207\nFalse\n0\n\n\n24566\n25000\n12.18\n0.21\nFalse\nFalse\nFalse\nFalse\nFalse\n0.097552\n0\n0.097552\nFalse\n0\n\n\n20105\n14500\n14.61\n0.56\nFalse\nFalse\nFalse\nFalse\nFalse\n0.978011\n1\n0.978011\nFalse\n0\n\n\n17825\n9450\n10.99\n0.12\nFalse\nFalse\nFalse\nFalse\nFalse\n0.083158\n0\n0.083158\nFalse\n0\n\n\n22678\n15000\n10.36\n0.23\nFalse\nFalse\nTrue\nFalse\nFalse\n0.078296\n0\n0.078296\nFalse\n0\n\n\n\n\n18325 rows × 13 columns"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#cross-validation",
    "href": "posts/New-test-thing-post/index.html#cross-validation",
    "title": "Whose Costs?",
    "section": "",
    "text": "I did not want to limit myself to only one model however, so I used cross_val_score from the sklearn.model_selection in order to compare how my ‘Optimal’ LR model did against other LR models with a different combination of features.\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\nselected_features = ['loan_amnt', 'loan_int_rate', 'loan_percent_income', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE']\n\n\nfeature_combinations = [\n    ['loan_amnt', 'loan_int_rate'],  \n    ['loan_percent_income', 'person_home_ownership_RENT'],  \n    selected_features  \n]\n\n\nresults = {}\n\nfor features in feature_combinations:\n    # Scale features\n    X_scaled = scaler.fit_transform(df[features])\n    \n    LR = LogisticRegression(max_iter=10000)\n    \n    cv_scores_LR = cross_val_score(LR, X_scaled, y, cv=5)\n    results[str(features)] = cv_scores_LR.mean()\n\nfor combo, score in results.items():\n    print(f\"Features: {combo}\\nCV Score (mean accuracy): {score}\\n\")\n\nFeatures: ['loan_amnt', 'loan_int_rate']\nCV Score (mean accuracy): 0.7973108709793388\n\nFeatures: ['loan_percent_income', 'person_home_ownership_RENT']\nCV Score (mean accuracy): 0.8471203482091736\n\nFeatures: ['loan_amnt', 'loan_int_rate', 'loan_percent_income', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'loan_intent_EDUCATION', 'loan_intent_PERSONAL', 'loan_intent_VENTURE']\nCV Score (mean accuracy): 0.848342893535451\n\n\n\nAs seen above, the ‘Optimal’ feature LR model performed the best in predicting the likelihood of a default."
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#part-d-finding-a-threshold",
    "href": "posts/New-test-thing-post/index.html#part-d-finding-a-threshold",
    "title": "Whose Costs?",
    "section": "",
    "text": "In this section, we will be exploring how I came up with the threshold t that will be used for our model.\nI used the numpy package to compute linear scores across all n of our training points.\nI plotted the scores below in order to easily visualize this process. By gathering scores, we can easily simulate decision-making with a given threshold.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef linear_score(X, w):\n    return X @ w\n\n\n# Extracting weights \nw = model.coef_.flatten()  \n\n\n\nscores = linear_score(X_train_selected, w) \n\n\nplt.hist(scores, bins=30, edgecolor='k', alpha=0.7)\nplt.xlabel(r\"Score $s$\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of Computed Scores\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure 3: Histogram of Computed Scores"
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#evaluate-different-thresholds",
    "href": "posts/New-test-thing-post/index.html#evaluate-different-thresholds",
    "title": "Whose Costs?",
    "section": "Evaluate Different Thresholds",
    "text": "Evaluate Different Thresholds\nAlthough the previous data from Figure 3 and ?@fig-ROC are useful, often times banks want to set a threshold that maximizes profit-loss.\nWe use a two assumptions below to account for the profit and loss of a bank if a loan is repaid in full and if a loan is defaulted on.\nWe then calculate the expected profit per issued loan for each threshold, and find the threshold that is the most optimal for generating overall profit. We can accomplish this by generating a gain_loss column within our X_train dataset in order to easily summarize the overall average gain/loss for each issued loan at each threshold value.\nThis relationship is illustrated in Figure 5.\n\nX = df[selected_features]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n## MODEL HERE\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train_scaled, y_train)\n\n# Predict probabilities\nprobabilities = model.predict_proba(X_train_scaled)[:, 1]\n\nX_train['actual'] = y_train\nX_train['prediction_probability'] = probabilities\n\n\n\ndef calculate_gain_loss(row):\n    if row['predicted_issued']:\n        if row['actual'] == 0:  # Loan was repaid\n            return row['loan_amnt'] * (1 + 0.25 * row['loan_int_rate'])**10 - row['loan_amnt']\n        elif row['actual'] == 1:  # Loan defaulted\n            return row['loan_amnt'] * (1 + 0.25 * row['loan_int_rate'])**3 - 1.7 * row['loan_amnt']\n    return 0\n\n\n# Estimating the profit here into rows \n\nthresholds = np.linspace(0, 1, 101)\naverage_gains = []\n\nfor threshold in thresholds:\n    X_train['predicted_issued'] = X_train['prediction_probability'] &gt;= threshold\n    X_train['gain_loss'] = X_train.apply(calculate_gain_loss, axis=1)\n    \n    # Check if any loans are predicted to be issued at this threshold\n    if X_train['predicted_issued'].sum() &gt; 0:\n        average_gain_loss_per_issued_loan = X_train.loc[X_train['predicted_issued'], 'gain_loss'].mean()\n    else:\n        # No loans are issued, so append a default value (e.g., 0) to avoid NaN\n        average_gain_loss_per_issued_loan = 0\n    \n    average_gains.append(average_gain_loss_per_issued_loan)\n\n\n# Plotting the average gain per issued loan across thresholds\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, average_gains, label='Average Gain Per Issued Loan')\nplt.xlabel('Threshold (Probability of Default)')\nplt.ylabel('Average Gain Per Issued Loan')\nplt.title('Average Gain Per Issued Loan vs. Threshold')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nFigure 5: Expected Profit per Loan vs Threshold\n\n\n\n\n\n\noptimal_index = np.argmax(average_gains)\noptimal_threshold = thresholds[optimal_index]\nprint(f\"Optimal Threshold: {optimal_threshold}\")\nprint(f\"Average Gain Per Issued Loan at Optimal Threshold: {average_gains[optimal_index]}\")\n\nOptimal Threshold: 0.25\nMaximum Average Gain Per Issued Loan at Optimal Threshold: 24992323662.99832\n\n\nWe use the above code to find the most optimal threshold which presents us with the highest level of expected profit.\nOur model finds that a threshold of 0.25 is the most optimal, with an expected average gain of $24,992,323,663 per borrower\nWhile yes, this number appears rather high, our simple assumptions assume compounded interest while real life situations may differ significantly"
  },
  {
    "objectID": "posts/new-test-post/goal-setting.html",
    "href": "posts/new-test-post/goal-setting.html",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "Donovan Wood\n\n\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nAs mentioned, I want to obviously grow in each of these areas, but in particular I hope to expand my knowledge in the Theory and Experimentation areas.\nI want to know why things work, not just “They work, and here is how to use them”. Throughout my college experience I oftentimes found myself just doing the motions and I really want to try to emphasize fully grasping the material.\nIn terms of relevance to my future, I want to join a quantitative firm in the future, in order to research further in the field and develop systems that work efficiently in the market. Being an Economics and Computer Science Major, my interest lies in both and looking forward, this path seems to be the most interesting to me. That being said, machine learning is a huge component of that job field and so I want to be adept in my learnings/experimentations in the course so that I can use some of what I learned in real-world applications.\n\n\n\n\n\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\nCurrently, I plan to complete as many blog posts as I can, not only for the obvious grade, but also in order to access my learning accordingly. By doing as many as I can, I can directly apply the most recent class teachings and hope to build on more fundamental skills.\nThat is my current goal, however I do realize I am taking five courses, three of which are computer science courses, so I acknowledge that there will be times in which I have too much on my plate in order to complete every one. Still, I hope to at least keep this mentality throughout the course of the semester and do the best job I can.\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\nPreparing for the warmups is a task that I believe I can achieve every week. Not only does it help with the understandings of the class material that day, but also reinforces the idea of completion due to societal and class pressure ha! All joking aside, I hope to continue to complete all the warmups throughout the semester and get help from my peers if I’m stuck on one.\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\nEver since I joined the class, I always wanted to develop something that directly applies to my future career aspirations.\nThis may seem a bit ambitious, but I want to develop an machine learning algorithm that can create a positive return rate trading on some good (stock, etf, futures, options) in the market consistently.\nObviously things that come to mind are data limitations (real-time data), effective implementation, and non bias results but I still want to try to achieve this goal.\nAs of now, we are only beginning week 3 of the class, so I am unsure as to if that is possible given the course teachings, but I am willing to learn additional things outside of the class in order to achieve this. Even if I am unsuccessful in developing a model that achieves all my goals, I believe that the learning experience and process will be useful for my future endeavors."
  },
  {
    "objectID": "posts/new-test-post/goal-setting.html#what-youll-learn",
    "href": "posts/new-test-post/goal-setting.html#what-youll-learn",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "The knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nAs mentioned, I want to obviously grow in each of these areas, but in particular I hope to expand my knowledge in the Theory and Experimentation areas.\nI want to know why things work, not just “They work, and here is how to use them”. Throughout my college experience I oftentimes found myself just doing the motions and I really want to try to emphasize fully grasping the material.\nIn terms of relevance to my future, I want to join a quantitative firm in the future, in order to research further in the field and develop systems that work efficiently in the market. Being an Economics and Computer Science Major, my interest lies in both and looking forward, this path seems to be the most interesting to me. That being said, machine learning is a huge component of that job field and so I want to be adept in my learnings/experimentations in the course so that I can use some of what I learned in real-world applications."
  },
  {
    "objectID": "posts/new-test-post/goal-setting.html#what-youll-achieve",
    "href": "posts/new-test-post/goal-setting.html#what-youll-achieve",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "Most blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\nCurrently, I plan to complete as many blog posts as I can, not only for the obvious grade, but also in order to access my learning accordingly. By doing as many as I can, I can directly apply the most recent class teachings and hope to build on more fundamental skills.\nThat is my current goal, however I do realize I am taking five courses, three of which are computer science courses, so I acknowledge that there will be times in which I have too much on my plate in order to complete every one. Still, I hope to at least keep this mentality throughout the course of the semester and do the best job I can.\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\nPreparing for the warmups is a task that I believe I can achieve every week. Not only does it help with the understandings of the class material that day, but also reinforces the idea of completion due to societal and class pressure ha! All joking aside, I hope to continue to complete all the warmups throughout the semester and get help from my peers if I’m stuck on one.\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\nEver since I joined the class, I always wanted to develop something that directly applies to my future career aspirations.\nThis may seem a bit ambitious, but I want to develop an machine learning algorithm that can create a positive return rate trading on some good (stock, etf, futures, options) in the market consistently.\nObviously things that come to mind are data limitations (real-time data), effective implementation, and non bias results but I still want to try to achieve this goal.\nAs of now, we are only beginning week 3 of the class, so I am unsure as to if that is possible given the course teachings, but I am willing to learn additional things outside of the class in order to achieve this. Even if I am unsuccessful in developing a model that achieves all my goals, I believe that the learning experience and process will be useful for my future endeavors."
  },
  {
    "objectID": "Lecture Notes/10-compas.html",
    "href": "Lecture Notes/10-compas.html",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Today we are going to study an extremely famous investigation into algorithmic decision-making in the sphere of criminal justice by @angwin2022machine, originally written for ProPublica in 2016. This investigation significantly accelerated the pace of research into bias and fairness in machine learning, due in combination to its simple message and publicly-available data.\nIt’s helpful to look at a sample form used for feature collection in the COMPAS risk assessment.\nYou may have already read about the COMPAS algorithm in the original article at ProPublica. Our goal today is to reproduce some of the main findings of this article and set the stage for a more systematic treatment of bias and fairness in machine learning.\nParts of these lecture notes are inspired by the original ProPublica analysis and Allen Downey’s expository case study on the same data.\n\n\n Let’s first obtain the data. I’ve hosted a copy on the course website, so we can download it using a URL.This data set was obtained by @angwin2022machine through a public records request. The data comprises two years worth of COMPAS scoring in Broward County, Florida.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nsns.set_style(\"whitegrid\")\nnp.set_printoptions(precision = 3)\npd.set_option('display.precision', 3)\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/compas/compas.csv\"\ncompas = pd.read_csv(url)\n\nFor today we are only going to consider a subset of columns.\n\ncols = [\"sex\", \"race\", \"decile_score\", \"two_year_recid\"]\ncompas = compas[cols]\n\nWe are also only going to consider white (Caucasian) and Black (African-American) defendants:\n\nis_white = compas[\"race\"] == \"Caucasian\"\nis_black = compas[\"race\"] == \"African-American\"\n\ncompas = compas[is_white | is_black]\ncompas = compas.copy()\n\nOur data now looks like this:\n\ncompas.head()\n\n\n\n\n\n\n\n\nsex\nrace\ndecile_score\ntwo_year_recid\n\n\n\n\n1\nMale\nAfrican-American\n3\n1\n\n\n2\nMale\nAfrican-American\n4\n1\n\n\n3\nMale\nAfrican-American\n8\n0\n\n\n6\nMale\nCaucasian\n6\n1\n\n\n8\nFemale\nCaucasian\n1\n0\n\n\n\n\n\n\n\n\n\n\nLet’s do some quick exploration of our data. How many defendants are present in this data of each sex?\n\ncompas.groupby(\"sex\").size()\n\nsex\nFemale    1219\nMale      4931\ndtype: int64\n\n\nWhat about race?\n\ncompas.groupby(\"race\").size()\n\nrace\nAfrican-American    3696\nCaucasian           2454\ndtype: int64\n\n\nThe decile score is the algorithm’s prediction. Higher decile scores indicate that, according to the COMPAS model, the defendant has higher likelihood to be charged with a crime within the next two years. In the framework we’ve developed in this class, you can think of the decile score as being produced by computing a score like \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\) for each defendant \\(i\\), and then dividing these into the lowest 10% (decile score 1), the next 10% (decile score 2), the next 10% (decile score 3) and so on.\nThe easiest way to see how this looks is with a bar chart, which we can make efficiently using the seaborn (sns) package.\n\ncounts = compas.groupby([\"race\", \"decile_score\"]).size().reset_index(name = \"n\")\np = sns.barplot(data = counts, \n                x = \"decile_score\", \n                y = \"n\", \n                hue = \"race\", \n                palette = \"BuPu\", \n                saturation = 0.5)\n\n\n\n\n\n\n\n\nYou may notice that the number of white defendants who receive a given decile score tends to decrease as the score increases, whereas the number of Black defendants remains relatively constant.\nLet’s also take a look at the recidivism rate in the data:\n\ncompas[\"two_year_recid\"].mean()\n\n0.4661788617886179\n\n\nSo, in these data, approximately 47% of all defendants went on to be charged of another crime within the next two years. This is sometimes called the prevalence of the outcome. Although this is not a “good” outcome, it is labeled 1 in the target data and so we refer to this as the “positive” outcome. Prevalence without further specification usually refers to prevalence of the positive outcome.\nThe base rate of prediction accuracy in this problem is 53%: if we always guessed that the defendant was not arrested within two years, we would be right 53% of the time.\nWe can also compute the prevalence broken down by race of the defendant:\n\ncompas.groupby(\"race\")[\"two_year_recid\"].mean()\n\nrace\nAfrican-American    0.514\nCaucasian           0.394\nName: two_year_recid, dtype: float64\n\n\n\n\nWhen interpreting these different prevalences, it is important to remember that\n\nRace is itself a socially-constructed system of human categorization invented by humans with political and economic motives to describe other humans as property [@bonilla-silvaRacismRacistsColorblind2018].\n\nThe relation between arrest and actual criminal offense can display racial bias, with effects varying by geography [@fogliatoValidityArrestProxy2021].\nDecisions about which behaviors are criminal are contingent political decisions which have, historically, fallen hardest on Black Americans [@yusefCriminalizingRaceRacializing2017].\n\nThe prevalences between the two groups are substantially different. This difference will have major consequences later on for the possibility of different kinds of fairness in classifiers.\nWe’re going to treat the COMPAS algorithm as a binary classifier, but you might notice a problem: the algorithm’s prediction is the decile_score column, which is not actually a 0-1 label. Following the analysis of @angwin2022machine, we are going to construct a new binary column in which we say that a defendant is predicted_high_risk if their decile_score is larger than 4.\n\ncompas[\"predicted_high_risk\"] = compas[\"decile_score\"] &gt; 4 \n\nNow that we’ve done that, we can ask: how likely are Black and white defendants to receive positive predictions in this data?\n\ncompas.groupby(\"race\")[[\"two_year_recid\", \"predicted_high_risk\"]].mean()\n\n\n\n\n\n\n\n\ntwo_year_recid\npredicted_high_risk\n\n\nrace\n\n\n\n\n\n\nAfrican-American\n0.514\n0.588\n\n\nCaucasian\n0.394\n0.348\n\n\n\n\n\n\n\nBlack defendants are substantially more likely to receive a positive prediction than white defendants, and the disparity is larger than the observed prevalence of the positive outcome.\n\n\n\n\n\n\nFairness (Part 1)\n\n\n\nIs this fair? What is your gut telling you? Yes, no, possibly? What information would you need in order to make a judgment? What is the principle on which your judgment rests?\n\n\n\n\n\nLet’s now ask a few questions about the the predictive accuracy of this algorithm. First, how accurate it is it overall?\n\ncompas[\"correct_prediction\"] = (compas[\"predicted_high_risk\"] == compas[\"two_year_recid\"])\ncompas[\"correct_prediction\"].mean()\n\n0.6508943089430894\n\n\nRecall that the base rate in this problem is 53%, so our accuracy is somewhat better than random guessing.\nWhat about the accuracy on Black and white defendants separately?\n\ncompas.groupby([\"race\"])[\"correct_prediction\"].mean()\n\nrace\nAfrican-American    0.638\nCaucasian           0.670\nName: correct_prediction, dtype: float64\n\n\nThe overall accuracies for Black and white defendants are comparable, and both are somewhat higher than the base rate of 53%.\nWhat about the error rates? Here is a simple calculation which computes the false positive rate (FPR) in the first row and the true positive rate (TPR) on the bottom row:\n\ncompas.groupby([\"two_year_recid\"])[\"predicted_high_risk\"].mean().reset_index()\n\n\n\n\n\n\n\n\ntwo_year_recid\npredicted_high_risk\n\n\n\n\n0\n0\n0.352\n\n\n1\n1\n0.654\n\n\n\n\n\n\n\nHowever, and this was the main finding of the ProPublica study, the FPR and FNR are very different when we break down the data by race:\n\ncompas.groupby([\"two_year_recid\", \"race\"])[\"predicted_high_risk\"].mean().reset_index()\n\n\n\n\n\n\n\n\ntwo_year_recid\nrace\npredicted_high_risk\n\n\n\n\n0\n0\nAfrican-American\n0.448\n\n\n1\n0\nCaucasian\n0.235\n\n\n2\n1\nAfrican-American\n0.720\n\n\n3\n1\nCaucasian\n0.523\n\n\n\n\n\n\n\nThe false positive rate for Black defendants is much higher than the false positive rate for white defendants. This was the main finding of @angwin2022machine. The FPR of 44% for Black defendants means that, out of every 100 Black defendants who in fact will not commit another crime, the algorithm nevertheless predicts that 44 of them will. In contrast, the FPR of 23% for white defendants indicates that only 23 out of 100 non-recidivating white defendants would be predicted to recidivate.\nThere are a few ways in which we can think of this result as reflecting bias:\n\nThe algorithm has learned an implicit pattern wherein Black defendants are intrinsically more “criminal” than white defendants, even among people who factually never committed another crime. This is a bias in the patterns that the algorithm has learned in order to formulate its predictions. This is related to the idea of representational bias, in which algorithms learn and reproduce toxic stereotypes about certain groups of people.\nRegardless of how the algorithm forms its predictions, the impact of the algorithm being used in the penal system is that more Black defendants will be classified as high-risk, resulting in more denials of parole, bail, early release, or other forms of freedom from the penal system. So, the algorithm has disparate impact on people. This is sometimes called allocative or distributional bias: bias in how resources or opportunities (in this case, freedom) are allocated or distributed between groups.\n\nSometimes predictive equality is also defined to require that the false negative rates (FNRs) be equal across the two groups as well.\nWe can think about the argument of @angwin2022machine as a two-step argument:\n\n\nThe COMPAS algorithm has disparate error rates by race.\nTherefore, the COMPAS algorithm is unjustly biased with respect to race.\n\n\nThis argument implicitly equates equality of error rates with lack of bias.\n\n\n\n\n\n\nFairness (Part 2)\n\n\n\n\nSuppose that we developed an alternative algorithm in which the false positive rates were equal, but there were still more positive predictions for Black defendants overall. Would that be enough to ensure fairness?\nSuppose that we developed an alternative prediction algorithm in which the rate of positive prediction was the same across racial groups, but the false positive rates were different. Would that be to ensure fairness?\n\n\n\n\n\n\n@angwin2022machine kicked off a vigorous discussion about what it means for an algorithm to fair and how to measure deviations from bias. In particular, Northpointe, the company that developed COMPAS, issued a report @flores2016false in which they argued that their algorithm was fair. Their argument is based on an idea of fairness which is sometimes called sufficiency @corbett-daviesAlgorithmicDecisionMaking2017.\nHere’s the intuition expressed by sufficiency. Imagine that you and your friend both received an A- in Data Structures. Suppose, however, that the instructor says different things to each of you:\n\nTo you, the instructor says: “You did fine in this class, but I don’t think that you are prepared to take Computer Architecture. I gave you a higher grade than I would normally because you wear cool hats in class.”\nTo your friend, the instructor says: “*You did fine in this class and I think you are prepared to take Computer Architecture. Some students got a bump in their grade because they are cool-hat-wearers, but you didn’t get that benefit.”\n\nFeels unfair, right? The instructor is saying that:\n\nWhat a grade means for you in terms of your future success depends on your identity group.\n\n\n\n\n\n\n\nNote\n\n\n\nSuppose that you heard this, but instead of cool hats it was because you are a member of an identity group that “needs some help” in order to achieve equitable representation in the CS major. How would you feel? Would that feel fair to you?\n\n\nWe’ll formally define sufficiency in a future lecture. For now, let’s use an informal definition:\n\nSufficiency means that a positive prediction means the same thing for future outcomes for each racial group.\n\nTo operationalize this idea, we are looking for the rate of re-arrest to be the same between (a) Black defendants who received a positive prediction and (b) white defendants who received a positive prediction.\nLet’s check this:\nThe rates of rearrest are relatively similar between groups when controlling for the predictions they collectively received. Formal statistical hypothesis tests are typically used to determine whether this difference is sufficiently “real” to warrant correction. In most of the published literature, scholars have considered that the two rates are sufficiently close that we should instead simply say that COMPAS appears to be relatively close to satisfying sufficiency.\nIndeed, in a rejoinder article published by affiliates of the company Northpointe which produced COMPAS, the fact that COMPAS satisfies sufficiency is one of the primary arguments [@flores2016false].\n\n\n\nIn these notes, we replicated the data analysis of @angwin2022machine, finding that the COMPAS algorithm has disparate error rates between Black and white defendants. We introduced the idea that fairness actually has several different facets in our moral intuitions, and found that the COMPAS algorithm satisfies one of them (sufficiency: equal scores mean the same thing regardless of your group membership) but not the others (equal prediction rates and equal error rates).\n\n\n\n\nCan we have it all? Could we modify the COMPAS algorithm in such a way that it satisfies all the ideas of fairness that we discussed above? Could we then call it “fair” or “unbiased?”\nAre there other ways to define fairness? Which ones are most compelling to us? Does the right idea of fairness depend on the context in which we apply it?\nHow did this happen? The COMPAS algorithm was never trained on race data about the defendant. How did it happen that this algorithm nevertheless made recommendations at different rates across groups?\nIs automated decision-making legitimate in this setting? Can it be legitimate (just, fair) to use an automated decision-system for making recommendations about parole and sentencing decisions at all? What safeguards and forms of recourse are necessary for the legitimate use of automated decision-making in criminal justice?\nWhat are the systemic impacts? Disparate sentencing decisions can have downstream impacts on communities and institutions. How could application of the COMPAS algorithm exacerbate systemic inequalities?"
  },
  {
    "objectID": "Lecture Notes/10-compas.html#data-preparation",
    "href": "Lecture Notes/10-compas.html#data-preparation",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Let’s first obtain the data. I’ve hosted a copy on the course website, so we can download it using a URL.This data set was obtained by @angwin2022machine through a public records request. The data comprises two years worth of COMPAS scoring in Broward County, Florida.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nsns.set_style(\"whitegrid\")\nnp.set_printoptions(precision = 3)\npd.set_option('display.precision', 3)\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/compas/compas.csv\"\ncompas = pd.read_csv(url)\n\nFor today we are only going to consider a subset of columns.\n\ncols = [\"sex\", \"race\", \"decile_score\", \"two_year_recid\"]\ncompas = compas[cols]\n\nWe are also only going to consider white (Caucasian) and Black (African-American) defendants:\n\nis_white = compas[\"race\"] == \"Caucasian\"\nis_black = compas[\"race\"] == \"African-American\"\n\ncompas = compas[is_white | is_black]\ncompas = compas.copy()\n\nOur data now looks like this:\n\ncompas.head()\n\n\n\n\n\n\n\n\nsex\nrace\ndecile_score\ntwo_year_recid\n\n\n\n\n1\nMale\nAfrican-American\n3\n1\n\n\n2\nMale\nAfrican-American\n4\n1\n\n\n3\nMale\nAfrican-American\n8\n0\n\n\n6\nMale\nCaucasian\n6\n1\n\n\n8\nFemale\nCaucasian\n1\n0"
  },
  {
    "objectID": "Lecture Notes/10-compas.html#preliminary-explorations",
    "href": "Lecture Notes/10-compas.html#preliminary-explorations",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Let’s do some quick exploration of our data. How many defendants are present in this data of each sex?\n\ncompas.groupby(\"sex\").size()\n\nsex\nFemale    1219\nMale      4931\ndtype: int64\n\n\nWhat about race?\n\ncompas.groupby(\"race\").size()\n\nrace\nAfrican-American    3696\nCaucasian           2454\ndtype: int64\n\n\nThe decile score is the algorithm’s prediction. Higher decile scores indicate that, according to the COMPAS model, the defendant has higher likelihood to be charged with a crime within the next two years. In the framework we’ve developed in this class, you can think of the decile score as being produced by computing a score like \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\) for each defendant \\(i\\), and then dividing these into the lowest 10% (decile score 1), the next 10% (decile score 2), the next 10% (decile score 3) and so on.\nThe easiest way to see how this looks is with a bar chart, which we can make efficiently using the seaborn (sns) package.\n\ncounts = compas.groupby([\"race\", \"decile_score\"]).size().reset_index(name = \"n\")\np = sns.barplot(data = counts, \n                x = \"decile_score\", \n                y = \"n\", \n                hue = \"race\", \n                palette = \"BuPu\", \n                saturation = 0.5)\n\n\n\n\n\n\n\n\nYou may notice that the number of white defendants who receive a given decile score tends to decrease as the score increases, whereas the number of Black defendants remains relatively constant.\nLet’s also take a look at the recidivism rate in the data:\n\ncompas[\"two_year_recid\"].mean()\n\n0.4661788617886179\n\n\nSo, in these data, approximately 47% of all defendants went on to be charged of another crime within the next two years. This is sometimes called the prevalence of the outcome. Although this is not a “good” outcome, it is labeled 1 in the target data and so we refer to this as the “positive” outcome. Prevalence without further specification usually refers to prevalence of the positive outcome.\nThe base rate of prediction accuracy in this problem is 53%: if we always guessed that the defendant was not arrested within two years, we would be right 53% of the time.\nWe can also compute the prevalence broken down by race of the defendant:\n\ncompas.groupby(\"race\")[\"two_year_recid\"].mean()\n\nrace\nAfrican-American    0.514\nCaucasian           0.394\nName: two_year_recid, dtype: float64\n\n\n\n\nWhen interpreting these different prevalences, it is important to remember that\n\nRace is itself a socially-constructed system of human categorization invented by humans with political and economic motives to describe other humans as property [@bonilla-silvaRacismRacistsColorblind2018].\n\nThe relation between arrest and actual criminal offense can display racial bias, with effects varying by geography [@fogliatoValidityArrestProxy2021].\nDecisions about which behaviors are criminal are contingent political decisions which have, historically, fallen hardest on Black Americans [@yusefCriminalizingRaceRacializing2017].\n\nThe prevalences between the two groups are substantially different. This difference will have major consequences later on for the possibility of different kinds of fairness in classifiers.\nWe’re going to treat the COMPAS algorithm as a binary classifier, but you might notice a problem: the algorithm’s prediction is the decile_score column, which is not actually a 0-1 label. Following the analysis of @angwin2022machine, we are going to construct a new binary column in which we say that a defendant is predicted_high_risk if their decile_score is larger than 4.\n\ncompas[\"predicted_high_risk\"] = compas[\"decile_score\"] &gt; 4 \n\nNow that we’ve done that, we can ask: how likely are Black and white defendants to receive positive predictions in this data?\n\ncompas.groupby(\"race\")[[\"two_year_recid\", \"predicted_high_risk\"]].mean()\n\n\n\n\n\n\n\n\ntwo_year_recid\npredicted_high_risk\n\n\nrace\n\n\n\n\n\n\nAfrican-American\n0.514\n0.588\n\n\nCaucasian\n0.394\n0.348\n\n\n\n\n\n\n\nBlack defendants are substantially more likely to receive a positive prediction than white defendants, and the disparity is larger than the observed prevalence of the positive outcome.\n\n\n\n\n\n\nFairness (Part 1)\n\n\n\nIs this fair? What is your gut telling you? Yes, no, possibly? What information would you need in order to make a judgment? What is the principle on which your judgment rests?"
  },
  {
    "objectID": "Lecture Notes/10-compas.html#the-propublica-findings",
    "href": "Lecture Notes/10-compas.html#the-propublica-findings",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Let’s now ask a few questions about the the predictive accuracy of this algorithm. First, how accurate it is it overall?\n\ncompas[\"correct_prediction\"] = (compas[\"predicted_high_risk\"] == compas[\"two_year_recid\"])\ncompas[\"correct_prediction\"].mean()\n\n0.6508943089430894\n\n\nRecall that the base rate in this problem is 53%, so our accuracy is somewhat better than random guessing.\nWhat about the accuracy on Black and white defendants separately?\n\ncompas.groupby([\"race\"])[\"correct_prediction\"].mean()\n\nrace\nAfrican-American    0.638\nCaucasian           0.670\nName: correct_prediction, dtype: float64\n\n\nThe overall accuracies for Black and white defendants are comparable, and both are somewhat higher than the base rate of 53%.\nWhat about the error rates? Here is a simple calculation which computes the false positive rate (FPR) in the first row and the true positive rate (TPR) on the bottom row:\n\ncompas.groupby([\"two_year_recid\"])[\"predicted_high_risk\"].mean().reset_index()\n\n\n\n\n\n\n\n\ntwo_year_recid\npredicted_high_risk\n\n\n\n\n0\n0\n0.352\n\n\n1\n1\n0.654\n\n\n\n\n\n\n\nHowever, and this was the main finding of the ProPublica study, the FPR and FNR are very different when we break down the data by race:\n\ncompas.groupby([\"two_year_recid\", \"race\"])[\"predicted_high_risk\"].mean().reset_index()\n\n\n\n\n\n\n\n\ntwo_year_recid\nrace\npredicted_high_risk\n\n\n\n\n0\n0\nAfrican-American\n0.448\n\n\n1\n0\nCaucasian\n0.235\n\n\n2\n1\nAfrican-American\n0.720\n\n\n3\n1\nCaucasian\n0.523\n\n\n\n\n\n\n\nThe false positive rate for Black defendants is much higher than the false positive rate for white defendants. This was the main finding of @angwin2022machine. The FPR of 44% for Black defendants means that, out of every 100 Black defendants who in fact will not commit another crime, the algorithm nevertheless predicts that 44 of them will. In contrast, the FPR of 23% for white defendants indicates that only 23 out of 100 non-recidivating white defendants would be predicted to recidivate.\nThere are a few ways in which we can think of this result as reflecting bias:\n\nThe algorithm has learned an implicit pattern wherein Black defendants are intrinsically more “criminal” than white defendants, even among people who factually never committed another crime. This is a bias in the patterns that the algorithm has learned in order to formulate its predictions. This is related to the idea of representational bias, in which algorithms learn and reproduce toxic stereotypes about certain groups of people.\nRegardless of how the algorithm forms its predictions, the impact of the algorithm being used in the penal system is that more Black defendants will be classified as high-risk, resulting in more denials of parole, bail, early release, or other forms of freedom from the penal system. So, the algorithm has disparate impact on people. This is sometimes called allocative or distributional bias: bias in how resources or opportunities (in this case, freedom) are allocated or distributed between groups.\n\nSometimes predictive equality is also defined to require that the false negative rates (FNRs) be equal across the two groups as well.\nWe can think about the argument of @angwin2022machine as a two-step argument:\n\n\nThe COMPAS algorithm has disparate error rates by race.\nTherefore, the COMPAS algorithm is unjustly biased with respect to race.\n\n\nThis argument implicitly equates equality of error rates with lack of bias.\n\n\n\n\n\n\nFairness (Part 2)\n\n\n\n\nSuppose that we developed an alternative algorithm in which the false positive rates were equal, but there were still more positive predictions for Black defendants overall. Would that be enough to ensure fairness?\nSuppose that we developed an alternative prediction algorithm in which the rate of positive prediction was the same across racial groups, but the false positive rates were different. Would that be to ensure fairness?"
  },
  {
    "objectID": "Lecture Notes/10-compas.html#the-rebuttal",
    "href": "Lecture Notes/10-compas.html#the-rebuttal",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "@angwin2022machine kicked off a vigorous discussion about what it means for an algorithm to fair and how to measure deviations from bias. In particular, Northpointe, the company that developed COMPAS, issued a report @flores2016false in which they argued that their algorithm was fair. Their argument is based on an idea of fairness which is sometimes called sufficiency @corbett-daviesAlgorithmicDecisionMaking2017.\nHere’s the intuition expressed by sufficiency. Imagine that you and your friend both received an A- in Data Structures. Suppose, however, that the instructor says different things to each of you:\n\nTo you, the instructor says: “You did fine in this class, but I don’t think that you are prepared to take Computer Architecture. I gave you a higher grade than I would normally because you wear cool hats in class.”\nTo your friend, the instructor says: “*You did fine in this class and I think you are prepared to take Computer Architecture. Some students got a bump in their grade because they are cool-hat-wearers, but you didn’t get that benefit.”\n\nFeels unfair, right? The instructor is saying that:\n\nWhat a grade means for you in terms of your future success depends on your identity group.\n\n\n\n\n\n\n\nNote\n\n\n\nSuppose that you heard this, but instead of cool hats it was because you are a member of an identity group that “needs some help” in order to achieve equitable representation in the CS major. How would you feel? Would that feel fair to you?\n\n\nWe’ll formally define sufficiency in a future lecture. For now, let’s use an informal definition:\n\nSufficiency means that a positive prediction means the same thing for future outcomes for each racial group.\n\nTo operationalize this idea, we are looking for the rate of re-arrest to be the same between (a) Black defendants who received a positive prediction and (b) white defendants who received a positive prediction.\nLet’s check this:\nThe rates of rearrest are relatively similar between groups when controlling for the predictions they collectively received. Formal statistical hypothesis tests are typically used to determine whether this difference is sufficiently “real” to warrant correction. In most of the published literature, scholars have considered that the two rates are sufficiently close that we should instead simply say that COMPAS appears to be relatively close to satisfying sufficiency.\nIndeed, in a rejoinder article published by affiliates of the company Northpointe which produced COMPAS, the fact that COMPAS satisfies sufficiency is one of the primary arguments [@flores2016false]."
  },
  {
    "objectID": "Lecture Notes/10-compas.html#recap",
    "href": "Lecture Notes/10-compas.html#recap",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "In these notes, we replicated the data analysis of @angwin2022machine, finding that the COMPAS algorithm has disparate error rates between Black and white defendants. We introduced the idea that fairness actually has several different facets in our moral intuitions, and found that the COMPAS algorithm satisfies one of them (sufficiency: equal scores mean the same thing regardless of your group membership) but not the others (equal prediction rates and equal error rates)."
  },
  {
    "objectID": "Lecture Notes/10-compas.html#some-questions-moving-forward",
    "href": "Lecture Notes/10-compas.html#some-questions-moving-forward",
    "title": "Introduction to Algorithmic Disparity: COMPAS",
    "section": "",
    "text": "Can we have it all? Could we modify the COMPAS algorithm in such a way that it satisfies all the ideas of fairness that we discussed above? Could we then call it “fair” or “unbiased?”\nAre there other ways to define fairness? Which ones are most compelling to us? Does the right idea of fairness depend on the context in which we apply it?\nHow did this happen? The COMPAS algorithm was never trained on race data about the defendant. How did it happen that this algorithm nevertheless made recommendations at different rates across groups?\nIs automated decision-making legitimate in this setting? Can it be legitimate (just, fair) to use an automated decision-system for making recommendations about parole and sentencing decisions at all? What safeguards and forms of recourse are necessary for the legitimate use of automated decision-making in criminal justice?\nWhat are the systemic impacts? Disparate sentencing decisions can have downstream impacts on communities and institutions. How could application of the COMPAS algorithm exacerbate systemic inequalities?"
  },
  {
    "objectID": "posts/WiDS-post/Palmer-Penguins-Warmup.html",
    "href": "posts/WiDS-post/Palmer-Penguins-Warmup.html",
    "title": "Donovan's Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/palmer-penguins.csv\"\ndf = pd.read_csv(url)\n\ndf\n\nsummary_table = df.groupby(['Species', 'Sex']).agg({'Body Mass (g)': 'mean'})\n\nprint(summary_table)\n\n# Create a scatterplot\nsns.scatterplot(data=df, x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", hue=\"Species\")\n\n                                                  Body Mass (g)\nSpecies                                   Sex                  \nAdelie Penguin (Pygoscelis adeliae)       FEMALE    3368.835616\n                                          MALE      4043.493151\nChinstrap penguin (Pygoscelis antarctica) FEMALE    3527.205882\n                                          MALE      3938.970588\nGentoo penguin (Pygoscelis papua)         .         4875.000000\n                                          FEMALE    4679.741379\n                                          MALE      5484.836066"
  },
  {
    "objectID": "posts/WiDS-post/index.html",
    "href": "posts/WiDS-post/index.html",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "",
    "text": "In this blog post, we explore the pivotal role of women in the fields of computing, math, and engineering, and the significant impact of conferences like WiDS (Women in Data Science) in fostering change. We delve into the historical context of women’s participation in computing, highlight the current challenges and barriers they face, and celebrate the achievements of notable women scholars who are pushing the boundaries of data science and its application in diverse fields. Through the insights from Professors Amy Yuen, Jessica L’Roe, Laura Biester, and Sarah Brown, we learn about the transformative power of data science in political science, geography, mental health, and the quest for fairness in machine learning. This post aims to illuminate the importance of diversity in STEM and inspire action towards a more inclusive future."
  },
  {
    "objectID": "posts/WiDS-post/index.html#abstract",
    "href": "posts/WiDS-post/index.html#abstract",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "",
    "text": "In this blog post, we explore the pivotal role of women in the fields of computing, math, and engineering, and the significant impact of conferences like WiDS (Women in Data Science) in fostering change. We delve into the historical context of women’s participation in computing, highlight the current challenges and barriers they face, and celebrate the achievements of notable women scholars who are pushing the boundaries of data science and its application in diverse fields. Through the insights from Professors Amy Yuen, Jessica L’Roe, Laura Biester, and Sarah Brown, we learn about the transformative power of data science in political science, geography, mental health, and the quest for fairness in machine learning. This post aims to illuminate the importance of diversity in STEM and inspire action towards a more inclusive future."
  },
  {
    "objectID": "posts/WiDS-post/index.html#part-1-why-spotlight-women-in-data-science",
    "href": "posts/WiDS-post/index.html#part-1-why-spotlight-women-in-data-science",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Part 1: Why Spotlight Women in Data Science?",
    "text": "Part 1: Why Spotlight Women in Data Science?"
  },
  {
    "objectID": "posts/WiDS-post/index.html#professor-amy-yuen",
    "href": "posts/WiDS-post/index.html#professor-amy-yuen",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Professor Amy Yuen",
    "text": "Professor Amy Yuen\nProfessor Amy Yuen is a Professor of Political Science at Middlebury College. Her research focused on the United Nations Security Council and aimed to answer if it is truly a democratic institution. Within the United Nations Security Council, elected members do not have veto power, Professor Yuen hoped to answer whether these members continue to run if this is the case, and if there is a lack of representation within the council. In her research, she measured the number of resolutions, formal meetings and consultations, presidential statements, resolution Co-sponsorships, and Aria-Formula Meetings. She found that sponsorships showed the biggest differences in who is doing what. Another aspect of her research was investigating the members that sit on the council, and the frequency of which. She found that the data showed that countries were geared more towards inclusion over time. She concluded by finding that while the council was not entirely representative, it could be worse. She plans to continue with this research to see how this finding plays into the output and effectiveness of the council. From her talk, I learned how data science can be applied to essentially any aspect of our lives. Before this talk, data science seemed strictly technical and I would have never thought about how it can be used in the context of politics and policy. The talk opened my eyes to the realm of possibilities that data science presents in so many varying fields."
  },
  {
    "objectID": "posts/WiDS-post/index.html#professor-jessica-lroe",
    "href": "posts/WiDS-post/index.html#professor-jessica-lroe",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Professor Jessica L’Roe",
    "text": "Professor Jessica L’Roe\nProfessor Jessica L’Roe is an assistant professor of Geography at Middlebury College. Professor Roe spoke about her research into monitoring change in tropical forest landscapes within East Africa. Within her research, she found that mothers were more willing to invest in education rather than land because it was a better bet for the success of their children. Although her research was an important crux of her talk, one of the main messages she wanted to convey was that all kinds of things can be “data”. Regardless of whether something is abstract or physical, it can often be used to analyze certain behaviors or outcomes. She concluded her talk by emphasizing how women are making large contributions in her field and to just “Go for it”! After listening to her talk, I learned the importance of varying types of data and utilization, but more importantly how it was collected. Her data was collected using the help of locals and traveling from settlement to settlement. It was so interesting to hear this type of data collection as before this, most of the data collection I was used to was simply clicking on a link or downloading a dataset. It showed me that data science can transcend the keyboard and has real implications for REAL people."
  },
  {
    "objectID": "posts/WiDS-post/index.html#professor-laura-biester",
    "href": "posts/WiDS-post/index.html#professor-laura-biester",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Professor Laura Biester",
    "text": "Professor Laura Biester\nProfessor Laura Biester is a Computer Science Professor at Middlebury College. Professor Biester spoke about her research regarding the connection between language and mental health. She used a corpus of Reddit posts and comments from 2006-2019 to predict whether or not language could be used to identify those who were diagnosed with depression before their diagnosis. Through multiple models such as logistic regression and MentalBeRT, she was able to successfully produce a model that was relatively accurate in predicting depression in users using only language. Like Professor L’Roe’s talk, although much of Professor Biester’s talk was focused on her research, a huge point she emphasized was that data collection is a huge part of the data science process - it’s not just about building models. This part struck true to me as I learned that no matter how good your model is, it is only as good as the data it is being supplied with. Data preprocessing and determining which parts of data to use or not use is crucial in the data science process."
  },
  {
    "objectID": "posts/WiDS-post/index.html#professor-sarah-brown",
    "href": "posts/WiDS-post/index.html#professor-sarah-brown",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Professor Sarah Brown",
    "text": "Professor Sarah Brown\nProfessor Sarah Brown is an assistant professor of Computer Science at the University of Rhode Island. In her talk, her research focused on how to make machine learning more fair. More broadly, however, she discussed the implications of data science through the use of “keys”. These keys were: Disciplines are communities, meet people where they are, and understand data’s context. She mentioned three projects and the “lock” for each, which is something she needed to figure out for it all to work and flow.\nThese keys were used to unlock these locks and culminate a successful data science process. The keys were picked up often from something that had no explicit relation to data science itself but were collected from other aspects of her life. For example, one of the skills she picked up from social studies was the ability to use context to understand primary sources. She mentions that oftentimes, data scientists fall into the trap of only thinking like a data scientist and approaching the problem from one particular angle. By using other disciplines/individuals’ perspectives, you may be able to figure out a problem in a more efficient/effective way than you would have otherwise. She concluded by stressing the importance of these three keys and emphasizing the idea of understanding “why” rather than just “doing” when it comes to data science. Through this, I learned key principles that will influence the future of my computer science journey."
  },
  {
    "objectID": "posts/WiDS-post/index.html#conclusion",
    "href": "posts/WiDS-post/index.html#conclusion",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Conclusion",
    "text": "Conclusion\nThis exploration into the status of women in computing and the supportive role of initiatives like WiDS has been enlightening. It has underscored the importance of diversity for innovation and the vast potential of data science to impact various aspects of society positively. From the historical context of women’s evolving role in computing to the cutting-edge research conducted by leading female data scientists, this journey has highlighted both the challenges and the incredible opportunities that lie ahead. As we look forward to a more inclusive and equitable future in STEM, the lessons learned here inspire continued advocacy and action. My next steps involve deepening my understanding of how to support diversity in tech and exploring more about how data science can be leveraged for social good."
  },
  {
    "objectID": "posts/health-blog-post/index.html",
    "href": "posts/health-blog-post/index.html",
    "title": "Replication Study, Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "",
    "text": "This blog post delves into the intricacies of algorithmic bias within healthcare management systems, inspired by Obermeyer et al. (2019), which highlighted racial biases in an algorithm used to manage health populations. By reconstructing key figures from the study and applying statistical models to assess cost disparities between Black and white patients, our analysis aims to quantify the disparity in healthcare costs and explore the nonlinear relationships between chronic conditions and healthcare expenditures. Our findings suggest that Black patients incur approximately 96.92% of the healthcare costs of white patients when adjusted for the number of chronic conditions, supporting the notion of underlying biases in healthcare algorithms that can perpetuate disparities in treatment and resource allocation."
  },
  {
    "objectID": "posts/health-blog-post/index.html#abstract",
    "href": "posts/health-blog-post/index.html#abstract",
    "title": "Replication Study, Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "",
    "text": "This blog post delves into the intricacies of algorithmic bias within healthcare management systems, inspired by Obermeyer et al. (2019), which highlighted racial biases in an algorithm used to manage health populations. By reconstructing key figures from the study and applying statistical models to assess cost disparities between Black and white patients, our analysis aims to quantify the disparity in healthcare costs and explore the nonlinear relationships between chronic conditions and healthcare expenditures. Our findings suggest that Black patients incur approximately 96.92% of the healthcare costs of white patients when adjusted for the number of chronic conditions, supporting the notion of underlying biases in healthcare algorithms that can perpetuate disparities in treatment and resource allocation."
  },
  {
    "objectID": "posts/health-blog-post/index.html#part-a-data-access",
    "href": "posts/health-blog-post/index.html#part-a-data-access",
    "title": "Replication Study, Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Part A: Data Access",
    "text": "Part A: Data Access\nFor this blog post, we will be using the file from labsysmed below in order to access the data.\n\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)"
  },
  {
    "objectID": "posts/health-blog-post/index.html#part-b-reproducing-fig.-1",
    "href": "posts/health-blog-post/index.html#part-b-reproducing-fig.-1",
    "title": "Replication Study, Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Part B: Reproducing Fig. 1",
    "text": "Part B: Reproducing Fig. 1\nIn this section, we will reproduce Fig. 1 from the paper, Obermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Science 366 (6464): 447–53. https://doi.org/10.1126/science.aax2342.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf['race'] = df['race'].str.lower()\n\nprint(\"Race counts:\", df['race'].value_counts())\n\ndf['gender'] = df['dem_female'].map({1: 'Female', 0: 'Male'})\n\nprint(\"Gender counts:\", df['gender'].value_counts())\n\ngenders = ['Male', 'Female']\nraces = ['black', 'white']\n\nmarkers = {'black': 'o', 'white': 'x'}\n\nRace counts: race\nwhite    43202\nblack     5582\nName: count, dtype: int64\nGender counts: gender\nFemale    30763\nMale      18021\nName: count, dtype: int64\n\n\nTo begin I provided a breakdown of the dataset by race and gender in order to give some context to the data.\n\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(12, 10), sharey=True)\n\nfor i, gender in enumerate(genders):\n    for race in races:\n        subset = df[(df['gender'] == gender) & (df['race'] == race)]\n        if subset.empty:\n            print(f\"No data for {gender} and {race}\")\n            continue\n\n        # Calculate percentiles of risk score\n        percentiles = np.linspace(0, 100, 101)  \n        percentile_scores = np.percentile(subset['risk_score_t'], percentiles)\n\n        # Calculate mean number of chronic conditions for each percentile\n        mean_conditions = [subset[subset['risk_score_t'] &lt;= score]['gagne_sum_t'].mean() for score in percentile_scores]\n        \n        # Plot\n        ax = axes[i]\n        ax.scatter(mean_conditions, percentiles, label=f'{race.capitalize()}')\n        \n        ax.set_title(f'{gender} Patients')\n        ax.set_xlabel('Mean Number of Active Chronic Conditions')\n        ax.set_ylabel('Percentile of Risk Score')\n        ax.legend(title='Race')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Figures of the mean number of active chronic conditions for each percentile of risk score, by gender and race\n\n\n\n\n\nAlgorithm scores are a key input to decisions about future enrollment in a care coordination program. So as we might expect, with less-healthy Blacks scored at similar risk scores to more-healthy Whites, we find evidence of substantial disparities in program screening.\nNote that for Fig 1 in the paper, the number of chronic conditions, ranged from 0 to 5, our plot here ranged from 0 to 2. While not exact, it still provides a similar representation as seen in the paper.\nSay for example that Patient A is Black, Patient B is White, and that both Patient A and Patient B have exactly the same chronic illnesses. Due to the disparity seen above, Patient A is less likely to be referred to the high risk care management program due to the current risk score screening practices."
  },
  {
    "objectID": "posts/health-blog-post/index.html#part-c-reproducing-fig-3.",
    "href": "posts/health-blog-post/index.html#part-c-reproducing-fig-3.",
    "title": "Replication Study, Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Part C: Reproducing Fig 3.",
    "text": "Part C: Reproducing Fig 3.\nIn this section, we will be focusing on reproducing Figure 3 from Obermeyer et al. (2019). Which focused on the relationship between medical expenditures and percentile of risk score and the relationship between medical expenditures and number of chronic illnesses.\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n\n# Plot 1: Mean Medical Expenditure by Percentile of Risk Score\nfor race in ['black', 'white']:\n    subset = df[df['race'] == race]\n    percentiles = np.linspace(0, 100, 101)\n    percentile_scores = np.percentile(subset['risk_score_t'], percentiles)\n    mean_costs = [subset[subset['risk_score_t'] &lt;= score]['cost_t'].mean() for score in percentile_scores]\n    axes[0].scatter(percentiles, mean_costs, marker=markers[race], label=f'{race.capitalize()}')\naxes[0].set_title('Mean Medical Expenditure by Percentile of Risk Score')\naxes[0].set_xlabel('Percentile of Risk Score')\naxes[0].set_ylabel('Mean Medical Expenditure')\naxes[0].legend()\n\n# Plot 2: Mean Medical Expenditure by Number of Chronic Illnesses\nfor race in ['black', 'white']:\n    subset = df[df['race'] == race]\n    unique_illnesses = sorted(subset['gagne_sum_t'].unique())\n    mean_costs = [subset[subset['gagne_sum_t'] == illness]['cost_t'].mean() for illness in unique_illnesses]\n    axes[1].scatter(unique_illnesses, mean_costs, marker=markers[race], label=f'{race.capitalize()}')\naxes[1].set_title('Mean Medical Expenditure by Number of Chronic Illnesses')\naxes[1].set_xlabel('Number of Chronic Illnesses')\naxes[1].set_ylabel('Mean Medical Expenditure')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 2: Scatterplots of mean medical expenditure by percentile of risk score and number of chronic illnesses\n\n\n\n\n\nFrom reproducing Figure 3, we find that there are small disparities in cost between races, but rather large disparities in health conditional on risk as seen in Figure 1\n\nPart D: Modeling Cost Disparity"
  },
  {
    "objectID": "posts/health-blog-post/index.html#data-prep",
    "href": "posts/health-blog-post/index.html#data-prep",
    "title": "Replication Study, Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Data Prep",
    "text": "Data Prep\nAs seen in Figure 2, there is a relatively stable pattern of disparity in the cost incurred by black and white patients with 5 or fewer chronic conditions, but this pattern begins to swing wildly in one direction or another as the number of active chronic conditions increases.\nDue to this, we will focus on patients with 5 or fewer active chronic conditions.\nWe will explore the percentage of patients in the dataset with 5 or fewer active chronic conditions, create a new column in the data set called log-transform which will act as our target variable for our model, create a dummy column for the qualitative race variable, and separate the data into predictor variables X and target variable y.\n\ndf['five_or_less'] = df['gagne_sum_t'] &lt;= 5\npercentage_five_or_less = df['five_or_less'].mean() * 100\nprint(f\"Percentage of patients with 5 or fewer chronic conditions: {percentage_five_or_less:.2f}%\")\n\nPercentage of patients with 5 or fewer chronic conditions: 95.54%\n\n\nAs seen with the percentage of patients with 5 or fewer active chronic conditions, the justification for focusing on this subset of the data is that the disparities in cost between black and white patients are relatively stable for this subset of the data and it accounts for a large portion of the data.\n\n# Log transform of the cost; first, remove patients with $0 medical costs\ndf = df[df['cost_t'] &gt; 0]\ndf['log_cost'] = np.log(df['cost_t'])\n\n# Create a dummy column for race where 0 = white and 1 = black\ndf['race_dummy'] = (df['race'] == 'black').astype(int)\n\n# Separate the data into predictor variables X and target variable y\nX = df[['race_dummy', 'gagne_sum_t']]\ny = df['log_cost']"
  },
  {
    "objectID": "posts/health-blog-post/index.html#modeling",
    "href": "posts/health-blog-post/index.html#modeling",
    "title": "Replication Study, Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Modeling",
    "text": "Modeling\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\n# Define a function to add polynomial features to the dataset\ndef add_polynomial_features(X, degree):\n    poly = PolynomialFeatures(degree)\n    X_poly = poly.fit_transform(X)\n    return X_poly, poly.get_feature_names(X.columns)\n\n# Evaluate models with different degrees of polynomial features using cross-validation\nscores = []\nmax_degree = 10\nfor degree in range(1, max_degree + 1):\n\n    pipeline = Pipeline([\n        ('poly', PolynomialFeatures(degree=degree)),\n        ('linear', LinearRegression())\n    ])\n    X_features = X.drop(columns='race_dummy')\n    X_poly = np.hstack([X[['race_dummy']].values, X_features])  # Adding the race dummy variable\n    score = cross_val_score(pipeline, X_poly, y, cv=5, scoring='neg_mean_squared_error')\n    scores.append(np.mean(score))\n\nIn the modeling part of the analysis, we used polynomial regression to explore and quantify disparities in healthcare costs between Black and white patients"
  },
  {
    "objectID": "posts/health-blog-post/index.html#discussion",
    "href": "posts/health-blog-post/index.html#discussion",
    "title": "Replication Study, Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Discussion",
    "text": "Discussion\nThe analysis conducted in this blog post reiterates the critical insights from Obermeyer et al. (2019), revealing subtle yet significant disparities in healthcare costs that suggest potential biases in the algorithms used for managing health populations. By employing polynomial regression models, we confirmed that the relationship between the number of chronic conditions and the associated healthcare costs is nonlinear and varies by race, reinforcing the need for models that adequately reflect the complexity of real-world data.\nThe crucial flaw was that the algorithm equated higher past costs with higher future needs without accounting for the fact that systemic inequalities often lead to underutilization of healthcare services by certain racial groups, particularly Black patients. As a result, despite having similar or even more severe chronic conditions, Black patients had been systematically assigned lower risk scores than white patients with comparable health conditions. This led to fewer healthcare resources being directed towards them, exacerbating existing health disparities.\nThe study by Obermeyer et al. is best described using the “conditional statistical parity” criterion discussed in Chapter 3 of Barocas, Hardt, and Narayanan (2023). This criterion pertains to ensuring that a decision-making process (like an algorithm) meets a fairness condition, specifically, that it is statistically independent of sensitive attributes (like race) conditioned on a legitimate set of variables (like health needs). Obermeyer et al. (2019) revealed that the algorithm’s reliance on healthcare costs as a proxy for health needs failed to account for the broader social and economic factors that influence these costs, leading to an underestimation of health needs for Black patients relative to white patients who incur similar healthcare costs."
  },
  {
    "objectID": "posts/mid-course-reflection/mid-course.html",
    "href": "posts/mid-course-reflection/mid-course.html",
    "title": "Mid-Course Reflection",
    "section": "",
    "text": "Donovan Wood\n\n\n\n\n\nHow often have you attended class? (e.g. “almost always,” “I missed three times,” etc.) Almost always, have only missed due to sickness.\nHow often have you taken notes on the core readings ahead of the class period? Again, almost always, except when I am sick\nHow often have you been prepared to present the daily warm-up exercise to your team, even if you weren’t actually called? 100% of the time so far.\nHow many times have you actually presented the daily warm-up to your team? Twice I believe.\nHow many times have you asked your team for help while presenting the daily warm-up? None, they weren’t ones I had questions on, I do, however, ask for my team’s input if anything is particularly interesting.\nHow often have you learned something new from a teammate’s presentation of the daily warm-up? Once or twice, our team usually does a good job in discussing all of the warm up when we are in our own groups.\nHow often have you helped a teammate during the daily warm-up presentation? Never actually, every time our teammates have had no trouble with the warmup.\n\n\n\n\n\nHow often have you attended Student Hours or Peer Help? Occasionally, they often conflict with my other classes or work.\nHow often have you asked for or received help from your fellow students? Hmm, not as often as I would like, but a couple times before class. Just to see if other folks had the same questions I had.\nHave you been regularly participating in a study group outside class? No, unfortunately. I wish I had a study group though!\nHow often have you posted questions or answers in Slack? Well not in the main questions tab, but I have been asking Professor Chondrow questions quite often recently via slack\n\n\n\n\n\nHow many blog posts have you submitted? 2\nHow many of your submitted blog posts are at each of the following feedback stages?\n\nE: No revisions suggested:\nM: Revisions useful: 1\nR: Revisions encouraged:\nN: Incomplete:\n\nRoughly how many hours per week have you spent on this course outside of class? Per week probably 15 lately, I’ve been working on a bunch of Blog Posts concurrently\n\n\n\n\n\nAt the beginning of the course, you may have expressed an interest in focusing a little extra on one or two of the following four categories:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nDid you choose to focus on any of these categories? If so, what have you done in order to pursue your interest?\nI would say mainly Theory and Implementation are the ones I have been focusing on lately. One of the main things I wanted to learn with this class was the implementation of ML in regards to the financial market. Through that interest, I have found a vast amount of information online with so many different communities and tips.\nI hope to use these in order to complete my final project and learn a lot through the process!\n\n\n\nFor each of the categories below, replace the “[your response here]” cell with 1-2 paragraphs in which you reflect on the following questions:\n\nIn what ways are you on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what you are doing in order to meet it.\nIn what ways are you not on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what gap you see between where you are and your goal.\nIf there’s any context you want to share about how you are faring relative to your goals, please do!\n\n\n\nAs of now, I’m currently working on three blog posts concurrently with one almost done and two nearing write up stage. On top of that I have two blog posts already submitted. All things considered, my goal of trying to complete every blog post was rather ambitious but I still want to keep it as a goal even if I don’t reach it in order to keep me motivated. One of the things I wish I did was work on my blog posts a bit over Spring Break but alas, the beach got to me unfortunately.\n\n\n\nBesides being sick a few days, I have went to class and completed the warm ups. While yes, theoretically I could have went back and did the warm ups for the classes I was sick for, I think this is a good benchmark of where I set out to be at the beginning of the semester.\nLooking forward, as the semester comes to a close, inevitably there will be deadlines and other projects that may take away from this, but I hope to not let that be the case and keep continuing onward.\n\n\n\nWell, since our project proposals were only due today, the stage of the project is still very early. That being said, I met with four individuals that seem to have a similar interest/idea to mine and I hope to work with them all in order to create something truly awesome!\nFrom talking with them, we all seem to recognize that the task will be hard and more unlike the work we have done currently in class but I believe that is part of the allure of the project. Hopefully by the end of the semester I can look back at our project and be proud of the model we created!\n\n\n\nIs there anything else that you want to share with me about what you have learned, how you have participated, or what you have achieved in CSCI 0451?\nI haven’t learned/had this much fun/struggled at times with a class in a pretty long time. I’m excited I was able to take it and look forward to developing my skills further!\n\n\n\nFrom your experience in CSCI 0451 and your other classes this semester, you may feel moved to make modifications to your goals. Are they still feasible? Too ambitious? Not ambitious enough? If you would like to revise any of your goals from your reflective goal-setting, you can do so below. For each goal you want to modify:\n\nClearly state what the goal was.\nClearly state how you’ve done on that goal so far.\nClearly state your proposed revised goal for the remainder of the course.\n\nI want to maintain goals that I had at the beginning of the semester. I feel as though it would be a disservice to my past self if I stopped pressing the go button. I want to learn as much as possible! I’ll keep trying to do so unless something drastic comes up.\n\n\n\n\nTake 15 minutes to look back on your responses in each of the sections above. Then, state the letter grade that you feel reflects your learning, participation, and achievement in CSCI 0451 so far, and contextualize it against some of the soundbytes below.\n\n\nAn A sounds like:\n\n“I am very proud of my time in this course.”\n“I have grown significantly in multiple ways that matter to me.”\n“I am ready to take the theory, techniques, and ideas of this course into my future classes, projects, hobbies, or career.”\n\nA B sounds like:\n\n“I had some opportunities to learn more, overall I feel good about my time in this course.”\n“I am able to explain some new things or achieve new tasks.”\n“I can see a few ideas from this course that will be relevant for my future classes, projects, hobbies, or career.”\n\nA C sounds like:\n\n“I often made a good effort, but I missed many opportunities to get more out of my time in this course.”\n“I might be able to complete some new tasks related to the course content, but only with significant further guidance.”\n“I don’t see any ways to take the contents of this course into my future classes, projects, hobbies, or career.”\n\nYou might find that some of these soundbytes resonate and other’s don’t! Take some time, see what feels right, and don’t be afraid to celebrate your achievements.\n\nUpon reflection, I feel that my learning, participation, and achievement in CSCI 0451 (so far) are best reflected by a grade of A-\n\n\nA way in which I resonate with the soundbytes for that grade above is…\n\nI truly am proud of my time spent so far in the course. It’s been a lot of time and effort but I’m proud of myself for not burning out and staying excited. I have learned a lot so far, but more importantly was actively seeking additional information that was interesting to me. I often find myself just ‘doing’ class so I’m excited that I actually WANT to do extra and learn. Lastly, I’m hoping this project will teach me a lot about this area of computing and be something I can look back on proudly!"
  },
  {
    "objectID": "posts/mid-course-reflection/mid-course.html#the-data",
    "href": "posts/mid-course-reflection/mid-course.html#the-data",
    "title": "Mid-Course Reflection",
    "section": "",
    "text": "How often have you attended class? (e.g. “almost always,” “I missed three times,” etc.) Almost always, have only missed due to sickness.\nHow often have you taken notes on the core readings ahead of the class period? Again, almost always, except when I am sick\nHow often have you been prepared to present the daily warm-up exercise to your team, even if you weren’t actually called? 100% of the time so far.\nHow many times have you actually presented the daily warm-up to your team? Twice I believe.\nHow many times have you asked your team for help while presenting the daily warm-up? None, they weren’t ones I had questions on, I do, however, ask for my team’s input if anything is particularly interesting.\nHow often have you learned something new from a teammate’s presentation of the daily warm-up? Once or twice, our team usually does a good job in discussing all of the warm up when we are in our own groups.\nHow often have you helped a teammate during the daily warm-up presentation? Never actually, every time our teammates have had no trouble with the warmup.\n\n\n\n\n\nHow often have you attended Student Hours or Peer Help? Occasionally, they often conflict with my other classes or work.\nHow often have you asked for or received help from your fellow students? Hmm, not as often as I would like, but a couple times before class. Just to see if other folks had the same questions I had.\nHave you been regularly participating in a study group outside class? No, unfortunately. I wish I had a study group though!\nHow often have you posted questions or answers in Slack? Well not in the main questions tab, but I have been asking Professor Chondrow questions quite often recently via slack\n\n\n\n\n\nHow many blog posts have you submitted? 2\nHow many of your submitted blog posts are at each of the following feedback stages?\n\nE: No revisions suggested:\nM: Revisions useful: 1\nR: Revisions encouraged:\nN: Incomplete:\n\nRoughly how many hours per week have you spent on this course outside of class? Per week probably 15 lately, I’ve been working on a bunch of Blog Posts concurrently"
  },
  {
    "objectID": "posts/mid-course-reflection/mid-course.html#what-youve-learned",
    "href": "posts/mid-course-reflection/mid-course.html#what-youve-learned",
    "title": "Mid-Course Reflection",
    "section": "",
    "text": "At the beginning of the course, you may have expressed an interest in focusing a little extra on one or two of the following four categories:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nDid you choose to focus on any of these categories? If so, what have you done in order to pursue your interest?\nI would say mainly Theory and Implementation are the ones I have been focusing on lately. One of the main things I wanted to learn with this class was the implementation of ML in regards to the financial market. Through that interest, I have found a vast amount of information online with so many different communities and tips.\nI hope to use these in order to complete my final project and learn a lot through the process!"
  },
  {
    "objectID": "posts/mid-course-reflection/mid-course.html#reflecting-on-goals",
    "href": "posts/mid-course-reflection/mid-course.html#reflecting-on-goals",
    "title": "Mid-Course Reflection",
    "section": "",
    "text": "For each of the categories below, replace the “[your response here]” cell with 1-2 paragraphs in which you reflect on the following questions:\n\nIn what ways are you on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what you are doing in order to meet it.\nIn what ways are you not on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what gap you see between where you are and your goal.\nIf there’s any context you want to share about how you are faring relative to your goals, please do!\n\n\n\nAs of now, I’m currently working on three blog posts concurrently with one almost done and two nearing write up stage. On top of that I have two blog posts already submitted. All things considered, my goal of trying to complete every blog post was rather ambitious but I still want to keep it as a goal even if I don’t reach it in order to keep me motivated. One of the things I wish I did was work on my blog posts a bit over Spring Break but alas, the beach got to me unfortunately.\n\n\n\nBesides being sick a few days, I have went to class and completed the warm ups. While yes, theoretically I could have went back and did the warm ups for the classes I was sick for, I think this is a good benchmark of where I set out to be at the beginning of the semester.\nLooking forward, as the semester comes to a close, inevitably there will be deadlines and other projects that may take away from this, but I hope to not let that be the case and keep continuing onward.\n\n\n\nWell, since our project proposals were only due today, the stage of the project is still very early. That being said, I met with four individuals that seem to have a similar interest/idea to mine and I hope to work with them all in order to create something truly awesome!\nFrom talking with them, we all seem to recognize that the task will be hard and more unlike the work we have done currently in class but I believe that is part of the allure of the project. Hopefully by the end of the semester I can look back at our project and be proud of the model we created!\n\n\n\nIs there anything else that you want to share with me about what you have learned, how you have participated, or what you have achieved in CSCI 0451?\nI haven’t learned/had this much fun/struggled at times with a class in a pretty long time. I’m excited I was able to take it and look forward to developing my skills further!\n\n\n\nFrom your experience in CSCI 0451 and your other classes this semester, you may feel moved to make modifications to your goals. Are they still feasible? Too ambitious? Not ambitious enough? If you would like to revise any of your goals from your reflective goal-setting, you can do so below. For each goal you want to modify:\n\nClearly state what the goal was.\nClearly state how you’ve done on that goal so far.\nClearly state your proposed revised goal for the remainder of the course.\n\nI want to maintain goals that I had at the beginning of the semester. I feel as though it would be a disservice to my past self if I stopped pressing the go button. I want to learn as much as possible! I’ll keep trying to do so unless something drastic comes up."
  },
  {
    "objectID": "posts/mid-course-reflection/mid-course.html#grade-and-goals",
    "href": "posts/mid-course-reflection/mid-course.html#grade-and-goals",
    "title": "Mid-Course Reflection",
    "section": "",
    "text": "Take 15 minutes to look back on your responses in each of the sections above. Then, state the letter grade that you feel reflects your learning, participation, and achievement in CSCI 0451 so far, and contextualize it against some of the soundbytes below.\n\n\nAn A sounds like:\n\n“I am very proud of my time in this course.”\n“I have grown significantly in multiple ways that matter to me.”\n“I am ready to take the theory, techniques, and ideas of this course into my future classes, projects, hobbies, or career.”\n\nA B sounds like:\n\n“I had some opportunities to learn more, overall I feel good about my time in this course.”\n“I am able to explain some new things or achieve new tasks.”\n“I can see a few ideas from this course that will be relevant for my future classes, projects, hobbies, or career.”\n\nA C sounds like:\n\n“I often made a good effort, but I missed many opportunities to get more out of my time in this course.”\n“I might be able to complete some new tasks related to the course content, but only with significant further guidance.”\n“I don’t see any ways to take the contents of this course into my future classes, projects, hobbies, or career.”\n\nYou might find that some of these soundbytes resonate and other’s don’t! Take some time, see what feels right, and don’t be afraid to celebrate your achievements.\n\nUpon reflection, I feel that my learning, participation, and achievement in CSCI 0451 (so far) are best reflected by a grade of A-\n\n\nA way in which I resonate with the soundbytes for that grade above is…\n\nI truly am proud of my time spent so far in the course. It’s been a lot of time and effort but I’m proud of myself for not burning out and staying excited. I have learned a lot so far, but more importantly was actively seeking additional information that was interesting to me. I often find myself just ‘doing’ class so I’m excited that I actually WANT to do extra and learn. Lastly, I’m hoping this project will teach me a lot about this area of computing and be something I can look back on proudly!"
  },
  {
    "objectID": "posts/bias-post/index.html",
    "href": "posts/bias-post/index.html",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "",
    "text": "This essay critically examines Arvind Narayanan’s assertion that quantitative methods used to assess bias and discrimination “primarily justify the status quo and do more harm than good.” By thoroughly exploring Narayanan’s arguments alongside related scholarly works and additional research, this analysis investigates the complex nature of quantitative methods in both perpetuating and addressing systemic biases."
  },
  {
    "objectID": "posts/bias-post/index.html#abstract",
    "href": "posts/bias-post/index.html#abstract",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "",
    "text": "This essay critically examines Arvind Narayanan’s assertion that quantitative methods used to assess bias and discrimination “primarily justify the status quo and do more harm than good.” By thoroughly exploring Narayanan’s arguments alongside related scholarly works and additional research, this analysis investigates the complex nature of quantitative methods in both perpetuating and addressing systemic biases."
  },
  {
    "objectID": "posts/New-test-thing-post/index.html#part-e-evaluating-the-model-from-the-banks-perspective",
    "href": "posts/New-test-thing-post/index.html#part-e-evaluating-the-model-from-the-banks-perspective",
    "title": "Whose Costs?",
    "section": "Part E: Evaluating the Model from the Bank’s Perspective",
    "text": "Part E: Evaluating the Model from the Bank’s Perspective\nAfter finding an optimal threshold value of 0.25, let’s see how our model compares to the test set."
  },
  {
    "objectID": "Lecture Notes/stochastic-gradient.html",
    "href": "Lecture Notes/stochastic-gradient.html",
    "title": "Donovan's Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import torch\nfrom matplotlib import pyplot as plt \nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef regression_data(n = 100, w = torch.Tensor([-0.7, 0.5]), x_max = 1):\n\n    x = torch.rand(n)*x_max\n    y = x*w[1] + w[0] + 0.05*torch.randn(n)\n    return x, y\n\nx, y = regression_data()\n\nplt.scatter(x, y, facecolors = \"none\", edgecolors = \"steelblue\")\nlabs = plt.gca().set(xlabel = r\"$x$\", ylabel = r\"$y$\")\n\n\n\n\n\n\n\n\n\n# Empirical risk calculation function\ndef empirical_risk(w0, w1, x, y):\n    return ((y - w1 * x - w0)**2).mean()\n\n# Weight update function for SGD\ndef update_weights(w0, w1, x, y, alpha, t, idx):\n    i = idx\n    err = y[i] - w1 * x[i] - w0\n    w0 -= (2 * alpha / t) * w0 * err\n    w1 -= (2 * alpha / t) * w1 * err * x[i]\n    return w0, w1\n\n# Stochastic Gradient Descent function for regression\ndef sgd_regression(x, y, alpha, t_max, plot_single_epoch=False):\n    w0, w1 = torch.randn(1), torch.randn(1)  # Initial random weights\n    risk_over_time = []\n\n    for t in range(1, t_max + 1):\n        perm = torch.randperm(x.size(0))\n        risks = []\n\n        for i in perm:\n            w0, w1 = update_weights(w0, w1, x, y, alpha, t, i)\n            if plot_single_epoch and t == 1:\n                current_risk = empirical_risk(w0, w1, x, y)\n                risks.append(current_risk)\n\n        end_epoch_risk = empirical_risk(w0, w1, x, y)\n        risk_over_time.append(end_epoch_risk)\n\n        if plot_single_epoch and t == 1:\n            plt.figure(figsize=(10, 4))\n            plt.plot(risks, label='Empirical Risk During First Epoch')\n            plt.xlabel('Iteration')\n            plt.ylabel('Empirical Risk')\n            plt.title('Empirical Risk Evolution During the First Epoch')\n            plt.legend()\n            plt.show()\n\n    return w0, w1, risk_over_time\n\n# Parameters for SGD\nalpha = 0.01\nt_max = 100\n\n# Generate data\nx, y = regression_data()\n\n# Run the SGD regression\nfinal_w0, final_w1, risks = sgd_regression(x, y, alpha, t_max, plot_single_epoch=True)\n\n# Plot the empirical risk over 100 epochs\nplt.figure(figsize=(10, 4))\nplt.plot(risks, label='Empirical Risk at the End of Each Epoch')\nplt.xlabel('Epoch')\nplt.ylabel('Empirical Risk')\nplt.title('Empirical Risk Evolution Over 100 Epochs')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/perceptron-post/index.html",
    "href": "posts/perceptron-post/index.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "In this blog post, I implement the Perceptron algorithm from scratch in Python. The Perceptron algorithm is a simple algorithm for learning a binary classifier. It is the simplest type of artificial neural network. It is a model of a single neuron that can be used for two-class classification problems. The Perceptron algorithm is a linear classifier, which means it can only be used for linearly separable data. We find that it does not work well on non-linearly separable data. We implement a Mini-batch Perceptron algorithm to improve the convergence speed of the Perceptron algorithm and to better deal with non-linearly separable data. We perform various experiments to evaluate the performance of the Perceptron algorithm and the Mini-batch Perceptron algorithm on synthetic datasets and real-world datasets. We find that the Mini-batch Perceptron algorithm converges faster than the Perceptron algorithm and is more robust to non-linearly separable data."
  },
  {
    "objectID": "posts/perceptron-post/index.html#abstract",
    "href": "posts/perceptron-post/index.html#abstract",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "In this blog post, I implement the Perceptron algorithm from scratch in Python. The Perceptron algorithm is a simple algorithm for learning a binary classifier. It is the simplest type of artificial neural network. It is a model of a single neuron that can be used for two-class classification problems. The Perceptron algorithm is a linear classifier, which means it can only be used for linearly separable data. We find that it does not work well on non-linearly separable data. We implement a Mini-batch Perceptron algorithm to improve the convergence speed of the Perceptron algorithm and to better deal with non-linearly separable data. We perform various experiments to evaluate the performance of the Perceptron algorithm and the Mini-batch Perceptron algorithm on synthetic datasets and real-world datasets. We find that the Mini-batch Perceptron algorithm converges faster than the Perceptron algorithm and is more robust to non-linearly separable data."
  },
  {
    "objectID": "posts/perceptron-post/index.html#part-a-implement-perceptron",
    "href": "posts/perceptron-post/index.html#part-a-implement-perceptron",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Part A: Implement Perceptron",
    "text": "Part A: Implement Perceptron\nIn this section of the blog post, I will be implementing the Perceptron algorithm from scratch. The Perceptron algorithm is a type of linear classifier, which is used to classify data points into one of two classes. The algorithm works by finding a hyperplane that separates the data points into two classes. The hyperplane is defined by a set of weights and a bias term. The weights are used to determine the orientation of the hyperplane, while the bias term is used to determine the position of the hyperplane in the feature space.\nThe Perceptron algorithm works by iteratively updating the weights and bias term in order to minimize the classification error. The algorithm starts with an initial set of weights and bias term, and then iterates over the training data points. For each data point, the algorithm computes the predicted class label based on the current weights and bias term, and then updates the weights and bias term based on the error between the predicted class label and the true class label.\nThe Perceptron algorithm is a simple and efficient algorithm for binary classification tasks. It is a type of online learning algorithm, which means that it updates the weights and bias term based on each data point in the training set. The algorithm is guaranteed to converge if the data is linearly separable, and it can be extended to handle non-linearly separable data by using a kernel function.\nIn the following sections, I will implement the Perceptron algorithm in Python and test it on a synthetic dataset. I will also visualize the decision boundary learned by the Perceptron algorithm to see how well it separates the data points into two classes.\nLet’s get started by implementing the Perceptron algorithm in Python.\nThis is done in perceptron.py file. I will not cover the entirety of the coding file, but will touch on the perceptron.grad() portion of the code:\n\n# grad() function from perceptron.py\n\ndef grad(self, X, y):\n       # Computes the vector to add to the weights to minimize the loss\n       s = X@self.w\n       return (s*y &lt; 0)*X*y\n\nIn the first line of grad() function, we calculate the inner product:\n\\(s_i = \\langle \\mathbf{w}^{(t)}, \\mathbf{x}_i \\rangle\\)\nX and self.w are used to compute this inner product with the @ operator from torch\nIn the final line of the grad() function we use the result from the first line to calculate:\n\\([s_i y_i &lt; 0] y_i \\mathbf{x}_i\\)\nWith that said, let’s begin testing our implementation\nIn order to test the implementation, I will run the “minimal training loop”. We need some data to first test this on, so below we will implement some testing data.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\nimport torch\nfrom matplotlib import pyplot as plt\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\n# define function to plot data\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n# set seed\ntorch.manual_seed(1234)\n\n# create linearly separable data\nX_ls, y_ls = perceptron_data(n_points = 50, noise = 0.3)\n\n# plot linearly separable data\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X_ls, y_ls, ax)\nax.set_title(\"Our Linearly Separable Data\");\n\n\n\n\n\n\n\n\nWith our data generated, we can now test the Perceptron algorithm by running the minimal training loop. From what we have seen so far, there is a clear separation between the two classes, so the Perceptron algorithm should be able to learn a decision boundary that separates the data points into two classes. Let’s run the minimal training loop and visualize the decision boundary learned by the Perceptron algorithm.\n\n# set seed\ntorch.manual_seed(1234567)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X_ls, y_ls) \n    loss_vec.append(loss)\n    \n    # perform a perceptron update using the random data point\n    opt.step(X_ls, y_ls)\n\nWe can track the progress of our training by checking the values of the loss function over time:\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\nFigure 1: Loss as a function of the number of perceptron updates\n\n\n\n\n\nFrom this, we can see that training completed with the achievement of zero loss; that is, perfect training accuracy. With this in mind, we can now move on to the next section of the blog post, where we will implement the Perceptron algorithm further."
  },
  {
    "objectID": "posts/perceptron-post/index.html#part-b-experiments",
    "href": "posts/perceptron-post/index.html#part-b-experiments",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Part B: Experiments",
    "text": "Part B: Experiments\nWith a functional implementation of the perceptron algorithm, we can now move on to the next section of the blog post, where we will experiment with the algorithm."
  },
  {
    "objectID": "posts/perceptron-post/index.html#part-c-minibatch-perceptron",
    "href": "posts/perceptron-post/index.html#part-c-minibatch-perceptron",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Part C: MiniBatch Perceptron",
    "text": "Part C: MiniBatch Perceptron\nIn this section of the blog post, I will be implementing the MiniBatch Perceptron algorithm from scratch. The MiniBatch Perceptron algorithm is a variant of the Perceptron algorithm that updates the weights and bias term based on a mini-batch of data points, rather than updating the weights and bias term based on each data point in the training set. This can lead to faster convergence and better generalization performance, especially when the training data is large.\nMathematically, the MiniBatch Perceptron algorithm works as follows:\n\nRandomly select an initial decision boundary \\(\\mathbf{w}^{(0)}\\)\nIteratively:\nSample \\(k\\) random integers \\(i_1, i_2, ..., i_k \\in \\{1,\\ldots,n\\}\\) without replacement.\nUpdate the decision boundary:\n\\(\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + \\frac{\\alpha}{k} \\sum_{j=1}^k \\mathbb{1} [\\langle \\mathbf{w}^{(t)}, \\mathbf{x}_{i_j} \\rangle y_{i_j} &lt; 0] y_{i_j} \\mathbf{x}_{i_j}\\)\n\nMy implementation can be found at minibatch_perceptron.py. Fitting the MiniBatch perceptron model is the same as the normal perceptron model just with the addition of \\(k\\) and \\(\\alpha\\) as hyperparameters.\nThe \\(k\\) paramater represents the number of data points to sample in each iteration, while the \\(\\alpha\\) parameter represents the learning rate of the algorithm. The learning rate controls how much the weights and bias term are updated in each iteration. A higher learning rate will result in larger updates to the weights and bias term, while a lower learning rate will result in smaller updates.\nWe will perform 3 experiments, C1, C2, and C3 to test the MiniBatch Perceptron algorithm on different datasets.\nLet’s begin by defining the experiment code for all of the experiments:\n\nfrom minibatch_perceptron import MiniBatchPerceptron, MiniBatchPerceptronOptimizer\n\ndef experiment(X, y, k, alpha):  \n    # set seed\n    torch.manual_seed(1234567)\n\n   # MiniBatch Perceptron\n    mb_p = MiniBatchPerceptron() \n    mb_opt = MiniBatchPerceptronOptimizer(mb_p)\n\n    # define loss variable\n    mb_loss = 1.0\n\n    # for keeping track of loss values\n    mb_loss_vec = []\n\n    # for recording iteration number\n    iter = 0\n\n    while mb_loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n        \n        # not part of the update: just for tracking our progress    \n        mb_loss = mb_p.loss(X, y) \n        mb_loss_vec.append(mb_loss)\n        \n        # perform a perceptron update using the random data point\n        mb_opt.step(X, y, k, alpha)\n\n        # update iter\n        iter += 1\n\n        # maxiter condition\n        if iter &gt;= 1000:\n            break\n    \n    # set seed\n    torch.manual_seed(1234567)\n\n    # Normal Perceptron\n    p = Perceptron() \n    opt = PerceptronOptimizer(p)\n\n    # define loss variable\n    loss = 1.0\n\n    # for keeping track of loss values\n    loss_vec = []\n\n    # for recording iteration number\n    iter = 0\n\n    while loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n        \n        # not part of the update: just for tracking our progress    \n        loss = p.loss(X, y) \n        loss_vec.append(loss)\n        \n        # perform a perceptron update using the random data point\n        opt.step(X, y)\n\n        # update iter\n        iter += 1\n\n        # maxiter condition\n        if iter &gt;= 1000:\n            break\n    \n    return loss_vec, mb_loss_vec, p.w, mb_p.w"
  },
  {
    "objectID": "posts/perceptron-post/index.html#part-d-runtime-implications",
    "href": "posts/perceptron-post/index.html#part-d-runtime-implications",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Part D: Runtime Implications",
    "text": "Part D: Runtime Implications\nIn this section, we will discuss the runtime implications of the Perceptron and MiniBatch Perceptron algorithms.\nWhen considering the runtime implications of the Perceptron and MiniBatch Perceptron algorithms, we need to evaluate the runtime complexity of opt.step(X,y)\nLet’s first go over perceptron,py. opt.step() involves the following line by line:\n\nn = X.size()[0] Determines the amount of rows in Constant time\ni = torch.randint(n, size = (1,)) Selects a random number between 0 and \\(n\\) in Constant time\nx_i = X[[i],:] and y_i = y[i] Creates subsets of the data in Constant time\ncurrent_loss = self.model.loss(X, y) Calculates the current loss using dot product in Linear time\nself.model.w += torch.reshape(self.model.grad(x_i, y_i),(self.model.w.size()[0],)) Updates the weights in Linear time\nnew_loss = self.model.loss(X, y) Calculates the new loss in Linear time through dot product\nreturn i, abs(current_loss - new_loss) Returns the index and the difference in loss in Constant time\n\nIt’s essential to specify that while the runtime is linear, it’s not solely in the number of data points, \\(n\\). Rather, it’s linear in the dimensionality of the feature space, \\(p\\). This is because each weight update involves operations proportional to the number of features. So, the time complexity of opt.step() in the Perceptron algorithm is indeed linear, \\(O(p)\\).\nSince the largest time complexity is Linear, we can say that the time complexity of opt.step() is Linear or \\(O(n)\\).\nNow let’s go over minibatch_perceptron.py. opt.step()\nThe two are actually very similar, the only difference in runtime is in the grad() function. In the normal Perceptron algorithm, the grad() function is called on a single data point, while in the MiniBatch Perceptron algorithm, the grad() function is called on a mini-batch of data points. This means in the normal Perceptron algorithm, a single dot product between 1 x \\(p\\) vectors is computed whereas in the MiniBatch perceptron algorithm a matrix product between \\(X^{k \\times p}\\) and \\(w^{p \\times 1}\\) is computed. Calculating the matrix product is equal to calculating \\(k\\) dot products, which are both \\(O(p)\\) operations. Meaning that the runtime of the Matrix product is \\(O(k \\cdot p)\\). However, there’s more to consider. In addition to the matrix product, the runtime also depends on the size of the mini-batch, \\(k\\). So, the overall time complexity of opt.step() in the MiniBatch Perceptron algorithm is \\(O(k \\cdot p)\\), considering both the matrix product and the size of the mini-batch."
  },
  {
    "objectID": "posts/perceptron-post/index.html#conclusion",
    "href": "posts/perceptron-post/index.html#conclusion",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, I have implemented the Perceptron algorithm from scratch and tested it on a synthetic dataset. I have also implemented the MiniBatch Perceptron algorithm and tested it on different datasets. I have shown that the Perceptron algorithm can learn a decision boundary that separates the data points into two classes, and that the MiniBatch Perceptron algorithm can converge faster than the Perceptron algorithm by updating the weights and bias term based on a mini-batch of data points in each iteration. I have also discussed the runtime implications of the Perceptron and MiniBatch Perceptron algorithms, and shown that the time complexity of the opt.step() function is Linear for the normal Perceptron algorithm and \\(O(k \\cdot p)\\) for the MiniBatch algorithm. Overall, the Perceptron and MiniBatch Perceptron algorithms are simple and efficient algorithms for binary classification tasks, and can be extended to handle data with more than two dimensions. It was great to be able to implement a simple machine learning algorithm from scratch and experiment with it on different datasets. I hope you enjoyed reading this blog post and learned something new about the Perceptron and MiniBatch Perceptron algorithms. Thank you for reading!"
  },
  {
    "objectID": "posts/logistic-regression-post/index.html",
    "href": "posts/logistic-regression-post/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer"
  },
  {
    "objectID": "posts/logistic-regression-post/index.html#abstract",
    "href": "posts/logistic-regression-post/index.html#abstract",
    "title": "Implementing Logistic Regression",
    "section": "Abstract:",
    "text": "Abstract:\nIn this Blog Post, we implement logistic regression. We will run three experiments to understand the working of logistic regression. In the first experiment, we will implement logistic regression using vanilla gradient descent. In the second experiment, we will implement logistic regression using gradient descent with momentum. In the third experiment, we will examine the issues of overfitting. We find that when \\(\\beta\\) is higher (momentum), the model converges faster. We also find that the model overfits when the the number of features is larger than the normal of observations\nIn this blog post we will:\n\nImplement gradient descent for logistic regression in an object-oriented paradigm.\nImplement a key variant of gradient descent with momentum in order to achieve faster convergence.\nPerform experiments to test our implementations."
  },
  {
    "objectID": "posts/logistic-regression-post/index.html#part-a-implement-logistic-regression",
    "href": "posts/logistic-regression-post/index.html#part-a-implement-logistic-regression",
    "title": "Implementing Logistic Regression",
    "section": "Part A: Implement Logistic Regression",
    "text": "Part A: Implement Logistic Regression\nTo view my implementation of logistic regression, please refer to logistic.py"
  },
  {
    "objectID": "posts/logistic-regression-post/index.html#part-b-experiments",
    "href": "posts/logistic-regression-post/index.html#part-b-experiments",
    "title": "Implementing Logistic Regression",
    "section": "Part B: Experiments",
    "text": "Part B: Experiments"
  },
  {
    "objectID": "posts/logistic-regression-post/index.html#conclusion",
    "href": "posts/logistic-regression-post/index.html#conclusion",
    "title": "Implementing Logistic Regression",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we implemented logistic regression in an object-oriented paradigm and tested our implementation with various experiments. We trained a logistic regression model using vanilla gradient descent and gradient descent with momentum. We also explored the concept of overfitting by generating data where the number of features is greater than the number of data points. Through these experiments, I gained a better understanding of logistic regression and gradient descent optimization techniques."
  },
  {
    "objectID": "posts/whose-cost-post/index.html",
    "href": "posts/whose-cost-post/index.html",
    "title": "Whose Costs?",
    "section": "",
    "text": "In this Blog Post I create a logistic regression model to predict the probability of a individual being accepted or rejected for a loan based on the features provided in the dataset. I find that the model has an accuracy of around 84% with an expected profit per loan of $1717.02"
  },
  {
    "objectID": "posts/whose-cost-post/index.html#visualizations-of-the-data",
    "href": "posts/whose-cost-post/index.html#visualizations-of-the-data",
    "title": "Whose Costs?",
    "section": "Visualizations of the Data",
    "text": "Visualizations of the Data\nFor the dataset, I created two plots and one summary table in order to contextualize the data.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\nplt.figure(figsize=(12, 8))\nsns.scatterplot(x='person_age', y='person_emp_length', hue='loan_intent', data=df, palette='viridis')\nplt.title('Loan Intent vs. Age and Employment Length')\nplt.xlabel('Age')\nplt.ylabel('Employment Length (Years)')\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Loan Intent\n\n\n\n\n\nRegarding Figure 1, we can see some patterns within the dataset among loan intentions.\nMost notably in the bottom left corner of the plot, which indicates both a young age and limited employment history, there is a culmination of both Educational and Venture loans being used.\nThis makes sense given the current context of the world, as young individuals would most likely be in college or other forms of education at this time. On a similar note, venture capital is often used for newly formed or aspiring companies. Hence, the youthful age and lack of employment history match.\nConversely, as age increases, loans for medical, home improvement, and debt consolidation become more likely. This evidence is runs concurrent with today’s societal concerns from our more elderly population.\n\nplt.figure(figsize=(10, 6))\nsns.countplot(x='person_home_ownership', hue='loan_grade', data=df, palette='Set2')\nplt.title('Loan Grade Distribution by Home Ownership')\nplt.xlabel('Home Ownership')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\n\nFigure 2: Loan Grade Distribution by Home Ownership\n\n\n\n\n\nFigure 2 illustrates the Loan grade distribution by home ownership. Loan Grade is viewed A-G with A being the highest loan grade and G being the lowest.\nAn interesting observation to make here is that for individuals in both the ‘Mortgage’ and ‘Home’ groups, the highest frequency of loans is rated as an A whereas for individuals in the ‘Rent’ group the highest frequency of loans is rated as an B.\nThis could give context to the economic state of home ownership and bank trust. Those with a mortgage/own property could perhaps be linked to being more trustworthy due to having a longer term connection with a property. Banks may view those in the ‘Rent’ category as a bit more risky due to the short term ability of renting.\n\nsummary_table = df.groupby('loan_intent').agg({\n    'loan_int_rate': 'mean',\n    'loan_amnt': 'mean'\n}).reset_index()\n\nsummary_table.rename(columns={\n    'loan_int_rate': 'Average Interest Rate',\n    'loan_amnt': 'Average Loan Amount'\n}, inplace=True)\n\nprint(summary_table)\n\n         loan_intent  Average Interest Rate  Average Loan Amount\n0  DEBTCONSOLIDATION              10.983305          9620.901149\n1          EDUCATION              10.965465          9460.015604\n2    HOMEIMPROVEMENT              11.160075         10348.725017\n3            MEDICAL              11.051946          9242.269907\n4           PERSONAL              11.009814          9549.427178\n5            VENTURE              10.940866          9516.417425\n\n\nThis summary table above displays the relative average interest rates and average loan amounts for each category of loan intent.\nHome improvement has the greatest average loan amount while similarly having the highest interest rate. On the flip side, education has the lowest average loan amount but the second lowest interest rate."
  },
  {
    "objectID": "posts/whose-cost-post/index.html#part-c-build-a-model",
    "href": "posts/whose-cost-post/index.html#part-c-build-a-model",
    "title": "Whose Costs?",
    "section": "Part C: Build a Model",
    "text": "Part C: Build a Model\nI will be utilizing a Logistic Regression Model for predicting whether a prospective borrower is likely to default on a given loan.\nI will use my findings from the previous section to help me determine which features to include in my model. Let’s begin with loan_int_rate and loan_amnt as they are the most likely to be correlated with the likelihood of defaulting on a loan.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\ny_train = df_train['loan_status']\nX_train = df_train.drop(['loan_status', 'loan_grade'], axis = 1)\n\nqual_cols = list(X_train.select_dtypes(exclude=['number']).columns)\nX_train = pd.get_dummies(X_train,\n                          columns = qual_cols)\n\nwant_cols = ['loan_int_rate', 'loan_amnt']\n\nLR = LogisticRegression()\ncross_val_score(LR, X_train[want_cols], y_train, cv = 5).mean()\n\n0.7849438872731398\n\n\nA score of around 79% is a good starting point for this model. However, I will be looking to improve this score by adding more features to the model. Through various combinations and manual testing we found that adding loan_percent_income and person_home_ownership, and removing loan_amnt to the model has an increase in accuracy.\nWe found this through iterating through different combinations of features and seeing which ones had the most impact/success on the model.\nFor example, we found that removing loan_amnt, generally increased the accuracy of the model for any subset of features that contained it.\nTo replicate this, simply change the different combinations of features in the cols variable in the code below.\nWith that said, let’s look at the new accuracy of the model using the subset of features we found to be the most successful.\n\ncols = ['loan_percent_income', 'loan_int_rate', 'person_home_ownership_MORTGAGE',\n  'person_home_ownership_OTHER',\n  'person_home_ownership_OWN',\n  'person_home_ownership_RENT']\n\nLR.fit(X_train[cols], y_train)\nscore = LR.score(X_train[cols], y_train)\n\nscore\n\n0.8473429107899219\n\n\nWith a score of around 85%, adding these two features increases our accuracy by almost 7% which is a significant improvement. Let’s find the weights for these coefficients to see which features are the most important in predicting whether a borrower will default on a loan.\n\nw = LR.coef_[0]\nw\n\narray([ 8.26009318, 20.76145371, -0.07428677,  0.36353638, -1.18076269,\n        0.89253454])\n\n\nUsing these weights, let’s determine the linear score function. Recall that the linear score function is given by:\n\\(s_i = \\langle \\mathbf{X}_i, \\mathbf{w} \\rangle\\)\nwhere \\(\\mathbf{X}_i\\) is the feature vector for the i-th data point and \\(\\mathbf{w}\\) is the weight vector.\nWe can code this as:\n\ndef linear_score(X, w):\n    return X@w\n\nWith a score function in place, we can now give each borrower a particular score and test thresholds by profit to determine the best threshold for our model:\n\nnum_thresholds = 101\nprofit = np.zeros(num_thresholds)\n\ns = linear_score(X_train[cols], w)\nT = np.linspace(s.min()-0.1, s.max()+0.1, num_thresholds)\nfor i in range(num_thresholds):\n    y_pred = s &gt;= T[i]\n    TN = X_train[((y_pred == 0) & (y_train == 0))]\n    FN = X_train[((y_pred == 0) & (y_train == 1))]\n    gain = (TN['loan_amnt']*(1 + 0.25*TN['loan_int_rate'])**10 - TN['loan_amnt']).sum()\n    cost = (FN['loan_amnt']*(1 + 0.25*FN['loan_int_rate'])**3 - 1.7*FN['loan_amnt']).sum()\n    total_loans = TN.shape[0] + FN.shape[0]\n    if total_loans == 0:\n        profit[i] = 0\n    else:\n        profit[i] = (gain + cost) / total_loans\n\nprofit_plt = sns.lineplot(x = T, y = profit)\nprofit_plt.grid()\nprofit_plt.set(xlabel = r\"Threshold $t$\", ylabel = \"Expected Profit Per Loan\");\n\n/Users/donovanwood/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/donovanwood/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nGiven this visual, we can see that the best threshold for our model is in between 4 and 6. Let’s run some code to find the exact threshold value:\n\nmax_profit_index = np.argmax(profit)\nprofit\nt = T[max_profit_index]\np = max(profit)\nt, p\n\n(4.811972981226952, 1759.7216924380693)\n\n\nWe find that the best threshold for our model is approximately 4.81 with an expected profit per loan value of $1759! This is a great result and shows that our model is working well so far. Let’s test it!"
  },
  {
    "objectID": "posts/whose-cost-post/index.html#pre-process-the-test-dataset",
    "href": "posts/whose-cost-post/index.html#pre-process-the-test-dataset",
    "title": "Whose Costs?",
    "section": "Pre-Process the Test Dataset",
    "text": "Pre-Process the Test Dataset\nFirst we have to prepare the test set by performing the same preprocessing steps we did on our training set.\n\n\n# Load test data\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\ndf_test = prepare_data(df_test)\ny_test = df_test['loan_status']\nX_test = df_test.drop(['loan_status', 'loan_grade'], axis = 1)\nX_test = pd.get_dummies(df_test,\n                        columns = qual_cols)\n\nLet’s use the same weight vector and threshold that we found in the previous section to evaluate our model on the test set:\n\nnum_thresholds = 101\n\ns = linear_score(X_test[cols], w)\ny_pred = s &gt;= t\nTN = X_test[((y_pred == 0) & (y_test == 0))]\nFN = X_test[((y_pred == 0) & (y_test == 1))]\ngain = (TN['loan_amnt']*(1 + 0.25*TN['loan_int_rate'])**10 - TN['loan_amnt']).sum()\ncost = (FN['loan_amnt']*(1 + 0.25*FN['loan_int_rate'])**3 - 1.7*FN['loan_amnt']).sum()\ntotal_loans = TN.shape[0]+FN.shape[0]\nprofit = (gain + cost) / total_loans\n\nprofit\n\n1717.020631545117\n\n\nEvaluating the model on the test set yields an expected profit per loan of $1717.02 which is slightly less than the expected profit per loan of the training set. That said, this is a good sign that our model is generalizing well to new data."
  },
  {
    "objectID": "posts/cs-451-quant-project/axiao_research.html",
    "href": "posts/cs-451-quant-project/axiao_research.html",
    "title": "Applying Machine Learning to Stock Price Prediction",
    "section": "",
    "text": "What problem did we address?\nWhat approach(es) did we use to address it?\nWhat are the big-picture results?\n\n\n\n\n\nPrompt: “Your introduction should describe the big-picture problem that you aimed to address in your project. What’s the problem to be solved, and why is it important? Who has tried solving this problem already, and what did they do? I would expect most introductions to reference no fewer than 2 scholarly studies that attempted similar tasks, although 5 is probably a better target.”\n\nIn this blog post, we train machine learning models on historical stock market data to predict future stock price movements. This is a highly popular problem to address because of the potential for significant monetary gain. This is an important problem societally because stock markets are mechanisms of price discovery: they answer the question “What is a company worth?” Finding the right answer to that question allows society to correctly allocate more or less capital (money) to that company. On an individual level, this is an important problem to us as the authors because it’s the problem for all quant trading: making a profitable model.\nAn enormous body of literature within and without computer science exists for stock market prediction. Among the papers most relevant to our work are Bhandari et al. (2022) and Zhang (2022).\nGunduz (2021) applies LSTM and ensemble learning (Light-GBM) models to predict the hourly directions of eight banking stocks in Borsa Istanbul. He achieved up to maximum success rate of 0.685 using individual features of bank stocks and LSTM.\nBhandari et al. (2022) apply single-layer and multi-layer LSTM models to the problem of predicting the S&P 500, the index of the largest 500 publicly traded companies in America. Their single-layer LTSM model with 150 neurons is their best performing specification. Their set of predicted values have an average correlation coefficient of 0.9976 with actual S&P index values.\nZhang (2022) finds the LSTM network model does not perform better than other models when applied to a short forecasting horizon (1 to 10 days). Zhang’s “other models” are linear regression, eXtreme gradient boosting (XGBoost), last value, and moving average.\nWe take some of the “best practices” we observe in the above papers, specifically benchmarking with last value and calculating accuracy with R, RMSE, and MAPE. Unlike the mentioned papers, we will be focusing on single stocks and attempting to build a model that outperforms the last value benchmark.\n\n\n\n\nWho are the potential users of your project? Who, other than your users, might still be affected by your project?\nWho benefits from technology that solves the problem you address?\nWho could be harmed from technology that solves the problem you well address?\nWhat is your personal reason for working on this problem?\nBased on your reflection, would the world be a more equitable, just, joyful, peaceful, or sustainable place based on the technology that you implemented?\n\n\nimport yfinance as yf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom yahoofinancials import YahooFinancials as YF\n\n\n# Define the ticker and the time period\nticker = 'TSLA'\nstart_date = '2019-01-01'\nend_date = '2020-01-01'\n\n# Fetch TSLA data\ntsla = yf.download(ticker, start=start_date, end=end_date)\n\nprint(tsla.head())\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n                 Open       High        Low      Close  Adj Close     Volume\nDate                                                                        \n2019-01-02  20.406668  21.008667  19.920000  20.674667  20.674667  174879000\n2019-01-03  20.466667  20.626667  19.825333  20.024000  20.024000  104478000\n2019-01-04  20.400000  21.200001  20.181999  21.179333  21.179333  110911500\n2019-01-07  21.448000  22.449333  21.183332  22.330667  22.330667  113268000\n2019-01-08  22.797333  22.934000  21.801332  22.356667  22.356667  105127500\n\n\n\n# Moving Average \nshort_window = 40\nlong_window = 100\n\ntsla['Short_MAvg'] = tsla['Close'].rolling(window=short_window, min_periods=1).mean()\ntsla['Long_MAvg'] = tsla['Close'].rolling(window=long_window, min_periods=1).mean()\n\n\n# Basic trading signal based on crossover \n\n# Create a 'Signal' column\ntsla['Signal'] = 0\ntsla['Signal'] = np.where(tsla['Short_MAvg'] &gt; tsla['Long_MAvg'], 1, 0)\n\n\n# Generate trading orders\ntsla['Position'] = tsla['Signal'].diff()\n\n\n# Basic Back Test\n\n# Plot the closing prices and moving averages\nplt.figure(figsize=(14,7))\nplt.plot(tsla['Close'], label='Close Price', alpha=0.5)\nplt.plot(tsla['Short_MAvg'], label=f'{short_window}-Day MA', alpha=0.75)\nplt.plot(tsla['Long_MAvg'], label=f'{long_window}-Day MA', alpha=0.75)\n\n# Plot buy signals\nplt.plot(tsla[tsla['Position'] == 1].index, tsla['Short_MAvg'][tsla['Position'] == 1], '^', markersize=10, color='g', lw=0, label='Buy Signal')\n\n# Plot sell signals\nplt.plot(tsla[tsla['Position'] == -1].index, tsla['Short_MAvg'][tsla['Position'] == -1], 'v', markersize=10, color='r', lw=0, label='Sell Signal')\n\nplt.title('AAPL Stock Price and Moving Average Crossovers')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\ntsla['Std_Dev'] = tsla['Close'].rolling(window=short_window, min_periods=1).std()\n\n# Calculate the z-score\ntsla['Z_Score'] = (tsla['Close'] - tsla['Short_MAvg']) / tsla['Std_Dev']\n\n\n# Define thresholds for buying and selling\nthreshold_buy = -1.5  # Buy signal threshold\nthreshold_sell = 1.5  # Sell signal threshold\n\n# Generate signals\ntsla['Signal'] = 0\ntsla['Signal'][tsla['Z_Score'] &gt; threshold_sell] = -1  # Sell signal\ntsla['Signal'][tsla['Z_Score'] &lt; threshold_buy] = 1  # Buy signal\n\n# Generate trading positions\ntsla['Position'] = tsla['Signal'].replace(0, np.nan).ffill().fillna(0)\n\nC:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_70572\\3954209935.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  tsla['Signal'][tsla['Z_Score'] &gt; threshold_sell] = -1  # Sell signal\nC:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_70572\\3954209935.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  tsla['Signal'][tsla['Z_Score'] &lt; threshold_buy] = 1  # Buy signal\n\n\n\n# Plot the results\nplt.figure(figsize=(14,7))\nplt.plot(tsla['Close'], label='Close Price', alpha=0.5)\nplt.plot(tsla['Short_MAvg'], label='Moving Average', alpha=0.75)\nplt.fill_between(tsla.index, tsla['Short_MAvg'] - tsla['Std_Dev'], tsla['Short_MAvg'] + tsla['Std_Dev'], color='gray', alpha=0.3, label='Standard Deviation Range')\n\n# Highlight the buy and sell signals\nplt.plot(tsla[tsla['Position'] == 1].index, tsla['Close'][tsla['Position'] == 1], '^', markersize=10, color='g', lw=0, label='Buy Signal')\nplt.plot(tsla[tsla['Position'] == -1].index, tsla['Close'][tsla['Position'] == -1], 'v', markersize=10, color='r', lw=0, label='Sell Signal')\n\nplt.title(f'{ticker} Stock Price and Mean Reversion Signals')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\nticker = 'TSLA'\nstart_date = '2010-01-01'\nend_date = '2020-01-01'\n\nmodel = LogisticRegression()\n\ndata = yf.download(ticker, start=start_date, end=end_date)\n\n# Calculate moving averages and std\ndata['SMA_20'] = data['Close'].rolling(window=20).mean()\ndata['SMA_50'] = data['Close'].rolling(window=50).mean()\ndata['Std_Dev'] = data['Close'].rolling(window=20).std()\n\n# Calculate the z-score\ndata['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n\n# Calculate RSI\ndelta = data['Close'].diff()\nup = delta.clip(lower=0)\ndown = -1 * delta.clip(upper=0)\nema_up = up.ewm(com=13, adjust=False).mean()\nema_down = down.ewm(com=13, adjust=False).mean()\nrs = ema_up / ema_down\n\ndata['RSI'] = 100 - (100 / (1 + rs))\n\n# Calculate the daily returns\ndata['Returns'] = data['Close'].pct_change()\n\n# Drop any NaNs\ndata.dropna(inplace=True)\n\n# If stock price goes up or down\ndata['Target'] = (data['Returns'].shift(-1) &gt; 0).astype(int)\n\n# Wanted features for X and y\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Returns']\nX = data[features]\ny = data['Target']\n\n# Split data into first 80% and last 20%\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Calculate cumulative strategy returns on test data\nX_test['Predicted_Signal'] = y_pred\nX_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1)\ncumulative_strategy_returns = (X_test['Strategy_Returns'] + 1).cumprod()\n\n# Calculate cumulative returns for the market\nspy = yf.download('SPY', start=X_test.index[0].date().strftime('%Y-%m-%d'), end=end_date)\nspy['Returns'] = spy['Close'].pct_change()\ncumulative_market_returns = (spy['Returns'] + 1).cumprod()\n\nplt.figure(figsize=(10,5))\nplt.plot(cumulative_strategy_returns, label='Strategy Returns')\nplt.plot(cumulative_market_returns, label='Market Returns')\nplt.legend()\nplt.show()\n\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n\n\nAccuracy: 0.4946695095948827\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\ndef train(model, ticker, start_date, end_date):\n    data = yf.download(ticker, start=start_date, end=end_date)\n    \n    # Calculate moving averages and std\n    data['SMA_20'] = data['Close'].rolling(window=20).mean()\n    data['SMA_50'] = data['Close'].rolling(window=50).mean()\n    data['Std_Dev'] = data['Close'].rolling(window=20).std()\n\n    # Calculate the z-score\n    data['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n\n    # Calculate RSI\n    delta = data['Close'].diff()\n    up = delta.clip(lower=0)\n    down = -1 * delta.clip(upper=0)\n    ema_up = up.ewm(com=13, adjust=False).mean()\n    ema_down = down.ewm(com=13, adjust=False).mean()\n    rs = ema_up / ema_down\n\n    data['RSI'] = 100 - (100 / (1 + rs))\n\n    # Calculate the daily returns\n    data['Returns'] = data['Close'].pct_change()\n\n    # Drop any NaNs\n    data.dropna(inplace=True)\n\n    # If stock price goes up or down\n    data['Target'] = (data['Returns'].shift(-1) &gt; 0).astype(int)\n\n    # Wanted features for X and y\n    features = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Returns']\n    X = data[features]\n    y = data['Target']\n\n    # Split data into first 80% and last 20%\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy:\", accuracy)\n\n    # Calculate cumulative strategy returns on test data\n    X_test['Predicted_Signal'] = y_pred\n    X_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1)\n    cumulative_strategy_returns = (X_test['Strategy_Returns'] + 1).cumprod()\n\n    # Calculate cumulative returns for the market\n    spy = yf.download('SPY', start=X_test.index[0].date().strftime('%Y-%m-%d'), end=end_date)\n    spy['Returns'] = spy['Close'].pct_change()\n    cumulative_market_returns = (spy['Returns'] + 1).cumprod()\n    \n    plt.figure(figsize=(10,5))\n    plt.plot(cumulative_strategy_returns, label='Strategy Returns')\n    plt.plot(cumulative_market_returns, label='Market Returns')\n    plt.legend()\n    plt.show()\n\n\n\nCode\n%load_ext autoreload\n%autoreload 2\nfrom strategy import MeanReversion\n\n\n\nticker = 'TSLA'\nmarket = 'SPY'\nstart = '2014-01-01'\nend = '2024-01-01'\nMR = MeanReversion(ticker, start, end, market)\n\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n\n\n\nRF = RandomForestClassifier(n_estimators=100, random_state=42)\nX_test = MR.evaluate(model=RF)\n\nAccuracy: 0.5263157894736842\n\n\n\n\n\n\n\n\n\n\nLR = LogisticRegression()\nX_test = MR.evaluate(model=LR)\n\nAccuracy: 0.5041322314049587\n\n\n\n\n\n\n\n\n\n\nSVM = SVC()\nX_test = MR.evaluate(model = SVM)\n\nAccuracy: 0.5337552742616034\n\n\n\n\n\n\n\n\n\nNote: Different models work better for different stocks.\nBelow, we calculate the “risk-free rate,” used as a reference for all returns in financial markets. It is considered the return on an investment that is considered to have zero risk (short-term US treasuries). We use the risk-free rate to calculate the Sharpe ratio, which is a widely-used measure of an investment or a strategy’s “risk-adjusted” performance.\n\ndef deannualize(annual_rate, periods=365):\n    return (1 + annual_rate) ** (1/periods) - 1\n\ndef get_risk_free_rate(start_date, end_date):\n    # download 3-month us treasury bills rates\n    annualized = yf.download('^IRX', start_date, end_date)['Close']\n    annualized = annualized / 100\n    \n    # de-annualize\n    daily = annualized.apply(deannualize)\n\n    # create dataframe\n    return pd.DataFrame({\"annualized\": annualized, \"daily\": daily})\n\n\n\n\n\nfrom pytrends.request import TrendReq\nfrom pytrends import dailydata\n\n\npytrends = TrendReq(hl = 'en-US', tz=360)\n\n\npytrends.build_payload(kw_list=['Microsoft', 'Tesla', 'Apple'], timeframe='2010-01-01 2020-01-01')\n\n\n\n\n\ntickers = ['XOM', 'CVX', 'COP', 'NEE', 'SO', 'EOG', 'DUK', 'MPC', 'SLB', 'PSX']\n\ndef prepare_data(tickers, start_date, end_date):\n    '''\n    Combines data of all tickers into a single dataframe for X_train. X_test is a list of dataframes for each ticker.\n    '''\n    X_train_list = []\n    y_train_list = []\n    X_test_list = []\n    y_test_list = []\n    for t in tickers:\n        data = yf.download(t, start=start_date, end=end_date)\n        # Calculate moving averages and std\n        data['SMA_20'] = data['Close'].rolling(window=20).mean()\n        data['SMA_50'] = data['Close'].rolling(window=50).mean()\n        data['Std_Dev'] = data['Close'].rolling(window=20).std()\n\n        # Calculate the z-score\n        data['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n\n        # Calculate RSI\n        delta = data['Close'].diff()\n        up = delta.clip(lower=0)\n        down = -1 * delta.clip(upper=0)\n        ema_up = up.ewm(com=13, adjust=False).mean()\n        ema_down = down.ewm(com=13, adjust=False).mean()\n        rs = ema_up / ema_down\n\n        data['RSI'] = 100 - (100 / (1 + rs))\n\n        # Calculate the daily returns\n        data['Returns'] = data['Close'].pct_change()\n\n        # Drop any NaNs\n        data.dropna(inplace=True)\n\n        # If stock price goes up or down\n        data['Target'] = (data['Returns'].shift(-1) &gt; 0).astype(int)\n        data['Ticker'] = t\n        features = ['Ticker', 'SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Returns']\n        X = data[features]\n        y = data['Target']\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n        X_train_list.append(X_train)\n        y_train_list.append(y_train)\n        X_test_list.append(X_test)\n        y_test_list.append(y_test)\n\n    return pd.concat(X_train_list, ignore_index=True), pd.concat(y_train_list, ignore_index=True), X_test_list, y_test_list\n\n\ndef evaluate(model, X_test_, y_test_, features, market_data):\n    '''\n    Compares returns to the market for a single ticker.\n    '''\n    X_test = X_test_.copy()\n    y_test = y_test_.copy()\n    \n    y_pred = model.predict(X_test[features])\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"{X_test.Ticker.iloc[0]} Accuracy:\", accuracy)\n\n    # Calculate cumulative strategy returns on test data\n    X_test['Predicted_Signal'] = y_pred\n    X_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1)\n    cumulative_strategy_returns = (X_test['Strategy_Returns'].fillna(0) + 1).cumprod()\n    returns = X_test.loc[X_test.index, 'Returns']\n    returns.iloc[0] = 0\n    cumulative_stock_returns = (returns + 1).cumprod()\n\n    # Calculate cumulative returns for the market\n    market_data['Returns'] = market_data['Close'].pct_change()\n    #cumulative_market_returns = (market_data['Returns'].fillna(0) + 1).cumprod()\n\n    plt.figure(figsize=(10,5))\n    plt.plot(cumulative_strategy_returns, label='Strategy Returns', alpha=0.5)\n    #plt.plot(cumulative_market_returns, label='Market Returns')\n    plt.plot(cumulative_stock_returns, label='Stock Returns', alpha=0.5)\n    plt.title(f'{X_test.Ticker.iloc[0]} Returns')\n    plt.legend()\n    plt.show()\n\n    return X_test\n\n\nstart = '2014-01-01'\nend = '2024-01-01'\nX_train, y_train, X_test, y_test = prepare_data(tickers, start, end)\n\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n\n\n\nfrom sklearn.svm import LinearSVC\n\n#model = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel = SVC()\n#model = LogisticRegression()\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Returns']\nmodel = model.fit(X_train[features], y_train)\n\nmarket_data = yf.download('SPY', start=start, end=end)\nX_test_vec = []\n\nfor i in range(10):\n    market = market_data[market_data.index &gt;= X_test[i].index[0]].copy()\n    X_test_vec.append(evaluate(model, X_test[i], y_test[i], features, market_data=market))\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nXOM Accuracy: 0.52834008097166\nCVX Accuracy: 0.48380566801619435\nCOP Accuracy: 0.5323886639676113\nNEE Accuracy: 0.48380566801619435\nSO Accuracy: 0.4939271255060729\nEOG Accuracy: 0.5425101214574899\nDUK Accuracy: 0.5182186234817814\nMPC Accuracy: 0.5303643724696356\nSLB Accuracy: 0.520242914979757\nPSX Accuracy: 0.5161943319838057\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX_test_vec[0]['Predicted_Signal'].mean()\n\n0.5040485829959515\n\n\n\n\n\n\n\n\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.svm import LinearSVC\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI']\nrfe = RFECV(LinearSVC(dual='auto'), cv = 5)\n\n\nrfe = rfe.fit(X_train[features], y_train)\n\n\nrfe.ranking_\n\narray([2, 3, 1, 1, 4])\n\n\n\nmarket_data = yf.download('SPY', start=start, end=end)\nmarket = market_data[market_data.index &gt;= X_test[i].index[0]].copy()\nX_test_vec = []\nfor i in range(10):\n    X_test_vec.append(evaluate(rfe, X_test[i], y_test[i], features, market_data=market))\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nXOM Accuracy: 0.5182186234817814\nCVX Accuracy: 0.46963562753036436\nCOP Accuracy: 0.5222672064777328\nNEE Accuracy: 0.4898785425101215\nSO Accuracy: 0.4939271255060729\nEOG Accuracy: 0.46963562753036436\nDUK Accuracy: 0.5141700404858299\nMPC Accuracy: 0.5323886639676113\nSLB Accuracy: 0.5242914979757085\nPSX Accuracy: 0.5323886639676113\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nfrom torch.autograd import Variable \n\n\n%load_ext autoreload\n%autoreload 2\nfrom lstm_model import LSTMModel\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nticker = 'XOM'\nstart_date = '2014-01-01'\nend_date = '2024-01-01'\n\n#model = LogisticRegression()\n\ndata = yf.download(ticker, start=start_date, end=end_date)\n\n# Calculate moving averages and std\ndata['SMA_20'] = data['Close'].rolling(window=20).mean()\ndata['SMA_50'] = data['Close'].rolling(window=50).mean()\ndata['Std_Dev'] = data['Close'].rolling(window=20).std()\n\n# Calculate the z-score\ndata['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n\n# Calculate RSI\ndelta = data['Close'].diff()\nup = delta.clip(lower=0)\ndown = -1 * delta.clip(upper=0)\nema_up = up.ewm(com=13, adjust=False).mean()\nema_down = down.ewm(com=13, adjust=False).mean()\nrs = ema_up / ema_down\n\ndata['RSI'] = 100 - (100 / (1 + rs))\n\n# Calculate the daily returns\ndata['Returns'] = data['Close'].pct_change()\n\n# If stock price goes up or down\ndata['Target'] = data['Close'].shift(-1)\n\ndata.dropna(inplace=True)\n\n# Wanted features for X and y\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Close', 'Returns']\nX = data.loc[:, features]\ny = data.iloc[:, (data.shape[1]-1):(data.shape[1])]\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nmm1 = MinMaxScaler()\nss1 = StandardScaler()\n# mm2 = MinMaxScaler()\n# ss2 = StandardScaler()\n\nX_ss = pd.DataFrame(ss1.fit_transform(X), index=X.index, columns=X.columns)\ny_mm = pd.DataFrame(mm1.fit_transform(y), index=y.index, columns=y.columns)\n\n# Split data into first 80% and last 20%\nX_train, X_test, y_train, y_test = train_test_split(X_ss, y_mm, test_size=0.2, random_state=42, shuffle=False)\n# X_train_ss = pd.DataFrame(ss1.fit_transform(X_train), index=X_train.index, columns=X.columns)\n# y_train_mm = pd.DataFrame(mm1.fit_transform(y_train), index=y_train.index, columns=y.columns)\n# X_test_ss = pd.DataFrame(ss2.fit_transform(X_test), index=X_test.index, columns=X.columns)\n# y_test_mm = pd.DataFrame(mm2.fit_transform(y_test), index=y_test.index, columns=y.columns)\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\nfeatures_ = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI']\nX_train_tensors = Variable(torch.Tensor(np.array(X_train[features_])))\nX_test_tensors = Variable(torch.Tensor(np.array(X_test[features_])))\n\ny_train_tensors = Variable(torch.Tensor(y_train.values))\ny_test_tensors = Variable(torch.Tensor(y_test.values))\n\nX_train_final = torch.reshape(X_train_tensors,   (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\nX_test_final = torch.reshape(X_test_tensors,  (X_test_tensors.shape[0], 1, X_test_tensors.shape[1]))\n\n\nprint(\"Training Shape\", X_train_final.shape, y_train_tensors.shape)\nprint(\"Testing Shape\", X_test_final.shape, y_test_tensors.shape) \n\nTraining Shape torch.Size([1972, 1, 5]) torch.Size([1972, 1])\nTesting Shape torch.Size([494, 1, 5]) torch.Size([494, 1])\n\n\n\nnum_epochs = 1000 #1000 epochs\nlearning_rate = 0.001 #0.001 lr\n\ninput_size = 5 #number of features\nhidden_size = 2 #number of features in hidden state\nnum_layers = 1 #number of stacked lstm layers\n\nnum_classes = 1 #number of output classes \n\n\nlstm = LSTMModel(num_classes, input_size, hidden_size, num_layers, X_train_final.shape[1]) #our lstm class \n\n\ncriterion = torch.nn.MSELoss()    # mean-squared error for regression\noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n\n\nfor epoch in range(num_epochs):\n  outputs = lstm.forward(X_train_final) #forward pass\n  optimizer.zero_grad() #calculate the gradient, manually setting to 0\n \n  # obtain the loss function\n  loss = criterion(outputs, y_train_tensors)\n \n  loss.backward() #calculates the loss of the loss function\n \n  optimizer.step() #improve from loss, i.e backprop\n  if epoch % 100 == 0:\n    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item())) \n\nEpoch: 0, loss: 0.16941\nEpoch: 100, loss: 0.01794\nEpoch: 200, loss: 0.00689\nEpoch: 300, loss: 0.00291\nEpoch: 400, loss: 0.00080\nEpoch: 500, loss: 0.00038\nEpoch: 600, loss: 0.00029\nEpoch: 700, loss: 0.00026\nEpoch: 800, loss: 0.00023\nEpoch: 900, loss: 0.00022\n\n\n\ntrain_predict = lstm(X_train_final)\ndata_predict = train_predict.data.numpy()\n\n\ndata_predict = mm1.inverse_transform(data_predict) #reverse transformation\n\n\nX_train = pd.DataFrame(ss1.inverse_transform(X_train), index=X_train.index, columns=features)\n\n\nX_train['Predicted_Price'] = data_predict\nX_train['Actual_Signal'] = (X_train['Returns'].shift(-1) &gt; 0).astype(int)\nX_train['Predicted_Returns'] = X_train['Predicted_Price'].pct_change()\nX_train['Predicted_Signal'] = (X_train['Predicted_Returns'].shift(-1) &gt; 0).astype(int)\nX_train['Strategy_Returns'] = X_train['Returns'] * X_train['Predicted_Signal'].shift(1)\ncumulative_strategy_returns = (X_train['Strategy_Returns'].fillna(0) + 1).cumprod()\nreturns = X_train.loc[X_train.index, 'Returns']\nreturns.iloc[0] = 0\ncumulative_stock_returns = (returns + 1).cumprod()\naccuracy = (X_train['Actual_Signal'] == X_train['Predicted_Signal']).mean()\nprint(f'Accuracy: {accuracy}')\n# Calculate cumulative returns for the market\n#market_data['Returns'] = market_data['Close'].pct_change()\n#cumulative_market_returns = (market_data['Returns'].fillna(0) + 1).cumprod()\n\nplt.figure(figsize=(10,5))\nplt.plot(cumulative_strategy_returns, label='Strategy Returns', alpha=0.5)\n#plt.plot(cumulative_market_returns, label='Market Returns')\nplt.plot(cumulative_stock_returns, label='Stock Returns', alpha=0.5)\nplt.legend();\n\nAccuracy: 0.9523326572008114\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10,5))\nplt.plot(X_train['Predicted_Price'], label='Predicted Price')\nplt.plot(X_train['Close'], label='Actual Price')\nplt.legend();\n\n\n\n\n\n\n\n\n\ntest_predict = lstm(X_test_final)\ndata_predict = test_predict.data.numpy()\n\n\ndata_predict = mm1.inverse_transform(data_predict)\nX_test = pd.DataFrame(ss1.inverse_transform(X_test), index=X_test.index, columns=features)\n\n\nX_test['Actual_Signal'] = (X_test['Returns'].shift(-1) &gt; 0).astype(int)\nX_test['Predicted_Price'] = data_predict\nX_test['Predicted_Returns'] = X_test['Predicted_Price'].pct_change()\nX_test['Predicted_Signal'] = (X_test['Predicted_Returns'].shift(-1) &gt; 0).astype(int)\nX_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1)\ncumulative_strategy_returns = (X_test['Strategy_Returns'].fillna(0) + 1).cumprod()\nreturns = X_test.loc[X_test.index, 'Returns']\nreturns.iloc[0] = 0\ncumulative_stock_returns = (returns + 1).cumprod()\naccuracy = (X_test['Actual_Signal'] == X_test['Predicted_Signal']).mean()\nprint(f'Accuracy: {accuracy}')\n# Calculate cumulative returns for the market\n#market_data['Returns'] = market_data['Close'].pct_change()\n#cumulative_market_returns = (market_data['Returns'].fillna(0) + 1).cumprod()\n\nplt.figure(figsize=(10,5))\n\nplt.plot(cumulative_strategy_returns, label='Strategy Returns')\n#plt.plot(cumulative_market_returns, label='Market Returns')\nplt.plot(cumulative_stock_returns, label='Stock Returns')\nplt.legend();\n\nAccuracy: 0.9473684210526315\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10,5))\nplt.plot(X_test['Predicted_Price'], label='Predicted Price')\nplt.plot(X_test['Close'], label='Actual Price')\nplt.legend();"
  },
  {
    "objectID": "posts/cs-451-quant-project/axiao_research.html#abstract",
    "href": "posts/cs-451-quant-project/axiao_research.html#abstract",
    "title": "Applying Machine Learning to Stock Price Prediction",
    "section": "",
    "text": "What problem did we address?\nWhat approach(es) did we use to address it?\nWhat are the big-picture results?"
  },
  {
    "objectID": "posts/cs-451-quant-project/axiao_research.html#introduction",
    "href": "posts/cs-451-quant-project/axiao_research.html#introduction",
    "title": "Applying Machine Learning to Stock Price Prediction",
    "section": "",
    "text": "Prompt: “Your introduction should describe the big-picture problem that you aimed to address in your project. What’s the problem to be solved, and why is it important? Who has tried solving this problem already, and what did they do? I would expect most introductions to reference no fewer than 2 scholarly studies that attempted similar tasks, although 5 is probably a better target.”\n\nIn this blog post, we train machine learning models on historical stock market data to predict future stock price movements. This is a highly popular problem to address because of the potential for significant monetary gain. This is an important problem societally because stock markets are mechanisms of price discovery: they answer the question “What is a company worth?” Finding the right answer to that question allows society to correctly allocate more or less capital (money) to that company. On an individual level, this is an important problem to us as the authors because it’s the problem for all quant trading: making a profitable model.\nAn enormous body of literature within and without computer science exists for stock market prediction. Among the papers most relevant to our work are Bhandari et al. (2022) and Zhang (2022).\nGunduz (2021) applies LSTM and ensemble learning (Light-GBM) models to predict the hourly directions of eight banking stocks in Borsa Istanbul. He achieved up to maximum success rate of 0.685 using individual features of bank stocks and LSTM.\nBhandari et al. (2022) apply single-layer and multi-layer LSTM models to the problem of predicting the S&P 500, the index of the largest 500 publicly traded companies in America. Their single-layer LTSM model with 150 neurons is their best performing specification. Their set of predicted values have an average correlation coefficient of 0.9976 with actual S&P index values.\nZhang (2022) finds the LSTM network model does not perform better than other models when applied to a short forecasting horizon (1 to 10 days). Zhang’s “other models” are linear regression, eXtreme gradient boosting (XGBoost), last value, and moving average.\nWe take some of the “best practices” we observe in the above papers, specifically benchmarking with last value and calculating accuracy with R, RMSE, and MAPE. Unlike the mentioned papers, we will be focusing on single stocks and attempting to build a model that outperforms the last value benchmark."
  },
  {
    "objectID": "posts/cs-451-quant-project/axiao_research.html#values",
    "href": "posts/cs-451-quant-project/axiao_research.html#values",
    "title": "Applying Machine Learning to Stock Price Prediction",
    "section": "",
    "text": "Who are the potential users of your project? Who, other than your users, might still be affected by your project?\nWho benefits from technology that solves the problem you address?\nWho could be harmed from technology that solves the problem you well address?\nWhat is your personal reason for working on this problem?\nBased on your reflection, would the world be a more equitable, just, joyful, peaceful, or sustainable place based on the technology that you implemented?\n\n\nimport yfinance as yf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom yahoofinancials import YahooFinancials as YF\n\n\n# Define the ticker and the time period\nticker = 'TSLA'\nstart_date = '2019-01-01'\nend_date = '2020-01-01'\n\n# Fetch TSLA data\ntsla = yf.download(ticker, start=start_date, end=end_date)\n\nprint(tsla.head())\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n                 Open       High        Low      Close  Adj Close     Volume\nDate                                                                        \n2019-01-02  20.406668  21.008667  19.920000  20.674667  20.674667  174879000\n2019-01-03  20.466667  20.626667  19.825333  20.024000  20.024000  104478000\n2019-01-04  20.400000  21.200001  20.181999  21.179333  21.179333  110911500\n2019-01-07  21.448000  22.449333  21.183332  22.330667  22.330667  113268000\n2019-01-08  22.797333  22.934000  21.801332  22.356667  22.356667  105127500\n\n\n\n# Moving Average \nshort_window = 40\nlong_window = 100\n\ntsla['Short_MAvg'] = tsla['Close'].rolling(window=short_window, min_periods=1).mean()\ntsla['Long_MAvg'] = tsla['Close'].rolling(window=long_window, min_periods=1).mean()\n\n\n# Basic trading signal based on crossover \n\n# Create a 'Signal' column\ntsla['Signal'] = 0\ntsla['Signal'] = np.where(tsla['Short_MAvg'] &gt; tsla['Long_MAvg'], 1, 0)\n\n\n# Generate trading orders\ntsla['Position'] = tsla['Signal'].diff()\n\n\n# Basic Back Test\n\n# Plot the closing prices and moving averages\nplt.figure(figsize=(14,7))\nplt.plot(tsla['Close'], label='Close Price', alpha=0.5)\nplt.plot(tsla['Short_MAvg'], label=f'{short_window}-Day MA', alpha=0.75)\nplt.plot(tsla['Long_MAvg'], label=f'{long_window}-Day MA', alpha=0.75)\n\n# Plot buy signals\nplt.plot(tsla[tsla['Position'] == 1].index, tsla['Short_MAvg'][tsla['Position'] == 1], '^', markersize=10, color='g', lw=0, label='Buy Signal')\n\n# Plot sell signals\nplt.plot(tsla[tsla['Position'] == -1].index, tsla['Short_MAvg'][tsla['Position'] == -1], 'v', markersize=10, color='r', lw=0, label='Sell Signal')\n\nplt.title('AAPL Stock Price and Moving Average Crossovers')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\ntsla['Std_Dev'] = tsla['Close'].rolling(window=short_window, min_periods=1).std()\n\n# Calculate the z-score\ntsla['Z_Score'] = (tsla['Close'] - tsla['Short_MAvg']) / tsla['Std_Dev']\n\n\n# Define thresholds for buying and selling\nthreshold_buy = -1.5  # Buy signal threshold\nthreshold_sell = 1.5  # Sell signal threshold\n\n# Generate signals\ntsla['Signal'] = 0\ntsla['Signal'][tsla['Z_Score'] &gt; threshold_sell] = -1  # Sell signal\ntsla['Signal'][tsla['Z_Score'] &lt; threshold_buy] = 1  # Buy signal\n\n# Generate trading positions\ntsla['Position'] = tsla['Signal'].replace(0, np.nan).ffill().fillna(0)\n\nC:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_70572\\3954209935.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  tsla['Signal'][tsla['Z_Score'] &gt; threshold_sell] = -1  # Sell signal\nC:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_70572\\3954209935.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  tsla['Signal'][tsla['Z_Score'] &lt; threshold_buy] = 1  # Buy signal\n\n\n\n# Plot the results\nplt.figure(figsize=(14,7))\nplt.plot(tsla['Close'], label='Close Price', alpha=0.5)\nplt.plot(tsla['Short_MAvg'], label='Moving Average', alpha=0.75)\nplt.fill_between(tsla.index, tsla['Short_MAvg'] - tsla['Std_Dev'], tsla['Short_MAvg'] + tsla['Std_Dev'], color='gray', alpha=0.3, label='Standard Deviation Range')\n\n# Highlight the buy and sell signals\nplt.plot(tsla[tsla['Position'] == 1].index, tsla['Close'][tsla['Position'] == 1], '^', markersize=10, color='g', lw=0, label='Buy Signal')\nplt.plot(tsla[tsla['Position'] == -1].index, tsla['Close'][tsla['Position'] == -1], 'v', markersize=10, color='r', lw=0, label='Sell Signal')\n\nplt.title(f'{ticker} Stock Price and Mean Reversion Signals')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/cs-451-quant-project/axiao_research.html#model-analysis",
    "href": "posts/cs-451-quant-project/axiao_research.html#model-analysis",
    "title": "Applying Machine Learning to Stock Price Prediction",
    "section": "",
    "text": "from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\nticker = 'TSLA'\nstart_date = '2010-01-01'\nend_date = '2020-01-01'\n\nmodel = LogisticRegression()\n\ndata = yf.download(ticker, start=start_date, end=end_date)\n\n# Calculate moving averages and std\ndata['SMA_20'] = data['Close'].rolling(window=20).mean()\ndata['SMA_50'] = data['Close'].rolling(window=50).mean()\ndata['Std_Dev'] = data['Close'].rolling(window=20).std()\n\n# Calculate the z-score\ndata['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n\n# Calculate RSI\ndelta = data['Close'].diff()\nup = delta.clip(lower=0)\ndown = -1 * delta.clip(upper=0)\nema_up = up.ewm(com=13, adjust=False).mean()\nema_down = down.ewm(com=13, adjust=False).mean()\nrs = ema_up / ema_down\n\ndata['RSI'] = 100 - (100 / (1 + rs))\n\n# Calculate the daily returns\ndata['Returns'] = data['Close'].pct_change()\n\n# Drop any NaNs\ndata.dropna(inplace=True)\n\n# If stock price goes up or down\ndata['Target'] = (data['Returns'].shift(-1) &gt; 0).astype(int)\n\n# Wanted features for X and y\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Returns']\nX = data[features]\ny = data['Target']\n\n# Split data into first 80% and last 20%\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Calculate cumulative strategy returns on test data\nX_test['Predicted_Signal'] = y_pred\nX_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1)\ncumulative_strategy_returns = (X_test['Strategy_Returns'] + 1).cumprod()\n\n# Calculate cumulative returns for the market\nspy = yf.download('SPY', start=X_test.index[0].date().strftime('%Y-%m-%d'), end=end_date)\nspy['Returns'] = spy['Close'].pct_change()\ncumulative_market_returns = (spy['Returns'] + 1).cumprod()\n\nplt.figure(figsize=(10,5))\nplt.plot(cumulative_strategy_returns, label='Strategy Returns')\nplt.plot(cumulative_market_returns, label='Market Returns')\nplt.legend()\nplt.show()\n\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n\n\nAccuracy: 0.4946695095948827\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\ndef train(model, ticker, start_date, end_date):\n    data = yf.download(ticker, start=start_date, end=end_date)\n    \n    # Calculate moving averages and std\n    data['SMA_20'] = data['Close'].rolling(window=20).mean()\n    data['SMA_50'] = data['Close'].rolling(window=50).mean()\n    data['Std_Dev'] = data['Close'].rolling(window=20).std()\n\n    # Calculate the z-score\n    data['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n\n    # Calculate RSI\n    delta = data['Close'].diff()\n    up = delta.clip(lower=0)\n    down = -1 * delta.clip(upper=0)\n    ema_up = up.ewm(com=13, adjust=False).mean()\n    ema_down = down.ewm(com=13, adjust=False).mean()\n    rs = ema_up / ema_down\n\n    data['RSI'] = 100 - (100 / (1 + rs))\n\n    # Calculate the daily returns\n    data['Returns'] = data['Close'].pct_change()\n\n    # Drop any NaNs\n    data.dropna(inplace=True)\n\n    # If stock price goes up or down\n    data['Target'] = (data['Returns'].shift(-1) &gt; 0).astype(int)\n\n    # Wanted features for X and y\n    features = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Returns']\n    X = data[features]\n    y = data['Target']\n\n    # Split data into first 80% and last 20%\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy:\", accuracy)\n\n    # Calculate cumulative strategy returns on test data\n    X_test['Predicted_Signal'] = y_pred\n    X_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1)\n    cumulative_strategy_returns = (X_test['Strategy_Returns'] + 1).cumprod()\n\n    # Calculate cumulative returns for the market\n    spy = yf.download('SPY', start=X_test.index[0].date().strftime('%Y-%m-%d'), end=end_date)\n    spy['Returns'] = spy['Close'].pct_change()\n    cumulative_market_returns = (spy['Returns'] + 1).cumprod()\n    \n    plt.figure(figsize=(10,5))\n    plt.plot(cumulative_strategy_returns, label='Strategy Returns')\n    plt.plot(cumulative_market_returns, label='Market Returns')\n    plt.legend()\n    plt.show()\n\n\n\nCode\n%load_ext autoreload\n%autoreload 2\nfrom strategy import MeanReversion\n\n\n\nticker = 'TSLA'\nmarket = 'SPY'\nstart = '2014-01-01'\nend = '2024-01-01'\nMR = MeanReversion(ticker, start, end, market)\n\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n\n\n\nRF = RandomForestClassifier(n_estimators=100, random_state=42)\nX_test = MR.evaluate(model=RF)\n\nAccuracy: 0.5263157894736842\n\n\n\n\n\n\n\n\n\n\nLR = LogisticRegression()\nX_test = MR.evaluate(model=LR)\n\nAccuracy: 0.5041322314049587\n\n\n\n\n\n\n\n\n\n\nSVM = SVC()\nX_test = MR.evaluate(model = SVM)\n\nAccuracy: 0.5337552742616034\n\n\n\n\n\n\n\n\n\nNote: Different models work better for different stocks.\nBelow, we calculate the “risk-free rate,” used as a reference for all returns in financial markets. It is considered the return on an investment that is considered to have zero risk (short-term US treasuries). We use the risk-free rate to calculate the Sharpe ratio, which is a widely-used measure of an investment or a strategy’s “risk-adjusted” performance.\n\ndef deannualize(annual_rate, periods=365):\n    return (1 + annual_rate) ** (1/periods) - 1\n\ndef get_risk_free_rate(start_date, end_date):\n    # download 3-month us treasury bills rates\n    annualized = yf.download('^IRX', start_date, end_date)['Close']\n    annualized = annualized / 100\n    \n    # de-annualize\n    daily = annualized.apply(deannualize)\n\n    # create dataframe\n    return pd.DataFrame({\"annualized\": annualized, \"daily\": daily})"
  },
  {
    "objectID": "posts/cs-451-quant-project/axiao_research.html#pytrends",
    "href": "posts/cs-451-quant-project/axiao_research.html#pytrends",
    "title": "Applying Machine Learning to Stock Price Prediction",
    "section": "",
    "text": "from pytrends.request import TrendReq\nfrom pytrends import dailydata\n\n\npytrends = TrendReq(hl = 'en-US', tz=360)\n\n\npytrends.build_payload(kw_list=['Microsoft', 'Tesla', 'Apple'], timeframe='2010-01-01 2020-01-01')"
  },
  {
    "objectID": "posts/cs-451-quant-project/axiao_research.html#basket-analysis",
    "href": "posts/cs-451-quant-project/axiao_research.html#basket-analysis",
    "title": "Applying Machine Learning to Stock Price Prediction",
    "section": "",
    "text": "tickers = ['XOM', 'CVX', 'COP', 'NEE', 'SO', 'EOG', 'DUK', 'MPC', 'SLB', 'PSX']\n\ndef prepare_data(tickers, start_date, end_date):\n    '''\n    Combines data of all tickers into a single dataframe for X_train. X_test is a list of dataframes for each ticker.\n    '''\n    X_train_list = []\n    y_train_list = []\n    X_test_list = []\n    y_test_list = []\n    for t in tickers:\n        data = yf.download(t, start=start_date, end=end_date)\n        # Calculate moving averages and std\n        data['SMA_20'] = data['Close'].rolling(window=20).mean()\n        data['SMA_50'] = data['Close'].rolling(window=50).mean()\n        data['Std_Dev'] = data['Close'].rolling(window=20).std()\n\n        # Calculate the z-score\n        data['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n\n        # Calculate RSI\n        delta = data['Close'].diff()\n        up = delta.clip(lower=0)\n        down = -1 * delta.clip(upper=0)\n        ema_up = up.ewm(com=13, adjust=False).mean()\n        ema_down = down.ewm(com=13, adjust=False).mean()\n        rs = ema_up / ema_down\n\n        data['RSI'] = 100 - (100 / (1 + rs))\n\n        # Calculate the daily returns\n        data['Returns'] = data['Close'].pct_change()\n\n        # Drop any NaNs\n        data.dropna(inplace=True)\n\n        # If stock price goes up or down\n        data['Target'] = (data['Returns'].shift(-1) &gt; 0).astype(int)\n        data['Ticker'] = t\n        features = ['Ticker', 'SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Returns']\n        X = data[features]\n        y = data['Target']\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n        X_train_list.append(X_train)\n        y_train_list.append(y_train)\n        X_test_list.append(X_test)\n        y_test_list.append(y_test)\n\n    return pd.concat(X_train_list, ignore_index=True), pd.concat(y_train_list, ignore_index=True), X_test_list, y_test_list\n\n\ndef evaluate(model, X_test_, y_test_, features, market_data):\n    '''\n    Compares returns to the market for a single ticker.\n    '''\n    X_test = X_test_.copy()\n    y_test = y_test_.copy()\n    \n    y_pred = model.predict(X_test[features])\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"{X_test.Ticker.iloc[0]} Accuracy:\", accuracy)\n\n    # Calculate cumulative strategy returns on test data\n    X_test['Predicted_Signal'] = y_pred\n    X_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1)\n    cumulative_strategy_returns = (X_test['Strategy_Returns'].fillna(0) + 1).cumprod()\n    returns = X_test.loc[X_test.index, 'Returns']\n    returns.iloc[0] = 0\n    cumulative_stock_returns = (returns + 1).cumprod()\n\n    # Calculate cumulative returns for the market\n    market_data['Returns'] = market_data['Close'].pct_change()\n    #cumulative_market_returns = (market_data['Returns'].fillna(0) + 1).cumprod()\n\n    plt.figure(figsize=(10,5))\n    plt.plot(cumulative_strategy_returns, label='Strategy Returns', alpha=0.5)\n    #plt.plot(cumulative_market_returns, label='Market Returns')\n    plt.plot(cumulative_stock_returns, label='Stock Returns', alpha=0.5)\n    plt.title(f'{X_test.Ticker.iloc[0]} Returns')\n    plt.legend()\n    plt.show()\n\n    return X_test\n\n\nstart = '2014-01-01'\nend = '2024-01-01'\nX_train, y_train, X_test, y_test = prepare_data(tickers, start, end)\n\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n\n\n\nfrom sklearn.svm import LinearSVC\n\n#model = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel = SVC()\n#model = LogisticRegression()\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Returns']\nmodel = model.fit(X_train[features], y_train)\n\nmarket_data = yf.download('SPY', start=start, end=end)\nX_test_vec = []\n\nfor i in range(10):\n    market = market_data[market_data.index &gt;= X_test[i].index[0]].copy()\n    X_test_vec.append(evaluate(model, X_test[i], y_test[i], features, market_data=market))\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nXOM Accuracy: 0.52834008097166\nCVX Accuracy: 0.48380566801619435\nCOP Accuracy: 0.5323886639676113\nNEE Accuracy: 0.48380566801619435\nSO Accuracy: 0.4939271255060729\nEOG Accuracy: 0.5425101214574899\nDUK Accuracy: 0.5182186234817814\nMPC Accuracy: 0.5303643724696356\nSLB Accuracy: 0.520242914979757\nPSX Accuracy: 0.5161943319838057\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX_test_vec[0]['Predicted_Signal'].mean()\n\n0.5040485829959515"
  },
  {
    "objectID": "posts/cs-451-quant-project/axiao_research.html#feature-selection",
    "href": "posts/cs-451-quant-project/axiao_research.html#feature-selection",
    "title": "Applying Machine Learning to Stock Price Prediction",
    "section": "",
    "text": "from sklearn.feature_selection import RFECV\nfrom sklearn.svm import LinearSVC\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI']\nrfe = RFECV(LinearSVC(dual='auto'), cv = 5)\n\n\nrfe = rfe.fit(X_train[features], y_train)\n\n\nrfe.ranking_\n\narray([2, 3, 1, 1, 4])\n\n\n\nmarket_data = yf.download('SPY', start=start, end=end)\nmarket = market_data[market_data.index &gt;= X_test[i].index[0]].copy()\nX_test_vec = []\nfor i in range(10):\n    X_test_vec.append(evaluate(rfe, X_test[i], y_test[i], features, market_data=market))\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nXOM Accuracy: 0.5182186234817814\nCVX Accuracy: 0.46963562753036436\nCOP Accuracy: 0.5222672064777328\nNEE Accuracy: 0.4898785425101215\nSO Accuracy: 0.4939271255060729\nEOG Accuracy: 0.46963562753036436\nDUK Accuracy: 0.5141700404858299\nMPC Accuracy: 0.5323886639676113\nSLB Accuracy: 0.5242914979757085\nPSX Accuracy: 0.5323886639676113"
  },
  {
    "objectID": "posts/cs-451-quant-project/axiao_research.html#lstm",
    "href": "posts/cs-451-quant-project/axiao_research.html#lstm",
    "title": "Applying Machine Learning to Stock Price Prediction",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nfrom torch.autograd import Variable \n\n\n%load_ext autoreload\n%autoreload 2\nfrom lstm_model import LSTMModel\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nticker = 'XOM'\nstart_date = '2014-01-01'\nend_date = '2024-01-01'\n\n#model = LogisticRegression()\n\ndata = yf.download(ticker, start=start_date, end=end_date)\n\n# Calculate moving averages and std\ndata['SMA_20'] = data['Close'].rolling(window=20).mean()\ndata['SMA_50'] = data['Close'].rolling(window=50).mean()\ndata['Std_Dev'] = data['Close'].rolling(window=20).std()\n\n# Calculate the z-score\ndata['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n\n# Calculate RSI\ndelta = data['Close'].diff()\nup = delta.clip(lower=0)\ndown = -1 * delta.clip(upper=0)\nema_up = up.ewm(com=13, adjust=False).mean()\nema_down = down.ewm(com=13, adjust=False).mean()\nrs = ema_up / ema_down\n\ndata['RSI'] = 100 - (100 / (1 + rs))\n\n# Calculate the daily returns\ndata['Returns'] = data['Close'].pct_change()\n\n# If stock price goes up or down\ndata['Target'] = data['Close'].shift(-1)\n\ndata.dropna(inplace=True)\n\n# Wanted features for X and y\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Close', 'Returns']\nX = data.loc[:, features]\ny = data.iloc[:, (data.shape[1]-1):(data.shape[1])]\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nmm1 = MinMaxScaler()\nss1 = StandardScaler()\n# mm2 = MinMaxScaler()\n# ss2 = StandardScaler()\n\nX_ss = pd.DataFrame(ss1.fit_transform(X), index=X.index, columns=X.columns)\ny_mm = pd.DataFrame(mm1.fit_transform(y), index=y.index, columns=y.columns)\n\n# Split data into first 80% and last 20%\nX_train, X_test, y_train, y_test = train_test_split(X_ss, y_mm, test_size=0.2, random_state=42, shuffle=False)\n# X_train_ss = pd.DataFrame(ss1.fit_transform(X_train), index=X_train.index, columns=X.columns)\n# y_train_mm = pd.DataFrame(mm1.fit_transform(y_train), index=y_train.index, columns=y.columns)\n# X_test_ss = pd.DataFrame(ss2.fit_transform(X_test), index=X_test.index, columns=X.columns)\n# y_test_mm = pd.DataFrame(mm2.fit_transform(y_test), index=y_test.index, columns=y.columns)\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\nfeatures_ = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI']\nX_train_tensors = Variable(torch.Tensor(np.array(X_train[features_])))\nX_test_tensors = Variable(torch.Tensor(np.array(X_test[features_])))\n\ny_train_tensors = Variable(torch.Tensor(y_train.values))\ny_test_tensors = Variable(torch.Tensor(y_test.values))\n\nX_train_final = torch.reshape(X_train_tensors,   (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\nX_test_final = torch.reshape(X_test_tensors,  (X_test_tensors.shape[0], 1, X_test_tensors.shape[1]))\n\n\nprint(\"Training Shape\", X_train_final.shape, y_train_tensors.shape)\nprint(\"Testing Shape\", X_test_final.shape, y_test_tensors.shape) \n\nTraining Shape torch.Size([1972, 1, 5]) torch.Size([1972, 1])\nTesting Shape torch.Size([494, 1, 5]) torch.Size([494, 1])\n\n\n\nnum_epochs = 1000 #1000 epochs\nlearning_rate = 0.001 #0.001 lr\n\ninput_size = 5 #number of features\nhidden_size = 2 #number of features in hidden state\nnum_layers = 1 #number of stacked lstm layers\n\nnum_classes = 1 #number of output classes \n\n\nlstm = LSTMModel(num_classes, input_size, hidden_size, num_layers, X_train_final.shape[1]) #our lstm class \n\n\ncriterion = torch.nn.MSELoss()    # mean-squared error for regression\noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n\n\nfor epoch in range(num_epochs):\n  outputs = lstm.forward(X_train_final) #forward pass\n  optimizer.zero_grad() #calculate the gradient, manually setting to 0\n \n  # obtain the loss function\n  loss = criterion(outputs, y_train_tensors)\n \n  loss.backward() #calculates the loss of the loss function\n \n  optimizer.step() #improve from loss, i.e backprop\n  if epoch % 100 == 0:\n    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item())) \n\nEpoch: 0, loss: 0.16941\nEpoch: 100, loss: 0.01794\nEpoch: 200, loss: 0.00689\nEpoch: 300, loss: 0.00291\nEpoch: 400, loss: 0.00080\nEpoch: 500, loss: 0.00038\nEpoch: 600, loss: 0.00029\nEpoch: 700, loss: 0.00026\nEpoch: 800, loss: 0.00023\nEpoch: 900, loss: 0.00022\n\n\n\ntrain_predict = lstm(X_train_final)\ndata_predict = train_predict.data.numpy()\n\n\ndata_predict = mm1.inverse_transform(data_predict) #reverse transformation\n\n\nX_train = pd.DataFrame(ss1.inverse_transform(X_train), index=X_train.index, columns=features)\n\n\nX_train['Predicted_Price'] = data_predict\nX_train['Actual_Signal'] = (X_train['Returns'].shift(-1) &gt; 0).astype(int)\nX_train['Predicted_Returns'] = X_train['Predicted_Price'].pct_change()\nX_train['Predicted_Signal'] = (X_train['Predicted_Returns'].shift(-1) &gt; 0).astype(int)\nX_train['Strategy_Returns'] = X_train['Returns'] * X_train['Predicted_Signal'].shift(1)\ncumulative_strategy_returns = (X_train['Strategy_Returns'].fillna(0) + 1).cumprod()\nreturns = X_train.loc[X_train.index, 'Returns']\nreturns.iloc[0] = 0\ncumulative_stock_returns = (returns + 1).cumprod()\naccuracy = (X_train['Actual_Signal'] == X_train['Predicted_Signal']).mean()\nprint(f'Accuracy: {accuracy}')\n# Calculate cumulative returns for the market\n#market_data['Returns'] = market_data['Close'].pct_change()\n#cumulative_market_returns = (market_data['Returns'].fillna(0) + 1).cumprod()\n\nplt.figure(figsize=(10,5))\nplt.plot(cumulative_strategy_returns, label='Strategy Returns', alpha=0.5)\n#plt.plot(cumulative_market_returns, label='Market Returns')\nplt.plot(cumulative_stock_returns, label='Stock Returns', alpha=0.5)\nplt.legend();\n\nAccuracy: 0.9523326572008114\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10,5))\nplt.plot(X_train['Predicted_Price'], label='Predicted Price')\nplt.plot(X_train['Close'], label='Actual Price')\nplt.legend();\n\n\n\n\n\n\n\n\n\ntest_predict = lstm(X_test_final)\ndata_predict = test_predict.data.numpy()\n\n\ndata_predict = mm1.inverse_transform(data_predict)\nX_test = pd.DataFrame(ss1.inverse_transform(X_test), index=X_test.index, columns=features)\n\n\nX_test['Actual_Signal'] = (X_test['Returns'].shift(-1) &gt; 0).astype(int)\nX_test['Predicted_Price'] = data_predict\nX_test['Predicted_Returns'] = X_test['Predicted_Price'].pct_change()\nX_test['Predicted_Signal'] = (X_test['Predicted_Returns'].shift(-1) &gt; 0).astype(int)\nX_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1)\ncumulative_strategy_returns = (X_test['Strategy_Returns'].fillna(0) + 1).cumprod()\nreturns = X_test.loc[X_test.index, 'Returns']\nreturns.iloc[0] = 0\ncumulative_stock_returns = (returns + 1).cumprod()\naccuracy = (X_test['Actual_Signal'] == X_test['Predicted_Signal']).mean()\nprint(f'Accuracy: {accuracy}')\n# Calculate cumulative returns for the market\n#market_data['Returns'] = market_data['Close'].pct_change()\n#cumulative_market_returns = (market_data['Returns'].fillna(0) + 1).cumprod()\n\nplt.figure(figsize=(10,5))\n\nplt.plot(cumulative_strategy_returns, label='Strategy Returns')\n#plt.plot(cumulative_market_returns, label='Market Returns')\nplt.plot(cumulative_stock_returns, label='Stock Returns')\nplt.legend();\n\nAccuracy: 0.9473684210526315\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10,5))\nplt.plot(X_test['Predicted_Price'], label='Predicted Price')\nplt.plot(X_test['Close'], label='Actual Price')\nplt.legend();"
  },
  {
    "objectID": "posts/cs-451-quant-project/prelim-analysis.html",
    "href": "posts/cs-451-quant-project/prelim-analysis.html",
    "title": "Donovan's Awesome CSCI 0451 Blog",
    "section": "",
    "text": "#Importing the data and storing it in variable df_all\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_all = pd.read_csv(\"crox-early-data.csv\")\n\ndf_all = df_all.dropna()\n\ndf_train, df_test = train_test_split(df_all, test_size=0.3, random_state=1)\nX_train = df_train.drop(columns=['Close Higher'])\ny_train = df_train['Close Higher']\n\nX_test = df_test.drop(columns=['Close Higher'])\ny_test = df_test['Close Higher']\n\nX_train.head()\n\n\ndf_all.groupby(\"Close Higher\").mean(numeric_only=True)\n\n\n#Choosing features\nfrom sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression()\n\ncols = [\"Previous GT\", \"TTM PE\"]\n\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\nLR.coef_[0]\n\n\ndef linear_score(w, x0, x1):\n    return w[0]*x0 + w[1]*x1\n\n\n#Predict makes binary predictions for data using a supplied score function with weights w and a supplied threshold. Taken from lecture notes from week 2.\n#We begin with a 0 threshold but later on test others to find an optimal threshold\n\nt = 0\n\ndef predict(score_fun, w, threshold, df):\n    \"\"\"\n    make binary predictions for data df using a supplied score function with weights w and supplied threshold. \n    \"\"\"\n    scores = score_fun(w, df[\"Previous GT\"], df[\"TTM PE\"])\n    return 1*(scores &gt; threshold)\n\ndf_train[\"decision\"] = predict(linear_score, LR.coef_[0], t, df_all)\n(df_train[\"decision\"] == df_train[\"Close Higher\"]).mean()\n\n\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\niterations = 200\npredictions = []\nfor i in range(iterations):\n    threshold = (-iterations/2)+(i)\n    df_train[\"decision\"] = predict(linear_score, LR.coef_[0], threshold, df_train)\n    predictions.append((threshold, (df_train[\"decision\"] == df_train[\"Close Higher\"]).mean()))\n\n\npredictions_df = pd.DataFrame(data=predictions)\npredictions_df.columns =['Threshold', 'Accuracy']\n\nsns.relplot(data=predictions_df, x=\"Threshold\", y=\"Accuracy\")\n\nt = predictions_df['Threshold'][predictions_df['Accuracy'].idxmax()]\n\npredictions_df['Threshold'][predictions_df['Accuracy'].idxmax()], predictions_df['Accuracy'].max()\n\n\ndf_test[\"decision\"] = predict(linear_score, LR.coef_[0], t, df_test)\n(df_test[\"decision\"] == df_test[\"Close Higher\"]).mean()"
  },
  {
    "objectID": "posts/cs-451-quant-project/dwood_test.html#donovan-test-code-just-to-get-started",
    "href": "posts/cs-451-quant-project/dwood_test.html#donovan-test-code-just-to-get-started",
    "title": "Donovan test code just to get started",
    "section": "Donovan test code just to get started",
    "text": "Donovan test code just to get started\n\nimport yfinance as yf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# Define the ticker and the time period\nticker = 'AAPL'\nstart_date = '2019-01-01'\nend_date = '2020-01-01'\n\n# Fetch data\ndata = yf.download(ticker, start=start_date, end=end_date)\nprint(data.head())\n\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n                 Open       High        Low      Close  Adj Close     Volume\nDate                                                                        \n2019-01-02  38.722500  39.712502  38.557499  39.480000  37.845047  148158800\n2019-01-03  35.994999  36.430000  35.500000  35.547501  34.075401  365248800\n2019-01-04  36.132500  37.137501  35.950001  37.064999  35.530048  234428400\n2019-01-07  37.174999  37.207500  36.474998  36.982498  35.450970  219111200\n2019-01-08  37.389999  37.955002  37.130001  37.687500  36.126774  164101200\n\n\n\n# Moving Average \n\nshort_window = 40\nlong_window = 100\n\ndata['Short_MAvg'] = data['Close'].rolling(window=short_window, min_periods=1).mean()\ndata['Long_MAvg'] = data['Close'].rolling(window=long_window, min_periods=1).mean()\n\n\n# Basic trading signal based on crossover \n\n# Create a 'Signal' column\ndata['Signal'] = 0\ndata['Signal'] = np.where(data['Short_MAvg'] &gt; data['Long_MAvg'], 1, 0)\n\n# Generate trading orders\ndata['Position'] = data['Signal'].diff()\n\n\n# Basic Back Test\n\n# Plot the closing prices and moving averages\nplt.figure(figsize=(14,7))\nplt.plot(data['Close'], label='Close Price', alpha=0.5)\nplt.plot(data['Short_MAvg'], label=f'{short_window}-Day MA', alpha=0.75)\nplt.plot(data['Long_MAvg'], label=f'{long_window}-Day MA', alpha=0.75)\n\n# Plot buy signals\nplt.plot(data[data['Position'] == 1].index, data['Short_MAvg'][data['Position'] == 1], '^', markersize=10, color='g', lw=0, label='Buy Signal')\n\n# Plot sell signals\nplt.plot(data[data['Position'] == -1].index, data['Short_MAvg'][data['Position'] == -1], 'v', markersize=10, color='r', lw=0, label='Sell Signal')\n\nplt.title('AAPL Stock Price and Moving Average Crossovers')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nMessing around with mean reversion\n\nimport yfinance as yf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nticker = 'AAPL'\nstart_date = '2019-01-01'\nend_date = '2020-01-01'\n\n\ndata = yf.download(ticker, start=start_date, end=end_date)\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n# Calculate the moving average and the standard deviation\nwindow = 20\ndata['Moving_Average'] = data['Close'].rolling(window=window).mean()\ndata['Std_Dev'] = data['Close'].rolling(window=window).std()\n\n# Calculate the z-score\ndata['Z_Score'] = (data['Close'] - data['Moving_Average']) / data['Std_Dev']\n\n# Define thresholds for buying and selling\nthreshold_buy = -1.5  # Buy signal threshold\nthreshold_sell = 1.5  # Sell signal threshold\n\n# Generate signals\ndata['Signal'] = 0\ndata['Signal'][data['Z_Score'] &gt; threshold_sell] = -1  # Sell signal\ndata['Signal'][data['Z_Score'] &lt; threshold_buy] = 1  # Buy signal\n\n# Generate trading positions\ndata['Position'] = data['Signal'].replace(0, np.nan).ffill().fillna(0)\n\n/var/folders/6z/mh947m1s1m17ylsnt3y4rkfc0000gn/T/ipykernel_33078/834088593.py:15: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data['Signal'][data['Z_Score'] &gt; threshold_sell] = -1  # Sell signal\n/var/folders/6z/mh947m1s1m17ylsnt3y4rkfc0000gn/T/ipykernel_33078/834088593.py:16: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data['Signal'][data['Z_Score'] &lt; threshold_buy] = 1  # Buy signal\n\n\n\n# Plot the results\nplt.figure(figsize=(14,7))\nplt.plot(data['Close'], label='Close Price', alpha=0.5)\nplt.plot(data['Moving_Average'], label='Moving Average', alpha=0.75)\nplt.fill_between(data.index, data['Moving_Average'] - data['Std_Dev'], data['Moving_Average'] + data['Std_Dev'], color='gray', alpha=0.3, label='Standard Deviation Range')\n\n# Highlight the buy and sell signals\nplt.plot(data[data['Position'] == 1].index, data['Close'][data['Position'] == 1], '^', markersize=10, color='g', lw=0, label='Buy Signal')\nplt.plot(data[data['Position'] == -1].index, data['Close'][data['Position'] == -1], 'v', markersize=10, color='r', lw=0, label='Sell Signal')\n\nplt.title('AAPL Stock Price and Mean Reversion Signals')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNow will actually do some machine learning stuff. Add some random features and try to make it work\n\nimport yfinance as yf\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n#Trying random forest here just for fun, could use LR or something else too\n\n\nticker = 'AAPL'\ndata = yf.download(ticker, start=\"2010-01-01\", end=\"2020-01-01\")\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n# Calculate moving averages\ndata['SMA_20'] = data['Close'].rolling(window=20).mean()\ndata['SMA_50'] = data['Close'].rolling(window=50).mean()\n\n# Calculate RSI\ndelta = data['Close'].diff()\nup = delta.clip(lower=0)\ndown = -1 * delta.clip(upper=0)\nema_up = up.ewm(com=13, adjust=False).mean()\nema_down = down.ewm(com=13, adjust=False).mean()\nrs = ema_up / ema_down\n\ndata['RSI'] = 100 - (100 / (1 + rs))\n\n# Calculate the daily returns\ndata['Returns'] = data['Close'].pct_change()\n\n# Drop any NaNs\ndata.dropna(inplace=True)\n\n\n# Data if it goes up or down\n\ndata['Target'] = (data['Returns'].shift(-1) &gt; 0).astype(int)\n\n\nfeatures = ['SMA_20', 'SMA_50', 'RSI', 'Returns']\nX = data[features]\ny = data['Target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\nRandomForestClassifier(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(random_state=42)\n\n\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\nAccuracy: 0.5080971659919028\n\n\n\ndata['Predicted_Signal'] = model.predict(X)\ndata['Strategy_Returns'] = data['Returns'] * data['Predicted_Signal'].shift(1)\ncumulative_strategy_returns = (data['Strategy_Returns'] + 1).cumprod()\n\nplt.figure(figsize=(10,5))\nplt.plot(cumulative_strategy_returns, label='Strategy Returns')\nplt.legend()\nplt.show()\n\n# Not fully correct but good starting point. Just wanted to start familiarizing myself with the library and the data."
  },
  {
    "objectID": "posts/cs-451-quant-project/lstm_research.html",
    "href": "posts/cs-451-quant-project/lstm_research.html",
    "title": "LSTM",
    "section": "",
    "text": "Source: [https://cnvrg.io/pytorch-lstm/]\nBasically a NN for sequential data.\nYou’ll see below that for the testing data, the predicted stock prices aren’t super accurate but the model identifies the trend of the stock pretty well.\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nfrom torch.autograd import Variable \nimport matplotlib.pyplot as plt\nimport yfinance as yf\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom dateutil.relativedelta import relativedelta\n\n\n%load_ext autoreload\n%autoreload 2\nfrom lstm_model import LSTMModel\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\nticker = 'XOM'\nstart_date = '2014-01-01'\nend_date = '2024-01-01'\n\n# data = yf.download(ticker, start=start_date, end=end_date)\nticker_data = yf.Ticker(ticker)\ndata = ticker_data.history(start=start_date, end=end_date)\n\n# Calculate the daily returns\ndata['Returns'] = data['Close'].pct_change()\n\n# Calculate moving averages and std\ndata['SMA_10'] = data['Close'].rolling(window=10).mean()\ndata['SMA_20'] = data['Close'].rolling(window=20).mean()\ndata['SMA_50'] = data['Close'].rolling(window=50).mean()\ndata['SMA_100'] = data['Close'].rolling(window=100).mean()\ndata['Std_Dev'] = data['Close'].rolling(window=20).std()\n\n# Calculate the z-score\ndata['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n\n# Calculate RSI\ndelta = data['Close'].diff()\nup = delta.clip(lower=0)\ndown = -1 * delta.clip(upper=0)\nema_up = up.ewm(com=13, adjust=False).mean()\nema_down = down.ewm(com=13, adjust=False).mean()\nrs = ema_up / ema_down\n\ndata['RSI'] = 100 - (100 / (1 + rs))\n\n# Calculate TTM EPS and P/E\neps = ticker_data.get_earnings_dates(limit=60)\neps = eps[(eps.index &gt;= (data.index[0]-relativedelta(years=1))) & (eps.index &lt;= data.index[-1])]\neps = eps.iloc[::-1]\neps['TTM'] = eps['Reported EPS'].rolling(window=4).sum()\neps.index = eps.index.date\nidx = pd.date_range(eps.index[0], eps.index[-1])\neps = eps.reindex(idx.date, fill_value=np.nan)\ndata.index = data.index.date\ndata['TTM_EPS'] = eps['TTM'].copy()\ndata[data['TTM_EPS'].notna()]\ndata['TTM_EPS'] = data['TTM_EPS'].ffill()\ndata['TTM_EPS'] = data['TTM_EPS'].fillna(eps['TTM'].loc[eps['TTM'].notna()].iloc[0])\ndata['TTM_P/E'] = data['Close'] / data['TTM_EPS']\n\n# If stock price goes up or down\ndata['Target'] = data['Close'].shift(-1)\n\ndata.dropna(inplace=True)\n\n# Wanted features for X and y\nfeatures = ['Close', 'Returns', 'SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'TTM_EPS', 'TTM_P/E']\nX = data.loc[:, data.columns != 'Target']\ny = data.iloc[:, (data.shape[1]-1):(data.shape[1])]\n\n\nX.head()\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\nDividends\nStock Splits\nReturns\nSMA_10\nSMA_20\nSMA_50\nSMA_100\nStd_Dev\nZ_Score\nRSI\nTTM_EPS\nTTM_P/E\n\n\n\n\n2014-05-27\n66.392317\n66.581804\n65.948002\n66.228966\n8359700\n0.0\n0.0\n0.000395\n66.249862\n66.348911\n64.490827\n63.077422\n0.407439\n-0.294387\n55.976566\n7.35\n9.010744\n\n\n2014-05-28\n66.144018\n66.692875\n66.026406\n66.032936\n7586700\n0.0\n0.0\n-0.002960\n66.164919\n66.358520\n64.587218\n63.094929\n0.396955\n-0.820204\n53.417316\n7.35\n8.984073\n\n\n2014-05-29\n66.346549\n66.418424\n66.006777\n66.170128\n6113700\n0.0\n0.0\n0.002078\n66.098270\n66.343837\n64.681291\n63.115356\n0.398285\n-0.436143\n54.969020\n7.35\n9.002738\n\n\n2014-05-30\n65.863060\n66.091750\n65.582096\n65.686638\n11499000\n0.0\n0.0\n-0.007307\n66.081938\n66.337431\n64.778413\n63.129980\n0.408152\n-1.594486\n48.799597\n7.35\n8.936958\n\n\n2014-06-02\n65.686622\n65.915313\n65.124695\n65.301117\n7761300\n0.0\n0.0\n-0.005869\n66.029665\n66.292278\n64.856793\n63.131664\n0.469074\n-2.113013\n44.509885\n7.35\n8.884506\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nss1 = StandardScaler()\nss2 = StandardScaler()\n# mm2 = MinMaxScaler()\n# ss2 = StandardScaler()\n\n# X_ss = pd.DataFrame(ss.fit_transform(X), index=X.index, columns=X.columns)\n# y_mm = pd.DataFrame(mm.fit_transform(y), index=y.index, columns=y.columns)\n\n# Split data into first 80% and last 20%\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, shuffle=False)\nX_train_ss = pd.DataFrame(ss1.fit_transform(X_train), index=X_train.index, columns=X.columns) # fit ss and transform X_train\ny_train_mm = pd.DataFrame(ss2.fit_transform(y_train), index=y_train.index, columns=y.columns) # fit mm and transform y_train\nX_test_ss = pd.DataFrame(ss1.transform(X_test), index=X_test.index, columns=X.columns) # transform X_test with fitted ss\ny_test_mm = pd.DataFrame(ss2.transform(y_test), index=y_test.index, columns=y.columns) # transform y_test with fitted mm\ntrain = pd.concat([X_train_ss, y_train_mm], axis=1)\ntest = pd.concat([X_test_ss, y_test_mm], axis=1)"
  },
  {
    "objectID": "posts/cs-451-quant-project/lstm_research.html#lstm",
    "href": "posts/cs-451-quant-project/lstm_research.html#lstm",
    "title": "LSTM",
    "section": "",
    "text": "Source: [https://cnvrg.io/pytorch-lstm/]\nBasically a NN for sequential data.\nYou’ll see below that for the testing data, the predicted stock prices aren’t super accurate but the model identifies the trend of the stock pretty well.\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nfrom torch.autograd import Variable \nimport matplotlib.pyplot as plt\nimport yfinance as yf\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom dateutil.relativedelta import relativedelta\n\n\n%load_ext autoreload\n%autoreload 2\nfrom lstm_model import LSTMModel\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\nticker = 'XOM'\nstart_date = '2014-01-01'\nend_date = '2024-01-01'\n\n# data = yf.download(ticker, start=start_date, end=end_date)\nticker_data = yf.Ticker(ticker)\ndata = ticker_data.history(start=start_date, end=end_date)\n\n# Calculate the daily returns\ndata['Returns'] = data['Close'].pct_change()\n\n# Calculate moving averages and std\ndata['SMA_10'] = data['Close'].rolling(window=10).mean()\ndata['SMA_20'] = data['Close'].rolling(window=20).mean()\ndata['SMA_50'] = data['Close'].rolling(window=50).mean()\ndata['SMA_100'] = data['Close'].rolling(window=100).mean()\ndata['Std_Dev'] = data['Close'].rolling(window=20).std()\n\n# Calculate the z-score\ndata['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n\n# Calculate RSI\ndelta = data['Close'].diff()\nup = delta.clip(lower=0)\ndown = -1 * delta.clip(upper=0)\nema_up = up.ewm(com=13, adjust=False).mean()\nema_down = down.ewm(com=13, adjust=False).mean()\nrs = ema_up / ema_down\n\ndata['RSI'] = 100 - (100 / (1 + rs))\n\n# Calculate TTM EPS and P/E\neps = ticker_data.get_earnings_dates(limit=60)\neps = eps[(eps.index &gt;= (data.index[0]-relativedelta(years=1))) & (eps.index &lt;= data.index[-1])]\neps = eps.iloc[::-1]\neps['TTM'] = eps['Reported EPS'].rolling(window=4).sum()\neps.index = eps.index.date\nidx = pd.date_range(eps.index[0], eps.index[-1])\neps = eps.reindex(idx.date, fill_value=np.nan)\ndata.index = data.index.date\ndata['TTM_EPS'] = eps['TTM'].copy()\ndata[data['TTM_EPS'].notna()]\ndata['TTM_EPS'] = data['TTM_EPS'].ffill()\ndata['TTM_EPS'] = data['TTM_EPS'].fillna(eps['TTM'].loc[eps['TTM'].notna()].iloc[0])\ndata['TTM_P/E'] = data['Close'] / data['TTM_EPS']\n\n# If stock price goes up or down\ndata['Target'] = data['Close'].shift(-1)\n\ndata.dropna(inplace=True)\n\n# Wanted features for X and y\nfeatures = ['Close', 'Returns', 'SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'TTM_EPS', 'TTM_P/E']\nX = data.loc[:, data.columns != 'Target']\ny = data.iloc[:, (data.shape[1]-1):(data.shape[1])]\n\n\nX.head()\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\nDividends\nStock Splits\nReturns\nSMA_10\nSMA_20\nSMA_50\nSMA_100\nStd_Dev\nZ_Score\nRSI\nTTM_EPS\nTTM_P/E\n\n\n\n\n2014-05-27\n66.392317\n66.581804\n65.948002\n66.228966\n8359700\n0.0\n0.0\n0.000395\n66.249862\n66.348911\n64.490827\n63.077422\n0.407439\n-0.294387\n55.976566\n7.35\n9.010744\n\n\n2014-05-28\n66.144018\n66.692875\n66.026406\n66.032936\n7586700\n0.0\n0.0\n-0.002960\n66.164919\n66.358520\n64.587218\n63.094929\n0.396955\n-0.820204\n53.417316\n7.35\n8.984073\n\n\n2014-05-29\n66.346549\n66.418424\n66.006777\n66.170128\n6113700\n0.0\n0.0\n0.002078\n66.098270\n66.343837\n64.681291\n63.115356\n0.398285\n-0.436143\n54.969020\n7.35\n9.002738\n\n\n2014-05-30\n65.863060\n66.091750\n65.582096\n65.686638\n11499000\n0.0\n0.0\n-0.007307\n66.081938\n66.337431\n64.778413\n63.129980\n0.408152\n-1.594486\n48.799597\n7.35\n8.936958\n\n\n2014-06-02\n65.686622\n65.915313\n65.124695\n65.301117\n7761300\n0.0\n0.0\n-0.005869\n66.029665\n66.292278\n64.856793\n63.131664\n0.469074\n-2.113013\n44.509885\n7.35\n8.884506\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nss1 = StandardScaler()\nss2 = StandardScaler()\n# mm2 = MinMaxScaler()\n# ss2 = StandardScaler()\n\n# X_ss = pd.DataFrame(ss.fit_transform(X), index=X.index, columns=X.columns)\n# y_mm = pd.DataFrame(mm.fit_transform(y), index=y.index, columns=y.columns)\n\n# Split data into first 80% and last 20%\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, shuffle=False)\nX_train_ss = pd.DataFrame(ss1.fit_transform(X_train), index=X_train.index, columns=X.columns) # fit ss and transform X_train\ny_train_mm = pd.DataFrame(ss2.fit_transform(y_train), index=y_train.index, columns=y.columns) # fit mm and transform y_train\nX_test_ss = pd.DataFrame(ss1.transform(X_test), index=X_test.index, columns=X.columns) # transform X_test with fitted ss\ny_test_mm = pd.DataFrame(ss2.transform(y_test), index=y_test.index, columns=y.columns) # transform y_test with fitted mm\ntrain = pd.concat([X_train_ss, y_train_mm], axis=1)\ntest = pd.concat([X_test_ss, y_test_mm], axis=1)"
  },
  {
    "objectID": "posts/cs-451-quant-project/lstm_research.html#prepare-data-for-lstm",
    "href": "posts/cs-451-quant-project/lstm_research.html#prepare-data-for-lstm",
    "title": "LSTM",
    "section": "Prepare Data for LSTM",
    "text": "Prepare Data for LSTM\nConverts the Numpy Arrays to Tensors and to Variables (which can be differentiated).\n\nfeatures_ = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Close', 'TTM_P/E'] # Features to train with.\nX_train_tensors = Variable(torch.Tensor(np.array(X_train_ss[features_])))\nX_test_tensors = Variable(torch.Tensor(np.array(X_test_ss[features_])))\n\ny_train_tensors = Variable(torch.Tensor(y_train_mm.values))\ny_test_tensors = Variable(torch.Tensor(y_test_mm.values))\n\n\ndef create_dataset(X_data, y_data, lookback):\n    \"\"\"Transform a time series into a prediction dataset\n    \n    Args:\n        dataset: A numpy array of time series, first dimension is the time steps\n        lookback: Size of window for prediction\n    \"\"\"\n    X, y = [], []\n    for i in range(len(X_data)-lookback):\n        feature = X_data[i:i+lookback]\n        target = y_data[i+1:i+lookback+1]\n        X.append(feature)\n        y.append(target)\n    return torch.stack(X), torch.stack(y)\n\n\n(create_dataset(X_train_tensors, y_train_tensors, 1))[1].shape\n\ntorch.Size([2173, 1, 1])\n\n\n\nimport torch\nfrom torch.utils.data import Dataset\n\nclass SequenceDataset(Dataset):\n    def __init__(self, dataframe, target, features, sequence_length=5):\n        self.features = features\n        self.target = target\n        self.sequence_length = sequence_length\n        self.y = torch.tensor(dataframe[target].values).float()\n        self.X = torch.tensor(dataframe[features].values).float()\n\n    def __len__(self):\n        return self.X.shape[0]\n\n    def __getitem__(self, i): \n        if i &gt;= self.sequence_length - 1:\n            i_start = i - self.sequence_length + 1\n            x = self.X[i_start:(i + 1), :]\n        else:\n            padding = self.X[0].repeat(self.sequence_length - i - 1, 1)\n            x = self.X[0:(i + 1), :]\n            x = torch.cat((padding, x), 0)\n\n        return x, self.y[i]\n\n\n# train_dataset = SequenceDataset(\n#     df_train_ss,\n#     target='Target',\n#     features=features_,\n#     sequence_length=30\n# )\n\n\n# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=6, shuffle=True)\n# #test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n\n# X, y = next(iter(train_loader))\n\n# print(\"Features shape:\", X.shape)\n# print(\"Target shape:\", y.shape)\n\n\nTraining\n\nX_train_final, y_train_final = create_dataset(X_train_tensors, y_train_tensors, lookback=1)\nX_test_final, y_test_final = create_dataset(X_test_tensors, y_test_tensors, lookback=1)\nprint(\"Training Shape\", X_train_final.shape, y_train_final.shape)\nprint(\"Testing Shape\", X_test_final.shape, y_test_final.shape) \n\nTraining Shape torch.Size([2173, 1, 7]) torch.Size([2173, 1, 1])\nTesting Shape torch.Size([241, 1, 7]) torch.Size([241, 1, 1])\n\n\n\nnum_epochs = 1000 # 1000 epochs\nlearning_rate = 0.001 # 0.001 lr\n\ninput_size = X_train_final.shape[2] # number of features\nhidden_size = 32 # number of features in hidden state\nnum_layers = 1 # number of stacked lstm layers\nbatch_size = X_train_final.shape[0]\nlookback = 1\n\nnum_classes = 1 #number of output classes\n\n\ndata_loader_train = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train_final, y_train_final), \n                                                shuffle=True, \n                                                batch_size=batch_size)\n\ndata_loader_test = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_test_final, y_test_final), \n                                                shuffle=True, \n                                                batch_size=batch_size)\n\n\nlstm = LSTMModel(num_classes, input_size, hidden_size, num_layers, seq_length=lookback, batch_size=batch_size) #our lstm class \n\n\ncriterion = torch.nn.MSELoss()    # mean-squared error for regression\noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate) # ADAM optimizer\n\n\n# from custom_data_loader import MyDataset\n\n\n# data_loader_train = torch.utils.data.DataLoader(\n#     MyDataset((X_train_tensors, y_train_tensors), window),\n#     batch_size=batch_size,\n#     shuffle=False\n# )\n\n\n# data_loader_train = torch.utils.data.DataLoader(\n#     torch.utils.data.TensorDataset(X_train_final, y_train_tensors),\n#     batch_size=batch_size,\n#     shuffle=True\n# )\n\n\nfor i, (X_, y_) in enumerate(data_loader_train):\n    if i == 0:\n        print(X_.size())\n        print(y_.size())\n    print(i)\n\ntorch.Size([2173, 1, 7])\ntorch.Size([2173, 1, 1])\n0\n\n\n\nfor epoch in range(num_epochs):\n  for i, data in enumerate(data_loader_train):\n    X_, y_ = data\n    outputs = lstm.forward(X_) #forward pass\n    optimizer.zero_grad() #calculate the gradient, manually setting to 0\n  \n    # obtain the loss function\n    loss = criterion(outputs, y_.reshape(y_.size(0)*y_.size(1), 1))\n  \n    loss.backward() #calculates the loss of the loss function\n  \n    optimizer.step() #improve from loss, i.e backprop\n    # if (i + 1) % 50 == 0:\n    #     print(f\"Epoch {epoch}, batch {i:&gt;3}, loss on batch: {loss.item():.3f}\")\n  if epoch % 100 == 0:\n    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n\nEpoch: 0, loss: 0.98336\nEpoch: 100, loss: 0.02265\nEpoch: 200, loss: 0.01078\nEpoch: 300, loss: 0.01014\nEpoch: 400, loss: 0.00979\nEpoch: 500, loss: 0.00954\nEpoch: 600, loss: 0.00935\nEpoch: 700, loss: 0.00917\nEpoch: 800, loss: 0.00898\nEpoch: 900, loss: 0.00880\n\n\n\nwith torch.no_grad():\n    # train_plot = np.ones_like(timeseries) * np.nan\n    # y_pred = model(X_train)\n    # y_pred = y_pred[:, -1, :]\n    # train_plot[lookback:train_size] = model(X_train)[:, -1, :]\n    # shift test predictions for plotting\n    #test_plot = np.ones_like(y_test) * np.nan\n    test_plot = lstm(X_test_final)[:, 0]\n\n\nplt.plot(test_plot.data.numpy())\nplt.plot(y_test_final[:, -1, :].data.numpy())\n#plt.plot(y_test_final[:, -1, :].data.numpy())\n\n\n\n\n\n\n\n\n\ntrain_predict = lstm(X_train_final).data.numpy()\ntrain_predict = ss2.inverse_transform(train_predict)\n\n\n# run these once\n#train_predict = ss2.inverse_transform(train_predict) # reverse transformation\nX_train = pd.DataFrame(ss1.inverse_transform(X_train_ss), index=X_train_ss.index, columns=X_train_ss.columns) # reverse transformation for training data\n\n\npredicted_price = pd.DataFrame(train_predict)\npredicted_price.columns = ['Predicted_Price']\npredicted_price.size\nidx = X_train.index[:predicted_price.size]\npredicted_price.index = idx\nX_train = pd.concat([X_train, predicted_price], ignore_index=False, axis=1)\nX_train = X_train.dropna()\n\n\n#X_train['Predicted_Price'] = data_predict\nX_train['Actual_Signal'] = (X_train['Returns'].shift(-1) &gt; 0).astype(int) # actual signal, 1 if next-day returns &gt; 0, 0 if next-day returns &lt;= 0\nX_train['Predicted_Returns'] = X_train['Predicted_Price'].pct_change() # calculate returns of predicted stock price\nX_train['Predicted_Signal'] = (X_train['Predicted_Returns'] &gt; 0).astype(int) # predicted signal based on predicted returns\nX_train['Strategy_Returns'] = X_train['Returns'] * X_train['Predicted_Signal'].shift(1) # strategy returns\ncumulative_strategy_returns = (X_train['Strategy_Returns'].fillna(0) + 1).cumprod()\nreturns = X_train.loc[X_train.index, 'Returns']\nreturns.iloc[0] = 0 # set the first actual return to 0 to ensure that both plots start at 1.\ncumulative_stock_returns = (returns + 1).cumprod() # returns of the stock (basically if we held the stock for the entire duration)\naccuracy = (X_train['Actual_Signal'] == X_train['Predicted_Signal']).mean() # accuracy of predicted signal\nprint(f'Accuracy: {accuracy}')\n\nplt.figure(figsize=(10,5))\nplt.plot(cumulative_strategy_returns, label='Strategy Returns')\nplt.plot(cumulative_stock_returns, label='Stock Returns')\nplt.legend();\n\nAccuracy: 0.49310027598896045\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10,5))\nplt.plot(X_train['Predicted_Price'].shift(1), label='Predicted Price', alpha=0.7)\nplt.plot(X_train['Close'], label='Actual Price', alpha=0.7)\nplt.legend();\n\n\n\n\n\n\n\n\n\ntest_predict = lstm(X_test_final).data.numpy()\n\n\ntest_predict = ss2.inverse_transform(test_predict)\nX_test = pd.DataFrame(ss1.inverse_transform(X_test_ss), index=X_test_ss.index, columns=X_test_ss.columns)\n\n\npredicted_price = pd.DataFrame(test_predict)\npredicted_price.columns = ['Predicted_Price']\npredicted_price.size\nidx = X_test.index[:predicted_price.size]\npredicted_price.index = idx\nX_test = pd.concat([X_test, predicted_price], ignore_index=False, axis=1)\nX_test = X_test.dropna()\n\n\nX_test['Actual_Signal'] = (X_test['Returns'].shift(-1) &gt; 0).astype(int)\n#X_test['Predicted_Price'] = data_predict\nX_test['Predicted_Returns'] = X_test['Predicted_Price'].pct_change()\nX_test['Predicted_Signal'] = (X_test['Predicted_Returns'] &gt; 0).astype(int)\nX_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1)\ncumulative_strategy_returns = (X_test['Strategy_Returns'].fillna(0) + 1).cumprod()\nreturns = X_test.loc[X_test.index, 'Returns']\nreturns.iloc[0] = 0\ncumulative_stock_returns = (returns + 1).cumprod()\naccuracy = (X_test['Actual_Signal'] == X_test['Predicted_Signal']).mean()\nprint(f'Accuracy: {accuracy}')\n# Calculate cumulative returns for the market\n#market_data['Returns'] = market_data['Close'].pct_change()\n#cumulative_market_returns = (market_data['Returns'].fillna(0) + 1).cumprod()\n\nplt.figure(figsize=(10,5))\n\nplt.plot(cumulative_strategy_returns, label='Strategy Returns')\n#plt.plot(cumulative_market_returns, label='Market Returns')\nplt.plot(cumulative_stock_returns, label='Stock Returns')\nplt.legend();\n\nAccuracy: 0.5247933884297521\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10,5))\nplt.plot(X_test['Predicted_Price'].shift(1), label='Predicted Price')\nplt.plot(X_test['Close'], label='Actual Price')\nplt.legend();\n\n\n\n\n\n\n\n\n\nfrom torchinfo import summary\n\n\nsummary(lstm)\n\n=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\nLSTMModel                                --\n├─LSTM: 1-1                              5,248\n├─Sequential: 1-2                        --\n│    └─Linear: 2-1                       4,224\n│    └─ReLU: 2-2                         --\n│    └─Linear: 2-3                       129\n=================================================================\nTotal params: 9,601\nTrainable params: 9,601\nNon-trainable params: 0\n================================================================="
  },
  {
    "objectID": "posts/cs-451-quant-project/lstm_research.html#lstm-on-10-biggest-energy-companies",
    "href": "posts/cs-451-quant-project/lstm_research.html#lstm-on-10-biggest-energy-companies",
    "title": "LSTM",
    "section": "LSTM on 10 Biggest Energy Companies",
    "text": "LSTM on 10 Biggest Energy Companies\n\ntickers = ['XOM', 'CVX', 'COP', 'NEE', 'SO', 'EOG', 'DUK', 'MPC', 'SLB', 'PSX']\n\n\nfrom lstm_data import prepare_data\n\n\n# preps data, see lstm_data.py, prints size of each ticker's dataset\nX_train, y_train, X_test, y_test, ss, mm, batch_size = prepare_data(tickers, start_date = '2014-01-01', end_date = '2024-01-01')\n\n(2039, 18)\n(2039, 18)\n(2039, 18)\n(2039, 18)\n(2039, 18)\n(2039, 18)\n(2039, 18)\n(2039, 18)\n(2039, 18)\n(2039, 18)\n\n\n\nfeatures = ['SMA_10', 'SMA_20', 'SMA_50', 'SMA_100', 'SMA_250', 'Std_Dev', 'Z_Score', 'RSI', 'TTM_P/E', 'Close']\n\nX_train_tensors = Variable(torch.Tensor(np.array(X_train[features])))\ny_train_tensors = Variable(torch.Tensor(y_train.values))\nX_train_final = torch.reshape(X_train_tensors, (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\n\n\n# split data by ticker\ndata_loader_train = torch.utils.data.DataLoader(\n    torch.utils.data.TensorDataset(X_train_final, y_train_tensors),\n    batch_size=batch_size,\n    shuffle=True\n)\n\n\nnum_epochs = 1000 # 1000 epochs\nlearning_rate = 0.001 # 0.001 lr\n\ninput_size = X_train_final.shape[2] # number of features\nhidden_size = 32 # number of features in hidden state\nnum_layers = 1 # number of stacked lstm layers\nbatch_size = batch_size\nwindow = 1 # number of windows, leave at 1, basically can ignore\n\nnum_classes = 1 # number of output classes\n\n\nlstm = LSTMModel(num_classes, input_size, hidden_size, num_layers, seq_length=window, batch_size=batch_size) #our lstm class \ncriterion = torch.nn.MSELoss()    # mean-squared error for regression\noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate) # ADAM optimizer\n\nBelow takes about 7 min to run.\n\nfor epoch in range(num_epochs):\n  for i, data in enumerate(data_loader_train):\n    X_, y_ = data\n    outputs = lstm.forward(X_) #forward pass\n    optimizer.zero_grad() #calculate the gradient, manually setting to 0\n  \n    # obtain the loss function\n    loss = criterion(outputs, y_.reshape(y_.size(0)*y_.size(1), 1))\n  \n    loss.backward() #calculates the loss of the loss function\n  \n    optimizer.step() #improve from loss, i.e backprop\n    # if (i + 1) % 50 == 0:\n    #     print(f\"Epoch {epoch}, batch {i:&gt;3}, loss on batch: {loss.item():.3f}\")\n  if epoch % 100 == 0:\n    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n\nEpoch: 0, loss: 0.88014\nEpoch: 100, loss: 0.00495\nEpoch: 200, loss: 0.00485\nEpoch: 300, loss: 0.00437\nEpoch: 400, loss: 0.00475\nEpoch: 500, loss: 0.00402\nEpoch: 600, loss: 0.00464\nEpoch: 700, loss: 0.00456\nEpoch: 800, loss: 0.00450\nEpoch: 900, loss: 0.00469\n\n\n\ndef evaluate_lstm(model, X_test, y_test, ss, mm, features):\n    ticker = X_test['Ticker'].iloc[0] # get ticker\n    X_test_tensors = Variable(torch.Tensor(np.array(X_test[features]))) # prepare for lstm\n    X_test_final = torch.reshape(X_test_tensors,  (X_test_tensors.shape[0], 1, X_test_tensors.shape[1])) # prepare for lstm\n\n    test_predict = model(X_test_final).data.numpy() # predict\n    test_predict = mm.inverse_transform(test_predict) # reverse transform back to original scale\n    cols = X_test.columns[X_test.columns != 'Ticker']\n    X_test = pd.DataFrame(ss.inverse_transform(X_test[cols]), index=X_test.index, columns=cols) # reverse transform X_test back to og scale\n    predicted_price = pd.DataFrame(test_predict)\n    predicted_price.columns = ['Predicted_Price']\n    predicted_price.size\n    idx = X_test.index[:predicted_price.size]\n    predicted_price.index = idx # fix index of predicted prices\n    X_test = pd.concat([X_test, predicted_price], ignore_index=False, axis=1) \n    X_test = X_test.dropna()\n    X_test['Actual_Signal'] = (X_test['Returns'].shift(-1) &gt; 0).astype(int) # actual buy/sell signal based on daily returns\n    X_test['Predicted_Returns'] = X_test['Predicted_Price'].pct_change()\n    X_test['Predicted_Signal'] = (X_test['Predicted_Returns'] &gt; 0)*1 # predicted buy/sell signal based on predicted returns\n    X_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1) # calculate daily strategy returns\n    cumulative_strategy_returns = (X_test['Strategy_Returns'].fillna(0) + 1).cumprod()\n    returns = X_test.loc[X_test.index, 'Returns']\n    returns.iloc[0] = 0\n    cumulative_stock_returns = (returns + 1).cumprod()\n    accuracy = (X_test['Actual_Signal'] == X_test['Predicted_Signal']).mean()\n    print(f'{ticker} Accuracy: {accuracy}')\n\n    # plot returns\n    plt.figure(figsize=(10,5))\n    plt.plot(cumulative_strategy_returns, label='Strategy Returns')\n    plt.plot(cumulative_stock_returns, label='Stock Returns')\n    plt.title(f'{ticker} Returns')\n    plt.legend();\n\n    # plot stock price\n    plt.figure(figsize=(10,5))\n    plt.plot(X_test['Predicted_Price'].shift(1), label='Predicted Price')\n    plt.plot(X_test['Close'], label='Actual Price')\n    plt.title(f'{ticker} Price')\n    plt.legend();\n    return X_test\n\n\nX_test0 = evaluate_lstm(lstm, X_test[0], y_test[0], ss[0], mm[0], features)\nX_test0[['Close', 'Predicted_Price', 'Actual_Signal', 'Predicted_Signal']]\n\nXOM Accuracy: 0.5550660792951542\n\n\n\n\n\n\n\n\n\nClose\nPredicted_Price\nActual_Signal\nPredicted_Signal\n\n\n\n\n2023-02-03\n107.228424\n107.144272\n0\n0\n\n\n2023-02-06\n107.046402\n106.897194\n1\n0\n\n\n2023-02-07\n110.102676\n109.846146\n0\n1\n\n\n2023-02-08\n109.144585\n108.950798\n1\n0\n\n\n2023-02-09\n109.556557\n109.331627\n1\n1\n\n\n...\n...\n...\n...\n...\n\n\n2023-12-21\n100.793266\n101.113884\n1\n1\n\n\n2023-12-22\n100.971611\n101.140839\n1\n1\n\n\n2023-12-26\n101.199486\n101.226402\n0\n1\n\n\n2023-12-27\n100.723907\n100.687119\n0\n0\n\n\n2023-12-28\n99.267441\n99.319710\n0\n0\n\n\n\n\n227 rows × 4 columns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor i in range(10):\n    evaluate_lstm(lstm, X_test[i], y_test[i], ss[i], mm[i], features)\n\nXOM Accuracy: 0.5550660792951542\nCVX Accuracy: 0.5198237885462555\nCOP Accuracy: 0.47577092511013214\nNEE Accuracy: 0.4801762114537445\nSO Accuracy: 0.5550660792951542\nEOG Accuracy: 0.5374449339207048\nDUK Accuracy: 0.5286343612334802\nMPC Accuracy: 0.5198237885462555\nSLB Accuracy: 0.5154185022026432\nPSX Accuracy: 0.5374449339207048"
  },
  {
    "objectID": "posts/cs-451-quant-project/lstm_research.html#further-testing",
    "href": "posts/cs-451-quant-project/lstm_research.html#further-testing",
    "title": "LSTM",
    "section": "Further Testing",
    "text": "Further Testing\nNote: need to test by combining training and test sets due to the nature of sequential data.\n\ni = 0\n\nticker = X_test[i]['Ticker'].iloc[0]\nX_test_tensors = Variable(torch.Tensor(np.array(X_test[i][features]))) # prepare for lstm\nX_test_final = torch.reshape(X_test_tensors,  (X_test_tensors.shape[0], 1, X_test_tensors.shape[1])) # prepare for lstm\n\ntest_predict = lstm(X_test_final).data.numpy() # predict\ntest_predict = mm_scalers[i].inverse_transform(test_predict) # reverse transform back to original scale\ncols = X_test[i].columns[X_test[i].columns != 'Ticker']\nX_test_ = pd.DataFrame(ss_scalers[i].inverse_transform(X_test[i][cols]), index=X_test[i].index, columns=cols)\npredicted_price = pd.DataFrame(test_predict)\npredicted_price.columns = ['Predicted_Price']\npredicted_price.size\nidx = X_test_.index[:predicted_price.size]\npredicted_price.index = idx\nX_test_ = pd.concat([X_test_, predicted_price], ignore_index=False, axis=1)\nX_test_ = X_test_.dropna()\nX_test_['Actual_Signal'] = (X_test_['Returns'].shift(-1) &gt; 0).astype(int)\n#X_test['Predicted_Price'] = data_predict\nX_test_['Predicted_Returns'] = X_test_['Predicted_Price'].pct_change()\nX_test_['Predicted_Signal'] = (X_test_['Predicted_Returns'].shift(-1) &gt; 0).astype(int)\nX_test_['Strategy_Returns'] = X_test_['Returns'] * X_test_['Predicted_Signal'].shift(1)\ncumulative_strategy_returns = (X_test_['Strategy_Returns'].fillna(0) + 1).cumprod()\nreturns = X_test_.loc[X_test_.index, 'Returns']\nreturns.iloc[0] = 0\ncumulative_stock_returns = (returns + 1).cumprod()\naccuracy = (X_test_['Actual_Signal'] == X_test_['Predicted_Signal']).mean()\nprint(f'Accuracy: {accuracy}')\n# Calculate cumulative returns for the market\n#market_data['Returns'] = market_data['Close'].pct_change()\n#cumulative_market_returns = (market_data['Returns'].fillna(0) + 1).cumprod()\n\nplt.figure(figsize=(10,5))\nplt.plot(cumulative_strategy_returns, label='Strategy Returns')\n#plt.plot(cumulative_market_returns, label='Market Returns')\nplt.plot(cumulative_stock_returns, label='Stock Returns')\nplt.title(f'{ticker} Returns')\nplt.legend();\n\n# plot stock price\nplt.figure(figsize=(10,5))\nplt.plot(X_test_['Predicted_Price'], label='Predicted Price')\nplt.plot(X_test_['Close'], label='Actual Price')\nplt.title(f'{ticker} Price')\nplt.legend();\n\nAccuracy: 0.7530364372469636\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsummary(lstm)\n\n=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\nLSTMModel                                --\n├─LSTM: 1-1                              5,248\n├─Sequential: 1-2                        --\n│    └─Linear: 2-1                       4,224\n│    └─ReLU: 2-2                         --\n│    └─Linear: 2-3                       129\n=================================================================\nTotal params: 9,601\nTrainable params: 9,601\nNon-trainable params: 0\n=================================================================\n\n\n\nX_list = []\nfor X_, y_ in data_loader_train:\n    X_list.append(X_)\nX1 = pd.concat([X_train.iloc[:batch_size,:], X_test[0]])\nticker = X1['Ticker'].iloc[0]\n# X1 = X_test[0]\n\n\ni = 0\n\n\nX_test_tensors = Variable(torch.Tensor(np.array(X1[features]))) # prepare for lstm\nX_test_final = torch.reshape(X_test_tensors,  (X_test_tensors.shape[0], 1, X_test_tensors.shape[1])) # prepare for lstm\n\ntest_predict = lstm(X_test_final).data.numpy() # predict\ntest_predict = mm_scalers[i].inverse_transform(test_predict) # reverse transform back to original scale\ncols = X1.columns[X_test[i].columns != 'Ticker']\nX_test_ = pd.DataFrame(ss_scalers[i].inverse_transform(X1[cols]), index=X1.index, columns=cols)\npredicted_price = pd.DataFrame(test_predict)\npredicted_price.columns = ['Predicted_Price']\npredicted_price.size\nidx = X_test_.index[:predicted_price.size]\npredicted_price.index = idx\nX_test_ = pd.concat([X_test_, predicted_price], ignore_index=False, axis=1)\nX_test_ = X_test_.dropna()\n\n\nX_test_\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\nDividends\nStock Splits\nSMA_20\nSMA_50\nStd_Dev\nZ_Score\nRSI\nTTM_EPS\nTTM_P/E\nReturns\nPredicted_Price\n\n\n\n\n2014-03-14\n60.733148\n61.148506\n60.544939\n60.661758\n11723300.0\n0.0\n0.0\n61.608636\n61.664019\n0.684117\n-1.384087\n44.303418\n7.37\n8.230903\n-0.001815\n57.683525\n\n\n2014-03-17\n60.973264\n61.349683\n60.843466\n61.213394\n8676100.0\n0.0\n0.0\n61.615450\n61.602643\n0.679205\n-0.591951\n48.780818\n7.37\n8.305752\n0.009094\n58.606907\n\n\n2014-03-18\n61.271811\n61.816966\n61.051149\n61.466507\n8757300.0\n0.0\n0.0\n61.636220\n61.549423\n0.667287\n-0.254332\n50.737673\n7.37\n8.340096\n0.004135\n58.927059\n\n\n2014-03-19\n61.427551\n61.849400\n60.441073\n60.830475\n9914700.0\n0.0\n0.0\n61.629080\n61.481549\n0.675467\n-1.182302\n45.983474\n7.37\n8.253796\n-0.010348\n58.022343\n\n\n2014-03-20\n60.642266\n61.485959\n60.363197\n61.382122\n10405000.0\n0.0\n0.0\n61.603444\n61.406536\n0.674578\n-0.328089\n50.330605\n7.37\n8.328646\n0.009069\n58.783268\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-12-21\n100.535658\n101.070687\n99.881732\n100.793266\n19250900.0\n0.0\n0.0\n100.424687\n103.165610\n1.916560\n0.192313\n48.493679\n10.44\n9.654527\n0.004542\n102.408745\n\n\n2023-12-22\n101.367922\n101.992126\n100.882436\n100.971611\n12921800.0\n0.0\n0.0\n100.292912\n103.094368\n1.771334\n0.383157\n49.168148\n10.44\n9.671610\n0.001769\n103.004120\n\n\n2023-12-26\n101.793959\n102.081290\n101.179673\n101.199486\n16835100.0\n0.0\n0.0\n100.202750\n102.960920\n1.669085\n0.597175\n50.067858\n10.44\n9.693437\n0.002257\n103.541283\n\n\n2023-12-27\n101.100406\n101.605712\n100.406847\n100.723907\n14558800.0\n0.0\n0.0\n100.091782\n102.816389\n1.546573\n0.408727\n48.152305\n10.44\n9.647884\n-0.004699\n103.415825\n\n\n2023-12-28\n100.456388\n100.674363\n99.207988\n99.267441\n16329300.0\n0.0\n0.0\n99.985271\n102.614453\n1.525110\n-0.470675\n42.757115\n10.44\n9.508376\n-0.014460\n101.797653\n\n\n\n\n2466 rows × 16 columns"
  },
  {
    "objectID": "posts/cs-451-quant-project/quant_research.html",
    "href": "posts/cs-451-quant-project/quant_research.html",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "",
    "text": "We applied machine learning methods to predict daily stock price movements in a basket of 10 US-listed energy companies. We found the most success using an LSTM model, achieving an accuracy of up to 61% on one stock (PSX). In line with prior literature, we compared our results to a benchmark established by a last value machine, which simply predicts the next day’s price to be the current day’s actual price. We tested our model on two different sets of observations, first on a year’s worth of historical data (historical test) and then on one week of recent trading data (live test). Comparing our LSTM results to our benchmark, we find mixed results. We achieve, on average, 53.57% accuracy vs. our benchmark’s 53.08% accuracy on the historical test. For the ten companies in our analysis, our model has superior for accuracy for 6 companies, has equal accuracy for 1 company, and has worse accuracy for 3 companies compared to our last value benchmark. This superior accuracy leads to higher simulated portfolio returns using our model compared to our benchmarks. Our live test accuracy of 49.99% was worse but still beat our benchmark’s accuracy of 48.33%.\n\n\n\nIn this blog post, we train machine learning models on historical stock market data to predict future stock price movements. This is a highly popular problem to address because of the potential for significant monetary gain. This is an important problem societally because stock markets are mechanisms of price discovery: they answer the question “What is a company worth?” Finding the right answer to that question allows society to correctly allocate more or less capital (money) to that company. On an individual level, this is an important problem to us as the authors because it’s the problem for all quant trading: making a profitable model.\nAn enormous body of literature within and without computer science exists for stock market prediction. Among the papers most relevant to our work are Gunduz (2021), Bhandari et al. (2022), and Zhang (2022).\nGunduz (2021) applies LSTM and ensemble learning (Light-GBM) models to predict the hourly directions of eight banking stocks in Borsa Istanbul. He achieved up to maximum success rate of 0.685 using individual features of bank stocks and LSTM.\nBhandari et al. (2022) apply single-layer and multi-layer LSTM models to the problem of predicting the S&P 500, the index of the largest 500 publicly traded companies in America. Their single-layer LTSM model with 150 neurons is their best performing specification. Their set of predicted values have an average correlation coefficient of 0.9976 with actual S&P index values.\nZhang (2022) finds the LSTM network model does not perform better than other models when applied to a short forecasting horizon (1 to 10 days). Zhang’s “other models” are linear regression, eXtreme gradient boosting (XGBoost), last value, and moving average.\nWe take some of the “best practices” we observe in the above papers, specifically benchmarking with last value and using MSE and R to assess our results. Unlike the mentioned papers, we will be focusing on single stocks and attempting to build a model that outperforms the last value benchmark.\n\n\n\nThe potential users are anyone interested in making profitable trades in the stock market. They are the individuals most likely to directly benefit from our work. Nonusers who could be affected by our work are those engaged in the stock market. The obvious affected nonusers are those on the opposite side of each trade as a user. In every trade, there’s a buyer and a seller, so in every trade, there’s a winner and a loser. These opposing nonusers are the individuals who are most likely to be harmed by the success of our program.\nUltimately, the point of the back and forth of markets is price discovery: to help society find the right prices of different companies. This leads to another nonuser effect: with better price discovery and more efficient markets, companies will raise money at prices that are closer to some “true” value, which is loosely defined as a value that best reflects the fundamental valuation of the company. Our model does not attempt to predict a true fundamental value for a company, but by making accurate predictions for the next day’s price, it should accelerate the market’s convergence to an appropriate value.\nA useful financial trading model should lead to a net societal benefit because better financial markets mean more or less money going to companies and therefore projects, leading to something closer to an “optimal” allocation of money in society.\nWe are personally motivated to work on this project because of personal interest, professional relevance, and the difficulty of the problem. All three of us personally invest in the stock market. Two of us (Donovan & James) are double majors in economics and have had experience working in the financial services industry. Andre is interested in pursuing a master’s in financial engineering after Middlebury. The problem itself is also inherently challenging: financial markets are constantly adapting and changing, making the findings of previous literature increasingly likely over time to be less applicable to today’s markets. This forces us to adopt new techniques. # Materials and Methods\n\n\n\nOur data was sourced from Yahoo Finance. We used the yfinance library to download historical stock price data for our 10 different stocks. We chose to focus on US-based oil companies. These companies are Exxon Mobil (XOM), Chevron (CVX), ConocoPhillips (COP), Enterprise Products Partners (EPD), Pioneer Natural Resources (PXD), EOG Resources (EOG), Duke Energy (DUK), Marathon Petroleum (MPC), Schlumberger (SLB), and Phillips 66 (PSX). We downloaded the data from May 6th, 2014 to May 6th, 2024.\nWithin the yfinance dataset we were given the following columns: Open, High, Low, Close, Adj Close, Volume.\nOpen is the opening price of the stock for the day. High is the highest price of the stock for the day. Low is the lowest price of the stock for the day. Close is the closing price of the stock for the day. Adj Close is the adjusted closing price of the stock for the day. Volume is the number of shares traded for the day.\nWe used the Close column as our target variable for our model. We also created the following features: SMA_20, SMA_50, Std_Dev, Z_Score, RSI, TTM_P/E which will be discussed below. Here’s a look at what the raw data looks like:\n\nimport yfinance as yf\n\nxom = yf.Ticker('XOM')\ndata = xom.history(start='2014-05-06', end='2024-05-07')\ndata.head()\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\nDividends\nStock Splits\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n2014-05-06 00:00:00-04:00\n66.050586\n66.501227\n65.928274\n66.095650\n9669800\n0.00\n0.0\n\n\n2014-05-07 00:00:00-04:00\n66.385376\n66.597816\n66.172931\n66.378937\n11007400\n0.00\n0.0\n\n\n2014-05-08 00:00:00-04:00\n66.366038\n66.494794\n65.773772\n65.870338\n8922500\n0.00\n0.0\n\n\n2014-05-09 00:00:00-04:00\n65.922184\n66.226810\n65.630524\n66.077736\n8948800\n0.69\n0.0\n\n\n2014-05-12 00:00:00-04:00\n66.324029\n66.336990\n65.805516\n66.259216\n8830500\n0.00\n0.0\n\n\n\n\n\n\n\nYou can find the full implementation of our data at lstm_data.py under the function prepare_data().\n\n\n\n\n\nWe used SMA_20, SMA_50, Std_Dev, Z_Score, RSI, Close, TTM_P/E as predictors for our models.\nThe SMA_20 and SMA_50 are the 20-day and 50-day simple moving averages of the stock price. This means that the average closing price of the stock over the last 20 and 50 days, respectively.\nThe Std_Dev is the standard deviation of the stock price meaning how much the stock price deviates from the mean.\nThe Z_Score is the z-score of the stock price meaning how many standard deviations the stock price is from the mean.\nThe RSI is the relative strength index of the stock price meaning how strong the stock price is relative to its past performance. It is calculated by taking the average of the gains and losses over a certain period of time.\nThe Close is the closing price of the stock per day.\nThe TTM_P/E is the trailing twelve months price-to-earnings ratio of the stock.\nWe used the next day’s Close price as the target variable for our model.\n\n\n\nWe collected 10 years of data from May 7th, 2014 to May 7th, 2024 and used a train-test split of 90-10 in order to train our model on the first 9 years worth of the data and test it on the remaining 1 year’s worth of data. We used a standard scaler for scaling our data in order to ensure that the data was normalized. We fit the scaler on the training data and then applied it to the test data to avoid any information leaking. We then combined the training data for each stock into one dataset. We used the closing price of the stock as the target variable for our model.\nHere’s what our data looks like after creating our features and scaling the data:\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\ntickers = ['XOM', 'CVX', 'COP', 'EPD', 'EOG', 'DUK', 'MPC', 'SLB', 'PSX', 'OXY']\nstart = '2014-05-06'\nend = '2024-05-07'\n\n# preps data, see lstm_data.py, prints size of each ticker's dataset\nX_train, y_train, X_test, y_test, X_scalers, y_scalers, batch_size = prepare_data(tickers, start_date=start, end_date=end, test_size=0.1)\nX_train\n\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\nDividends\nStock Splits\nSMA_10\nSMA_20\nSMA_50\nSMA_100\nSMA_250\nStd_Dev\nZ_Score\nRSI\nTTM_EPS\nTTM_P/E\nReturns\nTicker\n\n\n\n\n2015-05-01\n-0.139877\n-0.131185\n-0.112171\n-0.102371\n-0.540891\n-0.127836\n0.00000\n-0.149377\n-0.173825\n-0.178037\n-0.063809\n0.319177\n-0.679901\n1.186546\n0.879810\n0.650885\n-0.095652\n0.893759\nXOM\n\n\n2015-05-04\n-0.080169\n-0.096257\n-0.064154\n-0.092513\n-0.732495\n-0.127836\n0.00000\n-0.142218\n-0.166130\n-0.178686\n-0.064625\n0.316818\n-0.653451\n1.177769\n0.973560\n0.650885\n-0.095424\n0.133748\nXOM\n\n\n2015-05-05\n-0.059251\n-0.088369\n-0.080672\n-0.111473\n-0.577151\n-0.127836\n0.00000\n-0.135595\n-0.160600\n-0.179007\n-0.065519\n0.314254\n-0.642313\n0.791839\n0.672019\n0.650885\n-0.095861\n-0.329225\nXOM\n\n\n2015-05-06\n-0.071421\n-0.093251\n-0.108714\n-0.127400\n-0.639387\n-0.127836\n0.00000\n-0.132800\n-0.152615\n-0.179992\n-0.065441\n0.311766\n-0.751693\n0.518627\n0.427504\n0.650885\n-0.096229\n-0.281992\nXOM\n\n\n2015-05-07\n-0.134173\n-0.163112\n-0.142135\n-0.149774\n-0.767664\n-0.127836\n0.00000\n-0.132571\n-0.146911\n-0.181595\n-0.065840\n0.309079\n-0.859104\n0.047923\n0.100192\n0.650885\n-0.096744\n-0.387753\nXOM\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-06-02\n0.610803\n0.625792\n0.636749\n0.650958\n0.109535\n-0.105084\n-0.02214\n0.607044\n0.598059\n0.727834\n0.786385\n0.982249\n-0.739053\n0.869582\n0.136220\n1.924677\n-0.222355\n0.810058\nOXY\n\n\n2023-06-05\n0.714326\n0.672373\n0.676732\n0.633996\n-0.312158\n-0.105084\n-0.02214\n0.608629\n0.594259\n0.728931\n0.783608\n0.979744\n-0.834725\n0.776482\n0.011823\n1.924677\n-0.222464\n-0.165726\nOXY\n\n\n2023-06-06\n0.590332\n0.602501\n0.623814\n0.636336\n-0.325573\n-0.105084\n-0.02214\n0.610977\n0.595673\n0.730434\n0.780891\n0.977007\n-0.819762\n0.773031\n0.029482\n1.924677\n-0.222449\n0.003637\nOXY\n\n\n2023-06-07\n0.652329\n0.667133\n0.694371\n0.671429\n-0.292849\n-0.105084\n-0.02214\n0.613090\n0.598854\n0.730900\n0.777538\n0.974819\n-0.772194\n1.238762\n0.294437\n1.924677\n-0.222223\n0.292064\nOXY\n\n\n2023-06-08\n0.669287\n0.645055\n0.635925\n0.662629\n-0.147267\n2.262466\n-0.02214\n0.616022\n0.607809\n0.728131\n0.774355\n0.973557\n-0.847555\n1.077118\n0.218085\n1.924677\n-0.222280\n-0.093700\nOXY\n\n\n\n\n20410 rows × 19 columns\n\n\n\n\n\n\nOriginally, we used rather simplistic models like logistic regression, Random Forest, and SVM in order to predict stock price movements. We utilized Recursive Feature Elimination (RFE) in order to determine the optimal features for prediction for each model. However, we found that these models were not able to predict stock price movements consistently with much accuracy. We then decided to use a Long Short-Term Memory (LSTM) model to predict stock price movements. LSTM models are a type of recurrent neural network (RNN) with the addition of “gates” notably the input, forget and output gates. These gates allow for the model to determine what information to retain or discard at each timestep, mitigating the vanishing descent issue found in traditional recurrent neural networks. The LSTM model accounts for the shortfalls of an RNN by capturing long-term dependencies in the data.\nThe forget gate determines which information is either retained or discarded at each time step. It accepts the output from the previous time step \\(h_{t-1}\\) and the input \\(x_t\\) at the current time step. The forget gate is defined as:\n\\[f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\\]\nThe input gate determines which information is stored in the cell state. It avoids feeding the unimportant information into the current memory cell. It has three different components:\n\nGetting the state of the cell that must be updated.\nCreate a new cell state\nUpdate the cell state to the current cell state\n\nThese are defined as:\n\\[\\begin{aligned}\ni_{t} &= \\sigma(W_{t} \\cdot [h_{t-1}, x_{t}] + b_{i}) \\\\\n\\widetilde{C}_{t} &= \\tanh(W_{c} \\cdot [h_{t-1}, x_{t}] + b_{c}) \\\\\nC_{t} &= f_{t} \\ast C_{t-1} + i_{t} \\ast \\widetilde{C}_{t}\n\\end{aligned}\\]\nThe output gate determines how much of the newly created cell state will be discarded and how much will be passed to the output. It is defined as:\n\\[o_{t} = \\sigma(W_{o} \\cdot [h_{t-1}, x_{t}] + b_{o})\\]\nThis output information is firstly determined by a sigmoid layer, then the newly created cell state is processed by a tanh layer. The output is then multiplied by the sigmoid layer to determine the final output of the LSTM cell.\nWhich is defined as:\n\\[h_{t} = o_{t} \\ast \\tanh(C_{t})\\]\nTaking this all into account, the LSTM model is able to retain information from previous time steps and use it to predict future stock price movements while disregarding irrelevant information.\nThe implementation of our LSTM model can be found at: lstm_model.py\n\n\n\nWe first converted our wanted feature columns into a torch Variable to allow them to be differentiable. Then, we reshaped the data using torch.reshape() and torch.utils.data.DataLoader into [batch_size, seq_len, input_size].\n\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Close', 'TTM_P/E']\n\nX_train_tensors = Variable(torch.Tensor(np.array(X_train[features])))\ny_train_tensors = Variable(torch.Tensor(y_train.values))\nX_train_final = torch.reshape(X_train_tensors, (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\n\n# split data by ticker\ndata_loader_train = torch.utils.data.DataLoader(\n    torch.utils.data.TensorDataset(X_train_final, y_train_tensors),\n    batch_size=batch_size,\n    shuffle=True\n)\n\nnext(iter(data_loader_train))[0].shape\n\ntorch.Size([2041, 1, 7])\n\n\nWe trained our model using our own personal devices. We used the Adam optimizer with a learning rate of 0.001. We trained the model for 1000 epochs for each stock in our dataset (10 total) and used the torch.nn.MSELoss() loss function to train the model.\nMSE is defined as:\n\\[MSE = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_{i} - y_{i})^2\\]\nWhere \\(y_{i}\\) is the true price and \\(\\hat{y}_{i}\\) is the predicted price.\nAs mentioned previously our model was trained on 90% of the data and tested on the remaining 10%.\nIf the model predicted the next days price to be positive, we would purchase the stock at the closing price and sell it at the closing price the next day. If the model predicted the next days price to be negative, we would short the stock at the closing price and buy it back at the closing price the next day. We would then calculate the profit or loss percent change for each stock and compare it to the last value benchmark.\nBelow is our training code:\n\nnum_epochs = 1000 # 1000 epochs\nlearning_rate = 0.001 # 0.001 lr\n\ninput_size = X_train_final.shape[2] # number of features\nhidden_size = 32 # number of features in hidden state\nnum_layers = 1 # number of stacked lstm layers\nwindow = 1 # number of windows, leave at 1, basically can ignore\n\nnum_classes = 1 # number of output classes\n\nlstm = LSTMModel(num_classes, input_size, hidden_size, num_layers, seq_length=window, batch_size=batch_size) #our lstm class \ncriterion = torch.nn.MSELoss()    # mean-squared error for regression\noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate) # ADAM optimizer\n\n# training loop\nfor epoch in range(num_epochs):\n  for i, data in enumerate(data_loader_train):\n    X_, y_ = data\n    outputs = lstm.forward(X_) #forward pass\n    optimizer.zero_grad() #calculate the gradient, manually setting to 0\n  \n    # obtain the loss function\n    loss = criterion(outputs, y_.reshape(y_.size(0)*y_.size(1), 1))\n  \n    loss.backward() #calculates the loss of the loss function\n  \n    optimizer.step() #improve from loss, i.e backprop\n    # if (i + 1) % 50 == 0:\n    #     print(f\"Epoch {epoch}, batch {i:&gt;3}, loss on batch: {loss.item():.3f}\")\n  if epoch % 100 == 0:\n    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n\nEpoch: 0, loss: 0.91365\nEpoch: 100, loss: 0.00494\nEpoch: 200, loss: 0.00502\nEpoch: 300, loss: 0.00508\nEpoch: 400, loss: 0.00451\nEpoch: 500, loss: 0.00478\nEpoch: 600, loss: 0.00479\nEpoch: 700, loss: 0.00491\nEpoch: 800, loss: 0.00514\nEpoch: 900, loss: 0.00504\n\n\n\n\n\nWe evaluated our model by comparing the cumulative predicted stock price returns and accuracy to the actual cumulative stock price returns and accuracy and the cumulative last value benchmark returns and accuracy. The last value benchmark is defined as using the previous days value as the prediction for the current day. We would buy the stock at the current day’s close price and sell at the next day’s close price if the predicted returns were positive and do nothing if the predicted returns were negative. We followed the same principle in calculating actual cumulative stock returns and accuracy, and the cumulative last value benchmark returns and accuracy.\nWe define accuracy for our purposes as percentage of times the model correctly predicts an upward or downward movement in the share price of a company.\nConsider a simple test case where the model predicts the stock price to go up and the stock price actually goes up. In this case, the model is correct. If the model predicts the stock price to go up and the stock price actually goes down, the model is incorrect. We calculate the accuracy of the model by dividing the number of correct predictions by the total number of predictions.\nAccuracy per Stock\n\n\nCode\nfor i in range(10):\n    cum_strat_returns, cum_stock_returns, cum_lv_returns, accuracy, lv_accuracy, prediction_correl, lv_prediction_correl = evaluate_lstm(lstm, X_test[i], y_test[i], X_scalers[i], y_scalers[i], features)\n    if i == 0:\n        cum_strat_returns_list = np.array([cum_strat_returns])\n        cum_stock_returns_list = np.array([cum_stock_returns])\n        cum_lv_returns_list = np.array([cum_lv_returns])\n        accuracy_list = np.array([accuracy])\n        lv_accuracy_list = np.array([lv_accuracy])\n        correl_list = np.array([prediction_correl])\n        lv_correl_list = np.array([lv_prediction_correl])\n    else:\n        cum_strat_returns_list = np.append(cum_strat_returns_list, np.array([cum_strat_returns]), axis=0)\n        cum_stock_returns_list = np.append(cum_stock_returns_list, np.array([cum_stock_returns]), axis=0)\n        cum_lv_returns_list = np.append(cum_lv_returns_list, np.array([cum_lv_returns]), axis=0)\n        accuracy_list = np.append(accuracy_list, np.array([accuracy]), axis=0)\n        lv_accuracy_list = np.append(lv_accuracy_list, np.array([lv_accuracy]), axis=0)\n        correl_list = np.append(correl_list, np.array([prediction_correl]), axis=0)\n        lv_correl_list = np.append(lv_correl_list, np.array([lv_prediction_correl]))\n\n\nXOM Accuracy: 0.5418502202643172, Correlation: 0.972313389714224, Last Value Accuracy: 0.5242290748898678, Last Value Correlation: 0.9780145037734742\nCVX Accuracy: 0.5154185022026432, Correlation: 0.9613024878319475, Last Value Accuracy: 0.5066079295154186, Last Value Correlation: 0.9635678490020599\nCOP Accuracy: 0.5550660792951542, Correlation: 0.9689221084107935, Last Value Accuracy: 0.5418502202643172, Last Value Correlation: 0.9783667790929887\nEPD Accuracy: 0.5374449339207048, Correlation: 0.9860702298831613, Last Value Accuracy: 0.5506607929515418, Last Value Correlation: 0.9916250412456891\nEOG Accuracy: 0.5506607929515418, Correlation: 0.9686894982563453, Last Value Accuracy: 0.5462555066079295, Last Value Correlation: 0.9721381776147026\nDUK Accuracy: 0.4933920704845815, Correlation: 0.9693304786167968, Last Value Accuracy: 0.4889867841409692, Last Value Correlation: 0.9698313657418474\nMPC Accuracy: 0.5770925110132159, Correlation: 0.9716364908547462, Last Value Accuracy: 0.5550660792951542, Last Value Correlation: 0.9941712442141354\nSLB Accuracy: 0.4669603524229075, Correlation: 0.9745432552946375, Last Value Accuracy: 0.4669603524229075, Last Value Correlation: 0.9752418496648181\nPSX Accuracy: 0.6079295154185022, Correlation: 0.9801502512089683, Last Value Accuracy: 0.6123348017621145, Last Value Correlation: 0.995576915466044\nOXY Accuracy: 0.5110132158590308, Correlation: 0.9636470842152328, Last Value Accuracy: 0.5154185022026432, Last Value Correlation: 0.9638295520210561\n\n\nAverage Accuracy\nBelow shows the overall accuracy, summed across our 10 stocks, vs the Last Value Benchmark.\nWe find that the average accuracy of our model slightly outperforms the last value benchmark, but our correlation slighty underperforms the last value benchmark.\n\n\nCode\nprint(f'Avg Accuracy: {accuracy_list.mean()}, Avg Correlation: {correl_list.mean()}, Avg LV Accuracy: {lv_accuracy_list.mean()}, Avg LV Correlation: {lv_correl_list.mean()}')\n\n\nAvg Accuracy: 0.5356828193832598, Avg Correlation: 0.9716605274286853, Avg LV Accuracy: 0.5308370044052864, Avg LV Correlation: 0.9782363277836815\n\n\nCumulative Returns\nThe code below shows the comparison between our strategy returns, the baseline stock returns, and the last value benchmark returns.\nWe find that our strategy outperforms the baseline stock returns and the last value benchmark returns.\n\n\nCode\ndf_strat_returns = pd.DataFrame(cum_strat_returns_list.transpose(), columns=tickers)\ndf_strat_returns['Cum_Strat_Returns'] = df_strat_returns.mean(axis=1)\ndf_strat_returns.index = X_test[0].index\n\ndf_stock_returns = pd.DataFrame(cum_stock_returns_list.transpose(), columns=tickers)\ndf_stock_returns['Cum_Stock_Returns'] = df_stock_returns.mean(axis=1)\ndf_stock_returns.index = X_test[0].index\n\ndf_lv_returns = pd.DataFrame(cum_lv_returns_list.transpose(), columns=tickers)\ndf_lv_returns['Cum_LV_Returns'] = df_lv_returns.mean(axis=1)\ndf_lv_returns.index = X_test[0].index\n\ntotal_strat_returns = df_strat_returns['Cum_Strat_Returns'].iloc[-1]\ntotal_stock_returns = df_stock_returns['Cum_Stock_Returns'].iloc[-1]\ntotal_lv_returns = df_lv_returns['Cum_LV_Returns'].iloc[-1]\n\nprint(f'1 Year Portfolio Returns: {total_strat_returns}')\nprint(f'1 Year Stock Returns: {total_stock_returns}')\nprint(f'1 Year LV Returns: {total_lv_returns}')\n\nplt.figure(figsize=(10,5))\nplt.plot(df_strat_returns['Cum_Strat_Returns'], label='Strategy Returns')\nplt.plot(df_stock_returns['Cum_Stock_Returns'], label='Stock Returns')\nplt.plot(df_lv_returns['Cum_LV_Returns'], label='Last Value Benchmark')\nplt.legend();\n\n\n1 Year Portfolio Returns: 1.2360575366734956\n1 Year Stock Returns: 1.2066049840833908\n1 Year LV Returns: 1.195690607517653\n\n\n\n\n\n\n\n\nFigure 1: A comparison of cumulative returns between our strategy returns, baseline stock returns, and the last value benchmark.\n\n\n\n\n\n\n\n\n\nFor fun, we decided to do a live mock test for the past week (2024/05/07 - 2024/05/16) to see how are model does on current data. We followed the same procedures as above except we trained on 10 years of data prior to our test week.\n\n\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n\n\n\n\nEpoch: 0, loss: 0.89484\nEpoch: 100, loss: 0.00404\nEpoch: 200, loss: 0.00378\nEpoch: 300, loss: 0.00369\nEpoch: 400, loss: 0.00369\nEpoch: 500, loss: 0.00373\nEpoch: 600, loss: 0.00389\nEpoch: 700, loss: 0.00382\nEpoch: 800, loss: 0.00376\nEpoch: 900, loss: 0.00399\n\n\n\n\n['lstm_live.joblib']\n\n\n::: {#cell-26 .cell 0=‘e’ 1=‘c’ 2=‘h’ 3=‘o’ 4=‘:’ 5=‘f’ 6=‘a’ 7=‘l’ 8=‘s’ 9=‘e’ execution_count=67}\nlstm_live = load('lstm_live.joblib')\n:::\nHere are the accuracies and correlations for each stock based on our strategy and last value:\n\nfor i in range(10):\n    cum_strat_returns, cum_stock_returns, cum_lv_returns, accuracy, lv_accuracy, prediction_correl, lv_prediction_correl = evaluate_lstm(lstm_live, X_test_list[i], y_test_list[i], X_scalers[i], y_scalers[i], features)\n    if i == 0:\n        cum_strat_returns_list = np.array([cum_strat_returns])\n        cum_stock_returns_list = np.array([cum_stock_returns])\n        cum_lv_returns_list = np.array([cum_lv_returns])\n        accuracy_list = np.array([accuracy])\n        lv_accuracy_list = np.array([lv_accuracy])\n        correl_list = np.array([prediction_correl])\n    else:\n        cum_strat_returns_list = np.append(cum_strat_returns_list, np.array([cum_strat_returns]), axis=0)\n        cum_stock_returns_list = np.append(cum_stock_returns_list, np.array([cum_stock_returns]), axis=0)\n        cum_lv_returns_list = np.append(cum_lv_returns_list, np.array([cum_lv_returns]), axis=0)\n        accuracy_list = np.append(accuracy_list, np.array([accuracy]), axis=0)\n        lv_accuracy_list = np.append(lv_accuracy_list, np.array([lv_accuracy]), axis=0)\n        correl_list = np.append(correl_list, np.array([prediction_correl]), axis=0)\n\nXOM Accuracy: 0.5, Correlation: 0.45596059959860424, Last Value Accuracy: 0.3333333333333333, Last Value Correlation: 0.442960265038012\nCVX Accuracy: 0.6666666666666666, Correlation: 0.33422337988058554, Last Value Accuracy: 0.5, Last Value Correlation: 0.34341958688787927\nCOP Accuracy: 0.6666666666666666, Correlation: 0.7971408777137727, Last Value Accuracy: 0.6666666666666666, Last Value Correlation: 0.7558464387577114\nEPD Accuracy: 0.3333333333333333, Correlation: 0.8684150010738954, Last Value Accuracy: 0.3333333333333333, Last Value Correlation: 0.8603779889931645\nEOG Accuracy: 0.3333333333333333, Correlation: 0.2363276728915621, Last Value Accuracy: 0.3333333333333333, Last Value Correlation: 0.25582456053077646\nDUK Accuracy: 0.3333333333333333, Correlation: -0.09651392314802673, Last Value Accuracy: 0.5, Last Value Correlation: -0.159262707461855\nMPC Accuracy: 0.6666666666666666, Correlation: 0.7663651315254851, Last Value Accuracy: 0.6666666666666666, Last Value Correlation: 0.7016596578784171\nSLB Accuracy: 0.5, Correlation: 0.42526094830678546, Last Value Accuracy: 0.5, Last Value Correlation: 0.44817487130500694\nPSX Accuracy: 0.6666666666666666, Correlation: 0.19340309591226992, Last Value Accuracy: 0.8333333333333334, Last Value Correlation: 0.27552121319099077\nOXY Accuracy: 0.3333333333333333, Correlation: 0.40172522417713447, Last Value Accuracy: 0.16666666666666666, Last Value Correlation: 0.4083298375864765\n\n\nHere are the average accuracies and correlations:\n\n\nCode\nprint(f'Avg Accuracy: {accuracy_list.mean()}, Avg Correlation: {correl_list.mean()}, Avg LV Accuracy: {lv_accuracy_list.mean()}, Avg LV Correlation: {lv_prediction_correl.mean()}')\n\n\nAvg Accuracy: 0.4999999999999999, Avg Correlation: 0.43823080079320686, Avg LV Accuracy: 0.4833333333333333, Avg LV Correlation: 0.4083298375864765\n\n\nHere are the returns for the strategy, stocks, and last value:\n\n\nCode\ndf_strat_returns = pd.DataFrame(cum_strat_returns_list.transpose(), columns=tickers)\ndf_strat_returns['Cum_Strat_Returns'] = df_strat_returns.mean(axis=1)\ndf_strat_returns.index = X_test_list[0].index\n\ndf_stock_returns = pd.DataFrame(cum_stock_returns_list.transpose(), columns=tickers)\ndf_stock_returns['Cum_Stock_Returns'] = df_stock_returns.mean(axis=1)\ndf_stock_returns.index = X_test_list[0].index\n\ndf_lv_returns = pd.DataFrame(cum_lv_returns_list.transpose(), columns=tickers)\ndf_lv_returns['Cum_LV_Returns'] = df_lv_returns.mean(axis=1)\ndf_lv_returns.index = X_test_list[0].index\n\ntotal_strat_returns = df_strat_returns['Cum_Strat_Returns'].iloc[-1]\ntotal_stock_returns = df_stock_returns['Cum_Stock_Returns'].iloc[-1]\ntotal_lv_returns = df_lv_returns['Cum_LV_Returns'].iloc[-1]\n\nprint(f'1 Week Portfolio Returns: {total_strat_returns}')\nprint(f'1 Week Stock Returns: {total_stock_returns}')\nprint(f'1 Week LV Returns: {total_lv_returns}')\n\nplt.figure(figsize=(10,5))\nplt.plot(df_strat_returns['Cum_Strat_Returns'], label='Strategy Returns')\nplt.plot(df_stock_returns['Cum_Stock_Returns'], label='Stock Returns')\nplt.plot(df_lv_returns['Cum_LV_Returns'], label='Last Value Benchmark')\nplt.legend();\n\n\n1 Week Portfolio Returns: 0.9981978730664818\n1 Week Stock Returns: 0.9968738574121445\n1 Week LV Returns: 0.9952073325941001\n\n\n\n\n\n\n\n\nFigure 2: A comparison of cumulative returns between our strategy returns, baseline stock returns, and the last value benchmark for 2024-05-07 to 2024-05-16.\n\n\n\n\n\n\n\n\nOur project was a success in the sense that we constructed a model that is profitable and more accurate than our benchmarks. Our original goal was to be better than “random chance” but this last value machine provides a more reasonable benchmark given that stocks tend to rise more than they fall over long time horizons (so simply saying “an accuracy above 0.5 is a success” is not reasonable). On our historical data test, we also achieved an average correlation coefficient of 0.9717 between our predicted prices and actual prices, which slightly lags the best results from Bhandari et al. (2022) and is below our last value benchmark at 0.9782. We didn’t take the same approach as we initially expected of using alternative data, instead using more conventional features, but the ultimate goal was accuracy and profitability, so we aren’t concerned by this change in methodology. We had substantially lower accuracy than Gunduz (2021) , who achieved accuracy of up to 0.675. There are many factors that could have contributed to this difference, including time frames (Gunduz used hourly data vs. our daily data), company geographies (Gunduz studied companies on the Borsa Istanbul), and number of features (Gunduz created a substantially greater number of features).\nThere are a two main key assumptions worth noting. Our tests above make two simplifying assumptions about trading. First, we assume the entire portfolio enters every trade, which any reasonable asset manager would think is incredibly reckless and is a major risk management failure. Second, we assume we are able to buy and sell stocks exactly at their closing price on a given day. This isn’t as problematic an assumption as the first, but it’s still an assumption that may not reflect real-world circumstances, especially when trading small stocks with low trading volumes or, more generally, when trading with enough capital to influence stock prices.\nIf we had more time, data, and computational resources, we would have explored creating and filtering a substantially greater number of features. We also would have liked to have worked with larger baskets of companies. We chose energy companies based on intuition that training a model on data from the same industry would result in better predictions.\n\n\n\nAndre did research on RFE using logistic regression, random forest, and support vector machine before pivoting to an LSTM axiao_research.ipynb. He wrote the source code for the data preparation in lstm_data.py, the LSTM model in lstm_model.py, and the evaluation. He wrote the code for the plots for comparing cumulative returns and the code for calculating the accuracy of the strategy and the benchmarks.\nDonovan provided the initial research and the code for calculating the features SMA_20, SMA_50, RSI, Z_Score, and Std_Dev. He provided visualizations for the moving averages and performed inital tests using logistic regression in dwood_test.ipynb. He wrote the data section.\nJames worked on an early analysis using Google Trends data in prelim-analysis.ipynb, which we pivoted away from after realizing the limited supply of daily data. He created the presentation and wrote the abstract, the introduction, the values, and the conclusion sections. He also wrote the code to calculate the correlation coefficients between predicted prices and actual prices.\n\n\n\nThrough implementing this project, I learned a lot about the stock market and the difficulties of predicting stock prices. I was able to learn a lot about the quantitative side of the market, methods used, and correct implementation of such through research and hands on practice. I learned a lot about useful API’s in this field, the challenges/limitations of the basic data we were able to obtain, and how to manipulate/construct different data from our base dataset that would serve as more useful features and parameters.\nCommunication through this project went very smoothly, James and Andre were very responsive and helpful in planning meeting times and discussing the project. We were able to divide the work evenly and efficiently and were able to complete the project in a timely manner. I think a huge component of this was the matter of the project itself. This project held a lot of interest from all of us in one way or another so we didn’t see it as just another assignment but genuinely something that was intellectually stimulating and fun to work on.\nPersonally, I feel happy with the outcome of the project and the skills I’ve learned along the way. Going into it initially, I was not expecting to be able to create a model that drastically beats the market (otherwise I would just leave school now and start a firm) rather, I wanted to learn about the process of creating a model, the challenges that come with it, and the skills that are needed to do so. One of the most proud things that I did during the tenure of the project was thoroughly deep dive into the LSTM model, learn about how it works, and how we can implement it into our own project by researching the model through other papers, Youtube videos, and other resources. Overall I feel satisfied with the outcome of the project and the work that the group and I have achieved.\nThis project will hopefully be the starting foundation for my future career. As an aspiring Quantitative trader, learning the basics and implementing an actual model that can predict stock prices is a huge step in the right direction. Using this knowledge, I hope to continue to increase my expertise in the field and eventually create a career out of it.\nI want to thank Professor Chodrow for the amazing semester. I’ve learned and accomplished so much in this class and I’m excited to see where my future endeavors take me."
  },
  {
    "objectID": "posts/cs-451-quant-project/quant_research.html#abstract",
    "href": "posts/cs-451-quant-project/quant_research.html#abstract",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "",
    "text": "We applied machine learning methods to predict daily stock price movements in a basket of 10 US-listed energy companies. We found the most success using an LSTM model, achieving an accuracy of up to 61% on one stock (PSX). In line with prior literature, we compared our results to a benchmark established by a last value machine, which simply predicts the next day’s price to be the current day’s actual price. We tested our model on two different sets of observations, first on a year’s worth of historical data (historical test) and then on one week of recent trading data (live test). Comparing our LSTM results to our benchmark, we find mixed results. We achieve, on average, 53.57% accuracy vs. our benchmark’s 53.08% accuracy on the historical test. For the ten companies in our analysis, our model has superior for accuracy for 6 companies, has equal accuracy for 1 company, and has worse accuracy for 3 companies compared to our last value benchmark. This superior accuracy leads to higher simulated portfolio returns using our model compared to our benchmarks. Our live test accuracy of 49.99% was worse but still beat our benchmark’s accuracy of 48.33%."
  },
  {
    "objectID": "posts/cs-451-quant-project/quant_research.html#introduction",
    "href": "posts/cs-451-quant-project/quant_research.html#introduction",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "",
    "text": "In this blog post, we train machine learning models on historical stock market data to predict future stock price movements. This is a highly popular problem to address because of the potential for significant monetary gain. This is an important problem societally because stock markets are mechanisms of price discovery: they answer the question “What is a company worth?” Finding the right answer to that question allows society to correctly allocate more or less capital (money) to that company. On an individual level, this is an important problem to us as the authors because it’s the problem for all quant trading: making a profitable model.\nAn enormous body of literature within and without computer science exists for stock market prediction. Among the papers most relevant to our work are Gunduz (2021), Bhandari et al. (2022), and Zhang (2022).\nGunduz (2021) applies LSTM and ensemble learning (Light-GBM) models to predict the hourly directions of eight banking stocks in Borsa Istanbul. He achieved up to maximum success rate of 0.685 using individual features of bank stocks and LSTM.\nBhandari et al. (2022) apply single-layer and multi-layer LSTM models to the problem of predicting the S&P 500, the index of the largest 500 publicly traded companies in America. Their single-layer LTSM model with 150 neurons is their best performing specification. Their set of predicted values have an average correlation coefficient of 0.9976 with actual S&P index values.\nZhang (2022) finds the LSTM network model does not perform better than other models when applied to a short forecasting horizon (1 to 10 days). Zhang’s “other models” are linear regression, eXtreme gradient boosting (XGBoost), last value, and moving average.\nWe take some of the “best practices” we observe in the above papers, specifically benchmarking with last value and using MSE and R to assess our results. Unlike the mentioned papers, we will be focusing on single stocks and attempting to build a model that outperforms the last value benchmark."
  },
  {
    "objectID": "posts/cs-451-quant-project/quant_research.html#values",
    "href": "posts/cs-451-quant-project/quant_research.html#values",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "",
    "text": "The potential users are anyone interested in making profitable trades in the stock market. They are the individuals most likely to directly benefit from our work. Nonusers who could be affected by our work are those engaged in the stock market. The obvious affected nonusers are those on the opposite side of each trade as a user. In every trade, there’s a buyer and a seller, so in every trade, there’s a winner and a loser. These opposing nonusers are the individuals who are most likely to be harmed by the success of our program.\nUltimately, the point of the back and forth of markets is price discovery: to help society find the right prices of different companies. This leads to another nonuser effect: with better price discovery and more efficient markets, companies will raise money at prices that are closer to some “true” value, which is loosely defined as a value that best reflects the fundamental valuation of the company. Our model does not attempt to predict a true fundamental value for a company, but by making accurate predictions for the next day’s price, it should accelerate the market’s convergence to an appropriate value.\nA useful financial trading model should lead to a net societal benefit because better financial markets mean more or less money going to companies and therefore projects, leading to something closer to an “optimal” allocation of money in society.\nWe are personally motivated to work on this project because of personal interest, professional relevance, and the difficulty of the problem. All three of us personally invest in the stock market. Two of us (Donovan & James) are double majors in economics and have had experience working in the financial services industry. Andre is interested in pursuing a master’s in financial engineering after Middlebury. The problem itself is also inherently challenging: financial markets are constantly adapting and changing, making the findings of previous literature increasingly likely over time to be less applicable to today’s markets. This forces us to adopt new techniques. # Materials and Methods"
  },
  {
    "objectID": "posts/cs-451-quant-project/quant_research.html#our-data",
    "href": "posts/cs-451-quant-project/quant_research.html#our-data",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "",
    "text": "Our data was sourced from Yahoo Finance. We used the yfinance library to download historical stock price data for our 10 different stocks. We chose to focus on US-based oil companies. These companies are Exxon Mobil (XOM), Chevron (CVX), ConocoPhillips (COP), Enterprise Products Partners (EPD), Pioneer Natural Resources (PXD), EOG Resources (EOG), Duke Energy (DUK), Marathon Petroleum (MPC), Schlumberger (SLB), and Phillips 66 (PSX). We downloaded the data from May 6th, 2014 to May 6th, 2024.\nWithin the yfinance dataset we were given the following columns: Open, High, Low, Close, Adj Close, Volume.\nOpen is the opening price of the stock for the day. High is the highest price of the stock for the day. Low is the lowest price of the stock for the day. Close is the closing price of the stock for the day. Adj Close is the adjusted closing price of the stock for the day. Volume is the number of shares traded for the day.\nWe used the Close column as our target variable for our model. We also created the following features: SMA_20, SMA_50, Std_Dev, Z_Score, RSI, TTM_P/E which will be discussed below. Here’s a look at what the raw data looks like:\n\nimport yfinance as yf\n\nxom = yf.Ticker('XOM')\ndata = xom.history(start='2014-05-06', end='2024-05-07')\ndata.head()\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\nDividends\nStock Splits\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n2014-05-06 00:00:00-04:00\n66.050586\n66.501227\n65.928274\n66.095650\n9669800\n0.00\n0.0\n\n\n2014-05-07 00:00:00-04:00\n66.385376\n66.597816\n66.172931\n66.378937\n11007400\n0.00\n0.0\n\n\n2014-05-08 00:00:00-04:00\n66.366038\n66.494794\n65.773772\n65.870338\n8922500\n0.00\n0.0\n\n\n2014-05-09 00:00:00-04:00\n65.922184\n66.226810\n65.630524\n66.077736\n8948800\n0.69\n0.0\n\n\n2014-05-12 00:00:00-04:00\n66.324029\n66.336990\n65.805516\n66.259216\n8830500\n0.00\n0.0\n\n\n\n\n\n\n\nYou can find the full implementation of our data at lstm_data.py under the function prepare_data()."
  },
  {
    "objectID": "posts/cs-451-quant-project/quant_research.html#our-approach",
    "href": "posts/cs-451-quant-project/quant_research.html#our-approach",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "",
    "text": "We used SMA_20, SMA_50, Std_Dev, Z_Score, RSI, Close, TTM_P/E as predictors for our models.\nThe SMA_20 and SMA_50 are the 20-day and 50-day simple moving averages of the stock price. This means that the average closing price of the stock over the last 20 and 50 days, respectively.\nThe Std_Dev is the standard deviation of the stock price meaning how much the stock price deviates from the mean.\nThe Z_Score is the z-score of the stock price meaning how many standard deviations the stock price is from the mean.\nThe RSI is the relative strength index of the stock price meaning how strong the stock price is relative to its past performance. It is calculated by taking the average of the gains and losses over a certain period of time.\nThe Close is the closing price of the stock per day.\nThe TTM_P/E is the trailing twelve months price-to-earnings ratio of the stock.\nWe used the next day’s Close price as the target variable for our model.\n\n\n\nWe collected 10 years of data from May 7th, 2014 to May 7th, 2024 and used a train-test split of 90-10 in order to train our model on the first 9 years worth of the data and test it on the remaining 1 year’s worth of data. We used a standard scaler for scaling our data in order to ensure that the data was normalized. We fit the scaler on the training data and then applied it to the test data to avoid any information leaking. We then combined the training data for each stock into one dataset. We used the closing price of the stock as the target variable for our model.\nHere’s what our data looks like after creating our features and scaling the data:\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\ntickers = ['XOM', 'CVX', 'COP', 'EPD', 'EOG', 'DUK', 'MPC', 'SLB', 'PSX', 'OXY']\nstart = '2014-05-06'\nend = '2024-05-07'\n\n# preps data, see lstm_data.py, prints size of each ticker's dataset\nX_train, y_train, X_test, y_test, X_scalers, y_scalers, batch_size = prepare_data(tickers, start_date=start, end_date=end, test_size=0.1)\nX_train\n\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\nDividends\nStock Splits\nSMA_10\nSMA_20\nSMA_50\nSMA_100\nSMA_250\nStd_Dev\nZ_Score\nRSI\nTTM_EPS\nTTM_P/E\nReturns\nTicker\n\n\n\n\n2015-05-01\n-0.139877\n-0.131185\n-0.112171\n-0.102371\n-0.540891\n-0.127836\n0.00000\n-0.149377\n-0.173825\n-0.178037\n-0.063809\n0.319177\n-0.679901\n1.186546\n0.879810\n0.650885\n-0.095652\n0.893759\nXOM\n\n\n2015-05-04\n-0.080169\n-0.096257\n-0.064154\n-0.092513\n-0.732495\n-0.127836\n0.00000\n-0.142218\n-0.166130\n-0.178686\n-0.064625\n0.316818\n-0.653451\n1.177769\n0.973560\n0.650885\n-0.095424\n0.133748\nXOM\n\n\n2015-05-05\n-0.059251\n-0.088369\n-0.080672\n-0.111473\n-0.577151\n-0.127836\n0.00000\n-0.135595\n-0.160600\n-0.179007\n-0.065519\n0.314254\n-0.642313\n0.791839\n0.672019\n0.650885\n-0.095861\n-0.329225\nXOM\n\n\n2015-05-06\n-0.071421\n-0.093251\n-0.108714\n-0.127400\n-0.639387\n-0.127836\n0.00000\n-0.132800\n-0.152615\n-0.179992\n-0.065441\n0.311766\n-0.751693\n0.518627\n0.427504\n0.650885\n-0.096229\n-0.281992\nXOM\n\n\n2015-05-07\n-0.134173\n-0.163112\n-0.142135\n-0.149774\n-0.767664\n-0.127836\n0.00000\n-0.132571\n-0.146911\n-0.181595\n-0.065840\n0.309079\n-0.859104\n0.047923\n0.100192\n0.650885\n-0.096744\n-0.387753\nXOM\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-06-02\n0.610803\n0.625792\n0.636749\n0.650958\n0.109535\n-0.105084\n-0.02214\n0.607044\n0.598059\n0.727834\n0.786385\n0.982249\n-0.739053\n0.869582\n0.136220\n1.924677\n-0.222355\n0.810058\nOXY\n\n\n2023-06-05\n0.714326\n0.672373\n0.676732\n0.633996\n-0.312158\n-0.105084\n-0.02214\n0.608629\n0.594259\n0.728931\n0.783608\n0.979744\n-0.834725\n0.776482\n0.011823\n1.924677\n-0.222464\n-0.165726\nOXY\n\n\n2023-06-06\n0.590332\n0.602501\n0.623814\n0.636336\n-0.325573\n-0.105084\n-0.02214\n0.610977\n0.595673\n0.730434\n0.780891\n0.977007\n-0.819762\n0.773031\n0.029482\n1.924677\n-0.222449\n0.003637\nOXY\n\n\n2023-06-07\n0.652329\n0.667133\n0.694371\n0.671429\n-0.292849\n-0.105084\n-0.02214\n0.613090\n0.598854\n0.730900\n0.777538\n0.974819\n-0.772194\n1.238762\n0.294437\n1.924677\n-0.222223\n0.292064\nOXY\n\n\n2023-06-08\n0.669287\n0.645055\n0.635925\n0.662629\n-0.147267\n2.262466\n-0.02214\n0.616022\n0.607809\n0.728131\n0.774355\n0.973557\n-0.847555\n1.077118\n0.218085\n1.924677\n-0.222280\n-0.093700\nOXY\n\n\n\n\n20410 rows × 19 columns\n\n\n\n\n\n\nOriginally, we used rather simplistic models like logistic regression, Random Forest, and SVM in order to predict stock price movements. We utilized Recursive Feature Elimination (RFE) in order to determine the optimal features for prediction for each model. However, we found that these models were not able to predict stock price movements consistently with much accuracy. We then decided to use a Long Short-Term Memory (LSTM) model to predict stock price movements. LSTM models are a type of recurrent neural network (RNN) with the addition of “gates” notably the input, forget and output gates. These gates allow for the model to determine what information to retain or discard at each timestep, mitigating the vanishing descent issue found in traditional recurrent neural networks. The LSTM model accounts for the shortfalls of an RNN by capturing long-term dependencies in the data.\nThe forget gate determines which information is either retained or discarded at each time step. It accepts the output from the previous time step \\(h_{t-1}\\) and the input \\(x_t\\) at the current time step. The forget gate is defined as:\n\\[f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\\]\nThe input gate determines which information is stored in the cell state. It avoids feeding the unimportant information into the current memory cell. It has three different components:\n\nGetting the state of the cell that must be updated.\nCreate a new cell state\nUpdate the cell state to the current cell state\n\nThese are defined as:\n\\[\\begin{aligned}\ni_{t} &= \\sigma(W_{t} \\cdot [h_{t-1}, x_{t}] + b_{i}) \\\\\n\\widetilde{C}_{t} &= \\tanh(W_{c} \\cdot [h_{t-1}, x_{t}] + b_{c}) \\\\\nC_{t} &= f_{t} \\ast C_{t-1} + i_{t} \\ast \\widetilde{C}_{t}\n\\end{aligned}\\]\nThe output gate determines how much of the newly created cell state will be discarded and how much will be passed to the output. It is defined as:\n\\[o_{t} = \\sigma(W_{o} \\cdot [h_{t-1}, x_{t}] + b_{o})\\]\nThis output information is firstly determined by a sigmoid layer, then the newly created cell state is processed by a tanh layer. The output is then multiplied by the sigmoid layer to determine the final output of the LSTM cell.\nWhich is defined as:\n\\[h_{t} = o_{t} \\ast \\tanh(C_{t})\\]\nTaking this all into account, the LSTM model is able to retain information from previous time steps and use it to predict future stock price movements while disregarding irrelevant information.\nThe implementation of our LSTM model can be found at: lstm_model.py\n\n\n\nWe first converted our wanted feature columns into a torch Variable to allow them to be differentiable. Then, we reshaped the data using torch.reshape() and torch.utils.data.DataLoader into [batch_size, seq_len, input_size].\n\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Close', 'TTM_P/E']\n\nX_train_tensors = Variable(torch.Tensor(np.array(X_train[features])))\ny_train_tensors = Variable(torch.Tensor(y_train.values))\nX_train_final = torch.reshape(X_train_tensors, (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\n\n# split data by ticker\ndata_loader_train = torch.utils.data.DataLoader(\n    torch.utils.data.TensorDataset(X_train_final, y_train_tensors),\n    batch_size=batch_size,\n    shuffle=True\n)\n\nnext(iter(data_loader_train))[0].shape\n\ntorch.Size([2041, 1, 7])\n\n\nWe trained our model using our own personal devices. We used the Adam optimizer with a learning rate of 0.001. We trained the model for 1000 epochs for each stock in our dataset (10 total) and used the torch.nn.MSELoss() loss function to train the model.\nMSE is defined as:\n\\[MSE = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_{i} - y_{i})^2\\]\nWhere \\(y_{i}\\) is the true price and \\(\\hat{y}_{i}\\) is the predicted price.\nAs mentioned previously our model was trained on 90% of the data and tested on the remaining 10%.\nIf the model predicted the next days price to be positive, we would purchase the stock at the closing price and sell it at the closing price the next day. If the model predicted the next days price to be negative, we would short the stock at the closing price and buy it back at the closing price the next day. We would then calculate the profit or loss percent change for each stock and compare it to the last value benchmark.\nBelow is our training code:\n\nnum_epochs = 1000 # 1000 epochs\nlearning_rate = 0.001 # 0.001 lr\n\ninput_size = X_train_final.shape[2] # number of features\nhidden_size = 32 # number of features in hidden state\nnum_layers = 1 # number of stacked lstm layers\nwindow = 1 # number of windows, leave at 1, basically can ignore\n\nnum_classes = 1 # number of output classes\n\nlstm = LSTMModel(num_classes, input_size, hidden_size, num_layers, seq_length=window, batch_size=batch_size) #our lstm class \ncriterion = torch.nn.MSELoss()    # mean-squared error for regression\noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate) # ADAM optimizer\n\n# training loop\nfor epoch in range(num_epochs):\n  for i, data in enumerate(data_loader_train):\n    X_, y_ = data\n    outputs = lstm.forward(X_) #forward pass\n    optimizer.zero_grad() #calculate the gradient, manually setting to 0\n  \n    # obtain the loss function\n    loss = criterion(outputs, y_.reshape(y_.size(0)*y_.size(1), 1))\n  \n    loss.backward() #calculates the loss of the loss function\n  \n    optimizer.step() #improve from loss, i.e backprop\n    # if (i + 1) % 50 == 0:\n    #     print(f\"Epoch {epoch}, batch {i:&gt;3}, loss on batch: {loss.item():.3f}\")\n  if epoch % 100 == 0:\n    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n\nEpoch: 0, loss: 0.91365\nEpoch: 100, loss: 0.00494\nEpoch: 200, loss: 0.00502\nEpoch: 300, loss: 0.00508\nEpoch: 400, loss: 0.00451\nEpoch: 500, loss: 0.00478\nEpoch: 600, loss: 0.00479\nEpoch: 700, loss: 0.00491\nEpoch: 800, loss: 0.00514\nEpoch: 900, loss: 0.00504\n\n\n\n\n\nWe evaluated our model by comparing the cumulative predicted stock price returns and accuracy to the actual cumulative stock price returns and accuracy and the cumulative last value benchmark returns and accuracy. The last value benchmark is defined as using the previous days value as the prediction for the current day. We would buy the stock at the current day’s close price and sell at the next day’s close price if the predicted returns were positive and do nothing if the predicted returns were negative. We followed the same principle in calculating actual cumulative stock returns and accuracy, and the cumulative last value benchmark returns and accuracy.\nWe define accuracy for our purposes as percentage of times the model correctly predicts an upward or downward movement in the share price of a company.\nConsider a simple test case where the model predicts the stock price to go up and the stock price actually goes up. In this case, the model is correct. If the model predicts the stock price to go up and the stock price actually goes down, the model is incorrect. We calculate the accuracy of the model by dividing the number of correct predictions by the total number of predictions.\nAccuracy per Stock\n\n\nCode\nfor i in range(10):\n    cum_strat_returns, cum_stock_returns, cum_lv_returns, accuracy, lv_accuracy, prediction_correl, lv_prediction_correl = evaluate_lstm(lstm, X_test[i], y_test[i], X_scalers[i], y_scalers[i], features)\n    if i == 0:\n        cum_strat_returns_list = np.array([cum_strat_returns])\n        cum_stock_returns_list = np.array([cum_stock_returns])\n        cum_lv_returns_list = np.array([cum_lv_returns])\n        accuracy_list = np.array([accuracy])\n        lv_accuracy_list = np.array([lv_accuracy])\n        correl_list = np.array([prediction_correl])\n        lv_correl_list = np.array([lv_prediction_correl])\n    else:\n        cum_strat_returns_list = np.append(cum_strat_returns_list, np.array([cum_strat_returns]), axis=0)\n        cum_stock_returns_list = np.append(cum_stock_returns_list, np.array([cum_stock_returns]), axis=0)\n        cum_lv_returns_list = np.append(cum_lv_returns_list, np.array([cum_lv_returns]), axis=0)\n        accuracy_list = np.append(accuracy_list, np.array([accuracy]), axis=0)\n        lv_accuracy_list = np.append(lv_accuracy_list, np.array([lv_accuracy]), axis=0)\n        correl_list = np.append(correl_list, np.array([prediction_correl]), axis=0)\n        lv_correl_list = np.append(lv_correl_list, np.array([lv_prediction_correl]))\n\n\nXOM Accuracy: 0.5418502202643172, Correlation: 0.972313389714224, Last Value Accuracy: 0.5242290748898678, Last Value Correlation: 0.9780145037734742\nCVX Accuracy: 0.5154185022026432, Correlation: 0.9613024878319475, Last Value Accuracy: 0.5066079295154186, Last Value Correlation: 0.9635678490020599\nCOP Accuracy: 0.5550660792951542, Correlation: 0.9689221084107935, Last Value Accuracy: 0.5418502202643172, Last Value Correlation: 0.9783667790929887\nEPD Accuracy: 0.5374449339207048, Correlation: 0.9860702298831613, Last Value Accuracy: 0.5506607929515418, Last Value Correlation: 0.9916250412456891\nEOG Accuracy: 0.5506607929515418, Correlation: 0.9686894982563453, Last Value Accuracy: 0.5462555066079295, Last Value Correlation: 0.9721381776147026\nDUK Accuracy: 0.4933920704845815, Correlation: 0.9693304786167968, Last Value Accuracy: 0.4889867841409692, Last Value Correlation: 0.9698313657418474\nMPC Accuracy: 0.5770925110132159, Correlation: 0.9716364908547462, Last Value Accuracy: 0.5550660792951542, Last Value Correlation: 0.9941712442141354\nSLB Accuracy: 0.4669603524229075, Correlation: 0.9745432552946375, Last Value Accuracy: 0.4669603524229075, Last Value Correlation: 0.9752418496648181\nPSX Accuracy: 0.6079295154185022, Correlation: 0.9801502512089683, Last Value Accuracy: 0.6123348017621145, Last Value Correlation: 0.995576915466044\nOXY Accuracy: 0.5110132158590308, Correlation: 0.9636470842152328, Last Value Accuracy: 0.5154185022026432, Last Value Correlation: 0.9638295520210561\n\n\nAverage Accuracy\nBelow shows the overall accuracy, summed across our 10 stocks, vs the Last Value Benchmark.\nWe find that the average accuracy of our model slightly outperforms the last value benchmark, but our correlation slighty underperforms the last value benchmark.\n\n\nCode\nprint(f'Avg Accuracy: {accuracy_list.mean()}, Avg Correlation: {correl_list.mean()}, Avg LV Accuracy: {lv_accuracy_list.mean()}, Avg LV Correlation: {lv_correl_list.mean()}')\n\n\nAvg Accuracy: 0.5356828193832598, Avg Correlation: 0.9716605274286853, Avg LV Accuracy: 0.5308370044052864, Avg LV Correlation: 0.9782363277836815\n\n\nCumulative Returns\nThe code below shows the comparison between our strategy returns, the baseline stock returns, and the last value benchmark returns.\nWe find that our strategy outperforms the baseline stock returns and the last value benchmark returns.\n\n\nCode\ndf_strat_returns = pd.DataFrame(cum_strat_returns_list.transpose(), columns=tickers)\ndf_strat_returns['Cum_Strat_Returns'] = df_strat_returns.mean(axis=1)\ndf_strat_returns.index = X_test[0].index\n\ndf_stock_returns = pd.DataFrame(cum_stock_returns_list.transpose(), columns=tickers)\ndf_stock_returns['Cum_Stock_Returns'] = df_stock_returns.mean(axis=1)\ndf_stock_returns.index = X_test[0].index\n\ndf_lv_returns = pd.DataFrame(cum_lv_returns_list.transpose(), columns=tickers)\ndf_lv_returns['Cum_LV_Returns'] = df_lv_returns.mean(axis=1)\ndf_lv_returns.index = X_test[0].index\n\ntotal_strat_returns = df_strat_returns['Cum_Strat_Returns'].iloc[-1]\ntotal_stock_returns = df_stock_returns['Cum_Stock_Returns'].iloc[-1]\ntotal_lv_returns = df_lv_returns['Cum_LV_Returns'].iloc[-1]\n\nprint(f'1 Year Portfolio Returns: {total_strat_returns}')\nprint(f'1 Year Stock Returns: {total_stock_returns}')\nprint(f'1 Year LV Returns: {total_lv_returns}')\n\nplt.figure(figsize=(10,5))\nplt.plot(df_strat_returns['Cum_Strat_Returns'], label='Strategy Returns')\nplt.plot(df_stock_returns['Cum_Stock_Returns'], label='Stock Returns')\nplt.plot(df_lv_returns['Cum_LV_Returns'], label='Last Value Benchmark')\nplt.legend();\n\n\n1 Year Portfolio Returns: 1.2360575366734956\n1 Year Stock Returns: 1.2066049840833908\n1 Year LV Returns: 1.195690607517653\n\n\n\n\n\n\n\n\nFigure 1: A comparison of cumulative returns between our strategy returns, baseline stock returns, and the last value benchmark."
  },
  {
    "objectID": "posts/cs-451-quant-project/quant_research.html#live-mock-testing",
    "href": "posts/cs-451-quant-project/quant_research.html#live-mock-testing",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "",
    "text": "For fun, we decided to do a live mock test for the past week (2024/05/07 - 2024/05/16) to see how are model does on current data. We followed the same procedures as above except we trained on 10 years of data prior to our test week.\n\n\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n\n\n\n\nEpoch: 0, loss: 0.89484\nEpoch: 100, loss: 0.00404\nEpoch: 200, loss: 0.00378\nEpoch: 300, loss: 0.00369\nEpoch: 400, loss: 0.00369\nEpoch: 500, loss: 0.00373\nEpoch: 600, loss: 0.00389\nEpoch: 700, loss: 0.00382\nEpoch: 800, loss: 0.00376\nEpoch: 900, loss: 0.00399\n\n\n\n\n['lstm_live.joblib']\n\n\n::: {#cell-26 .cell 0=‘e’ 1=‘c’ 2=‘h’ 3=‘o’ 4=‘:’ 5=‘f’ 6=‘a’ 7=‘l’ 8=‘s’ 9=‘e’ execution_count=67}\nlstm_live = load('lstm_live.joblib')\n:::\nHere are the accuracies and correlations for each stock based on our strategy and last value:\n\nfor i in range(10):\n    cum_strat_returns, cum_stock_returns, cum_lv_returns, accuracy, lv_accuracy, prediction_correl, lv_prediction_correl = evaluate_lstm(lstm_live, X_test_list[i], y_test_list[i], X_scalers[i], y_scalers[i], features)\n    if i == 0:\n        cum_strat_returns_list = np.array([cum_strat_returns])\n        cum_stock_returns_list = np.array([cum_stock_returns])\n        cum_lv_returns_list = np.array([cum_lv_returns])\n        accuracy_list = np.array([accuracy])\n        lv_accuracy_list = np.array([lv_accuracy])\n        correl_list = np.array([prediction_correl])\n    else:\n        cum_strat_returns_list = np.append(cum_strat_returns_list, np.array([cum_strat_returns]), axis=0)\n        cum_stock_returns_list = np.append(cum_stock_returns_list, np.array([cum_stock_returns]), axis=0)\n        cum_lv_returns_list = np.append(cum_lv_returns_list, np.array([cum_lv_returns]), axis=0)\n        accuracy_list = np.append(accuracy_list, np.array([accuracy]), axis=0)\n        lv_accuracy_list = np.append(lv_accuracy_list, np.array([lv_accuracy]), axis=0)\n        correl_list = np.append(correl_list, np.array([prediction_correl]), axis=0)\n\nXOM Accuracy: 0.5, Correlation: 0.45596059959860424, Last Value Accuracy: 0.3333333333333333, Last Value Correlation: 0.442960265038012\nCVX Accuracy: 0.6666666666666666, Correlation: 0.33422337988058554, Last Value Accuracy: 0.5, Last Value Correlation: 0.34341958688787927\nCOP Accuracy: 0.6666666666666666, Correlation: 0.7971408777137727, Last Value Accuracy: 0.6666666666666666, Last Value Correlation: 0.7558464387577114\nEPD Accuracy: 0.3333333333333333, Correlation: 0.8684150010738954, Last Value Accuracy: 0.3333333333333333, Last Value Correlation: 0.8603779889931645\nEOG Accuracy: 0.3333333333333333, Correlation: 0.2363276728915621, Last Value Accuracy: 0.3333333333333333, Last Value Correlation: 0.25582456053077646\nDUK Accuracy: 0.3333333333333333, Correlation: -0.09651392314802673, Last Value Accuracy: 0.5, Last Value Correlation: -0.159262707461855\nMPC Accuracy: 0.6666666666666666, Correlation: 0.7663651315254851, Last Value Accuracy: 0.6666666666666666, Last Value Correlation: 0.7016596578784171\nSLB Accuracy: 0.5, Correlation: 0.42526094830678546, Last Value Accuracy: 0.5, Last Value Correlation: 0.44817487130500694\nPSX Accuracy: 0.6666666666666666, Correlation: 0.19340309591226992, Last Value Accuracy: 0.8333333333333334, Last Value Correlation: 0.27552121319099077\nOXY Accuracy: 0.3333333333333333, Correlation: 0.40172522417713447, Last Value Accuracy: 0.16666666666666666, Last Value Correlation: 0.4083298375864765\n\n\nHere are the average accuracies and correlations:\n\n\nCode\nprint(f'Avg Accuracy: {accuracy_list.mean()}, Avg Correlation: {correl_list.mean()}, Avg LV Accuracy: {lv_accuracy_list.mean()}, Avg LV Correlation: {lv_prediction_correl.mean()}')\n\n\nAvg Accuracy: 0.4999999999999999, Avg Correlation: 0.43823080079320686, Avg LV Accuracy: 0.4833333333333333, Avg LV Correlation: 0.4083298375864765\n\n\nHere are the returns for the strategy, stocks, and last value:\n\n\nCode\ndf_strat_returns = pd.DataFrame(cum_strat_returns_list.transpose(), columns=tickers)\ndf_strat_returns['Cum_Strat_Returns'] = df_strat_returns.mean(axis=1)\ndf_strat_returns.index = X_test_list[0].index\n\ndf_stock_returns = pd.DataFrame(cum_stock_returns_list.transpose(), columns=tickers)\ndf_stock_returns['Cum_Stock_Returns'] = df_stock_returns.mean(axis=1)\ndf_stock_returns.index = X_test_list[0].index\n\ndf_lv_returns = pd.DataFrame(cum_lv_returns_list.transpose(), columns=tickers)\ndf_lv_returns['Cum_LV_Returns'] = df_lv_returns.mean(axis=1)\ndf_lv_returns.index = X_test_list[0].index\n\ntotal_strat_returns = df_strat_returns['Cum_Strat_Returns'].iloc[-1]\ntotal_stock_returns = df_stock_returns['Cum_Stock_Returns'].iloc[-1]\ntotal_lv_returns = df_lv_returns['Cum_LV_Returns'].iloc[-1]\n\nprint(f'1 Week Portfolio Returns: {total_strat_returns}')\nprint(f'1 Week Stock Returns: {total_stock_returns}')\nprint(f'1 Week LV Returns: {total_lv_returns}')\n\nplt.figure(figsize=(10,5))\nplt.plot(df_strat_returns['Cum_Strat_Returns'], label='Strategy Returns')\nplt.plot(df_stock_returns['Cum_Stock_Returns'], label='Stock Returns')\nplt.plot(df_lv_returns['Cum_LV_Returns'], label='Last Value Benchmark')\nplt.legend();\n\n\n1 Week Portfolio Returns: 0.9981978730664818\n1 Week Stock Returns: 0.9968738574121445\n1 Week LV Returns: 0.9952073325941001\n\n\n\n\n\n\n\n\nFigure 2: A comparison of cumulative returns between our strategy returns, baseline stock returns, and the last value benchmark for 2024-05-07 to 2024-05-16."
  },
  {
    "objectID": "posts/cs-451-quant-project/quant_research.html#concluding-discussion",
    "href": "posts/cs-451-quant-project/quant_research.html#concluding-discussion",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "",
    "text": "Our project was a success in the sense that we constructed a model that is profitable and more accurate than our benchmarks. Our original goal was to be better than “random chance” but this last value machine provides a more reasonable benchmark given that stocks tend to rise more than they fall over long time horizons (so simply saying “an accuracy above 0.5 is a success” is not reasonable). On our historical data test, we also achieved an average correlation coefficient of 0.9717 between our predicted prices and actual prices, which slightly lags the best results from Bhandari et al. (2022) and is below our last value benchmark at 0.9782. We didn’t take the same approach as we initially expected of using alternative data, instead using more conventional features, but the ultimate goal was accuracy and profitability, so we aren’t concerned by this change in methodology. We had substantially lower accuracy than Gunduz (2021) , who achieved accuracy of up to 0.675. There are many factors that could have contributed to this difference, including time frames (Gunduz used hourly data vs. our daily data), company geographies (Gunduz studied companies on the Borsa Istanbul), and number of features (Gunduz created a substantially greater number of features).\nThere are a two main key assumptions worth noting. Our tests above make two simplifying assumptions about trading. First, we assume the entire portfolio enters every trade, which any reasonable asset manager would think is incredibly reckless and is a major risk management failure. Second, we assume we are able to buy and sell stocks exactly at their closing price on a given day. This isn’t as problematic an assumption as the first, but it’s still an assumption that may not reflect real-world circumstances, especially when trading small stocks with low trading volumes or, more generally, when trading with enough capital to influence stock prices.\nIf we had more time, data, and computational resources, we would have explored creating and filtering a substantially greater number of features. We also would have liked to have worked with larger baskets of companies. We chose energy companies based on intuition that training a model on data from the same industry would result in better predictions."
  },
  {
    "objectID": "posts/cs-451-quant-project/quant_research.html#group-contributions-statement",
    "href": "posts/cs-451-quant-project/quant_research.html#group-contributions-statement",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "",
    "text": "Andre did research on RFE using logistic regression, random forest, and support vector machine before pivoting to an LSTM axiao_research.ipynb. He wrote the source code for the data preparation in lstm_data.py, the LSTM model in lstm_model.py, and the evaluation. He wrote the code for the plots for comparing cumulative returns and the code for calculating the accuracy of the strategy and the benchmarks.\nDonovan provided the initial research and the code for calculating the features SMA_20, SMA_50, RSI, Z_Score, and Std_Dev. He provided visualizations for the moving averages and performed inital tests using logistic regression in dwood_test.ipynb. He wrote the data section.\nJames worked on an early analysis using Google Trends data in prelim-analysis.ipynb, which we pivoted away from after realizing the limited supply of daily data. He created the presentation and wrote the abstract, the introduction, the values, and the conclusion sections. He also wrote the code to calculate the correlation coefficients between predicted prices and actual prices."
  },
  {
    "objectID": "posts/cs-451-quant-project/quant_research.html#personal-reflection",
    "href": "posts/cs-451-quant-project/quant_research.html#personal-reflection",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "",
    "text": "Through implementing this project, I learned a lot about the stock market and the difficulties of predicting stock prices. I was able to learn a lot about the quantitative side of the market, methods used, and correct implementation of such through research and hands on practice. I learned a lot about useful API’s in this field, the challenges/limitations of the basic data we were able to obtain, and how to manipulate/construct different data from our base dataset that would serve as more useful features and parameters.\nCommunication through this project went very smoothly, James and Andre were very responsive and helpful in planning meeting times and discussing the project. We were able to divide the work evenly and efficiently and were able to complete the project in a timely manner. I think a huge component of this was the matter of the project itself. This project held a lot of interest from all of us in one way or another so we didn’t see it as just another assignment but genuinely something that was intellectually stimulating and fun to work on.\nPersonally, I feel happy with the outcome of the project and the skills I’ve learned along the way. Going into it initially, I was not expecting to be able to create a model that drastically beats the market (otherwise I would just leave school now and start a firm) rather, I wanted to learn about the process of creating a model, the challenges that come with it, and the skills that are needed to do so. One of the most proud things that I did during the tenure of the project was thoroughly deep dive into the LSTM model, learn about how it works, and how we can implement it into our own project by researching the model through other papers, Youtube videos, and other resources. Overall I feel satisfied with the outcome of the project and the work that the group and I have achieved.\nThis project will hopefully be the starting foundation for my future career. As an aspiring Quantitative trader, learning the basics and implementing an actual model that can predict stock prices is a huge step in the right direction. Using this knowledge, I hope to continue to increase my expertise in the field and eventually create a career out of it.\nI want to thank Professor Chodrow for the amazing semester. I’ve learned and accomplished so much in this class and I’m excited to see where my future endeavors take me."
  }
]